{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Docprompt - Getting Started","text":""},{"location":"#supercharged-document-analysis","title":"Supercharged Document Analysis","text":"<ul> <li>Common utilities for interacting with PDFs</li> <li>PDF loading and serialization</li> <li>PDF byte compression using Ghostscript </li> <li>Fast rasterization  </li> <li>Page splitting, re-export with PDFium</li> <li>Support for most OCR providers with batched inference</li> <li>Google </li> <li>Azure Document Intelligence </li> <li>Amazon Textract </li> <li>Tesseract </li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>Base installation</p> <pre><code>pip install docprompt\n</code></pre> <p>With an OCR provider</p> <pre><code>pip install \"docprompt[google]\n</code></pre>"},{"location":"#usage","title":"Usage","text":""},{"location":"#simple-operations","title":"Simple Operations","text":"<pre><code>from docprompt import load_document\n\n# Load a document\ndocument = load_document(\"path/to/my.pdf\")\n\n# Rasterize a single page using Ghostscript\npage_number = 5\nrastered = document.rasterize_page(page_number, dpi=120)\n\n# Split a pdf based on a page range\ndocument_2 = document.split(start=125, stop=130)\n</code></pre>"},{"location":"#performing-ocr","title":"Performing OCR","text":"<pre><code>from docprompt import load_document, DocumentNode\nfrom docprompt.tasks.ocr.gcp import GoogleOcrProvider\n\nprovider = GoogleOcrProvider.from_service_account_file(\n  project_id=my_project_id,\n  processor_id=my_processor_id,\n  service_account_file=path_to_service_file\n)\n\ndocument = load_document(\"path/to/my.pdf\")\n\n# A container holds derived data for a document, like OCR or classification results\ndocument_node = DocumentNode.from_document(document)\n\nprovider.process_document_node(document_node) # Caches results on the document_node\n\ndocument_node[0].ocr_result # Access OCR results\n</code></pre>"},{"location":"#document-search","title":"Document Search","text":"<p>When a large language model returns a result, we might want to highlight that result for our users. However, language models return results as text, while what we need to show our users requires a page number and a bounding box.</p> <p>After extracting text from a PDF, we can support this pattern using <code>DocumentProvenanceLocator</code>, which lives on a <code>DocumentNode</code></p> <pre><code>from docprompt import load_document, DocumentNode\nfrom docprompt.tasks.ocr.gcp import GoogleOcrProvider\n\nprovider = GoogleOcrProvider.from_service_account_file(\n  project_id=my_project_id,\n  processor_id=my_processor_id,\n  service_account_file=path_to_service_file\n)\n\ndocument = load_document(\"path/to/my.pdf\")\n\n# A container holds derived data for a document, like OCR or classification results\ndocument_node = DocumentNode.from_document(document)\n\nprovider.process_document_node(document_node) # Caches results on the document_node\n\n# With OCR results available, we can now instantiate a locator and search through documents.\n\ndocument_node.locator.search(\"John Doe\") # This will return a list of all terms across the document that contain \"John Doe\"\ndocument_node.locator.search(\"Jane Doe\", page_number=4) # Just return results a list of matching results from page 4\n</code></pre> <p>This functionality uses a combination of <code>rtree</code> and the Rust library <code>tantivy</code>, allowing you to perform thousands of searches in seconds </p>"},{"location":"enterprise/","title":"Enterprise","text":"<p>For companies looking to unlock data, build custom language models, or for general professional support</p> <p>Talk to founders</p> <p>This covers:</p> <ul> <li>\u2705 Assistance with PDF-optimized prompt engineering for Document AI tasks</li> <li>\u2705 Feature Prioritization</li> <li>\u2705 Custom Integrations</li> <li>\u2705 Professional Support - Dedicated discord + slack</li> </ul>"},{"location":"enterprise/#what-topics-does-professional-support-cover","title":"What topics does Professional support cover?","text":"<p>The expertise we've developed during our time building medical processing pipelines has equipped us with the tools and knowhow needed to perform highly accurate information extraction in document-heavy domains. We offer consulting services that leverage this expertise to assist with prompt engineering, deployments, and general ML-ops in the Document AI space.</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"concepts/nodes/","title":"Nodes in Docprompt","text":""},{"location":"concepts/nodes/#overview","title":"Overview","text":"<p>In Docprompt, nodes are fundamental structures used to represent and manage documents and their pages. They provide a way to store state and metadata associated with documents and individual pages, enabling advanced document analysis and processing capabilities.</p>"},{"location":"concepts/nodes/#key-concepts","title":"Key Concepts","text":""},{"location":"concepts/nodes/#documentnode","title":"DocumentNode","text":"<p>A <code>DocumentNode</code> represents a single document within the Docprompt system. It serves as a container for document-level metadata and provides access to individual pages through <code>PageNode</code> instances.</p> <pre><code>class DocumentNode(BaseModel, Generic[DocumentNodeMetadata, PageNodeMetadata]):\n    document: Document\n    page_nodes: List[PageNode[PageNodeMetadata]]\n    metadata: Optional[DocumentNodeMetadata]\n</code></pre> <p>Key features: - Stores a reference to the underlying <code>Document</code> object - Maintains a list of <code>PageNode</code> instances representing individual pages - Allows for custom document-level metadata - Provides access to a <code>DocumentProvenanceLocator</code> for efficient text search within the document</p>"},{"location":"concepts/nodes/#pagenode","title":"PageNode","text":"<p>A <code>PageNode</code> represents a single page within a document. It stores page-specific information and provides access to various analysis results, such as OCR data.</p> <pre><code>class PageNode(BaseModel, Generic[PageNodeMetadata]):\n    document: \"DocumentNode\"\n    page_number: PositiveInt\n    metadata: Optional[PageNodeMetadata]\n    extra: Dict[str, Any]\n    ocr_results: ResultContainer[OcrPageResult]\n</code></pre> <p>Key features: - References the parent <code>DocumentNode</code> - Stores the page number - Allows for custom page-level metadata - Provides a flexible <code>extra</code> field for additional data storage - Stores OCR results in a <code>ResultContainer</code></p>"},{"location":"concepts/nodes/#usage","title":"Usage","text":""},{"location":"concepts/nodes/#creating-a-documentnode","title":"Creating a DocumentNode","text":"<p>You can create a <code>DocumentNode</code> from a <code>Document</code> instance:</p> <pre><code>from docprompt import load_document, DocumentNode\n\ndocument = load_document(\"path/to/my.pdf\")\ndocument_node = DocumentNode.from_document(document)\n</code></pre>"},{"location":"concepts/nodes/#working-with-ocr-results","title":"Working with OCR Results","text":"<p>After processing a document with an OCR provider, you can access the results through the <code>DocumentNode</code> and <code>PageNode</code> structures:</p> <pre><code>from docprompt.tasks.ocr.gcp import GoogleOcrProvider\n\nprovider = GoogleOcrProvider.from_service_account_file(\n    project_id=my_project_id,\n    processor_id=my_processor_id,\n    service_account_file=path_to_service_file\n)\n\nprovider.process_document_node(document_node)\n\n# Access OCR results for a specific page\nocr_result = document_node.page_nodes[0].ocr_results\n</code></pre>"},{"location":"concepts/nodes/#using-documentprovenancelocator","title":"Using DocumentProvenanceLocator","text":"<p>The <code>DocumentProvenanceLocator</code> is a powerful tool for searching text within a document:</p> <pre><code># Search for text across the entire document\nresults = document_node.locator.search(\"John Doe\")\n\n# Search for text on a specific page\npage_results = document_node.locator.search(\"Jane Doe\", page_number=4)\n</code></pre>"},{"location":"concepts/nodes/#benefits-of-using-nodes","title":"Benefits of Using Nodes","text":"<ol> <li> <p>Separation of Concerns: Nodes allow you to separate the core PDF functionality (handled by the <code>Document</code> class) from additional metadata and analysis results.</p> </li> <li> <p>Flexible Metadata: Both <code>DocumentNode</code> and <code>PageNode</code> support generic metadata types, allowing you to add custom, type-safe metadata to your documents and pages.</p> </li> <li> <p>Result Caching: Nodes provide a convenient way to cache and access results from various analysis tasks, such as OCR.</p> </li> <li> <p>Efficient Text Search: The <code>DocumentProvenanceLocator</code> enables fast text search capabilities, leveraging OCR results for improved performance.</p> </li> <li> <p>Extensibility: The node structure allows for easy integration of new analysis tools and result types in the future.</p> </li> </ol> <p>By using the node structure in Docprompt, you can build powerful document analysis workflows that combine the core PDF functionality with advanced processing and search capabilities.</p>"},{"location":"concepts/primatives/","title":"Docprompt Primitives","text":"<p>Docprompt uses several primitive objects that are fundamental to its operation. These primitives are used throughout the library and are essential for understanding how Docprompt processes and represents documents.</p>"},{"location":"concepts/primatives/#pdfdocument","title":"PdfDocument","text":"<p>The <code>PdfDocument</code> class is a core primitive in Docprompt, representing a PDF document with various utilities for manipulation and analysis.</p> <pre><code>class PdfDocument(BaseModel):\n    name: str\n    file_bytes: bytes\n    file_path: Optional[str] = None\n</code></pre>"},{"location":"concepts/primatives/#key-features","title":"Key Features","text":"<ol> <li>Document Properties</li> <li><code>name</code>: The name of the document</li> <li><code>file_bytes</code>: The raw bytes of the PDF file</li> <li><code>file_path</code>: Optional path to the PDF file on disk</li> <li><code>page_count</code>: The number of pages in the document (computed field)</li> <li> <p><code>document_hash</code>: A unique hash of the document (computed field)</p> </li> <li> <p>Utility Methods</p> </li> <li><code>from_path(file_path)</code>: Create a PdfDocument from a file path</li> <li><code>from_bytes(file_bytes, name)</code>: Create a PdfDocument from bytes</li> <li><code>get_page_render_size(page_number, dpi)</code>: Get the render size of a specific page</li> <li><code>to_compressed_bytes()</code>: Compress the PDF using Ghostscript</li> <li><code>rasterize_page(page_number, ...)</code>: Rasterize a specific page with various options</li> <li><code>rasterize_pdf(...)</code>: Rasterize the entire PDF</li> <li><code>split(start, stop)</code>: Split the PDF into a new document</li> <li><code>as_tempfile()</code>: Create a temporary file from the PDF</li> <li><code>write_to_path(path)</code>: Write the PDF to a specific path</li> </ol>"},{"location":"concepts/primatives/#usage-example","title":"Usage Example","text":"<pre><code>from docprompt import PdfDocument\n\n# Load a PDF\npdf = PdfDocument.from_path(\"path/to/document.pdf\")\n\n# Get document properties\nprint(f\"Document name: {pdf.name}\")\nprint(f\"Page count: {pdf.page_count}\")\n\n# Rasterize a page\npage_image = pdf.rasterize_page(1, dpi=300)\n\n# Split the document\nnew_pdf = pdf.split(start=5, stop=10)\n</code></pre>"},{"location":"concepts/primatives/#layout-primitives","title":"Layout Primitives","text":"<p>Docprompt uses several layout primitives to represent the structure and content of documents.</p>"},{"location":"concepts/primatives/#normbbox","title":"NormBBox","text":"<p><code>NormBBox</code> represents a normalized bounding box with values between 0 and 1.</p> <pre><code>class NormBBox(BaseModel):\n    x0: BoundedFloat\n    top: BoundedFloat\n    x1: BoundedFloat\n    bottom: BoundedFloat\n</code></pre> <p>Key features: - Intersection operations (<code>__and__</code>) - Union operations (<code>__add__</code>) - Intersection over Union (IoU) calculation - Area and centroid properties</p>"},{"location":"concepts/primatives/#textblock","title":"TextBlock","text":"<p><code>TextBlock</code> represents a block of text within a document, including its bounding box and metadata.</p> <pre><code>class TextBlock(BaseModel):\n    text: str\n    type: SegmentLevels\n    source: TextblockSource\n    bounding_box: NormBBox\n    bounding_poly: Optional[BoundingPoly]\n    text_spans: Optional[List[TextSpan]]\n    metadata: Optional[TextBlockMetadata]\n</code></pre>"},{"location":"concepts/primatives/#point-and-boundingpoly","title":"Point and BoundingPoly","text":"<p><code>Point</code> and <code>BoundingPoly</code> are used to represent more complex shapes within a document.</p> <pre><code>class Point(BaseModel):\n    x: BoundedFloat\n    y: BoundedFloat\n\nclass BoundingPoly(BaseModel):\n    normalized_vertices: List[Point]\n</code></pre>"},{"location":"concepts/primatives/#textspan","title":"TextSpan","text":"<p><code>TextSpan</code> represents a span of text within a document or page.</p> <pre><code>class TextSpan(BaseModel):\n    start_index: int\n    end_index: int\n    level: Literal[\"page\", \"document\"]\n</code></pre>"},{"location":"concepts/primatives/#usage-example_1","title":"Usage Example","text":"<pre><code>from docprompt.schema.layout import NormBBox, TextBlock, TextBlockMetadata\n\n# Create a bounding box\nbbox = NormBBox(x0=0.1, top=0.1, x1=0.9, bottom=0.2)\n\n# Create a text block\ntext_block = TextBlock(\n    text=\"Example text\",\n    type=\"block\",\n    source=\"ocr\",\n    bounding_box=bbox,\n    metadata=TextBlockMetadata(confidence=0.95)\n)\n\n# Use the text block\nprint(f\"Text: {text_block.text}\")\nprint(f\"Bounding box: {text_block.bounding_box}\")\nprint(f\"Confidence: {text_block.confidence}\")\n</code></pre> <p>These primitives form the foundation of Docprompt's document processing capabilities, allowing for precise representation and manipulation of document content and structure.</p>"},{"location":"concepts/provenance/","title":"Provenance in Docprompt","text":""},{"location":"concepts/provenance/#overview","title":"Overview","text":"<p>Provenance in Docprompt refers to the ability to trace and locate specific pieces of text within a document. The <code>DocumentProvenanceLocator</code> class is a powerful tool that enables efficient text search, spatial queries, and fine-grained text location within documents that have been processed with OCR.</p>"},{"location":"concepts/provenance/#key-concepts","title":"Key Concepts","text":""},{"location":"concepts/provenance/#documentprovenancelocator","title":"DocumentProvenanceLocator","text":"<p>The <code>DocumentProvenanceLocator</code> is a class that provides advanced search capabilities for documents in Docprompt. It combines full-text search with spatial indexing to offer fast and accurate text location services.</p> <pre><code>@dataclass\nclass DocumentProvenanceLocator:\n    document_name: str\n    search_index: \"tantivy.Index\"\n    block_mapping: Dict[int, OcrPageResult]\n    geo_index: DocumentProvenanceGeoMap\n</code></pre> <p>Key features: - Full-text search using the Tantivy search engine - Spatial indexing using R-tree for efficient bounding box queries - Support for different granularity levels (word, line, block) - Ability to refine search results to word-level precision</p>"},{"location":"concepts/provenance/#main-functionalities","title":"Main Functionalities","text":""},{"location":"concepts/provenance/#1-text-search","title":"1. Text Search","text":"<p>The <code>search</code> method allows you to find specific text within a document:</p> <pre><code>def search(\n    self,\n    query: str,\n    page_number: Optional[int] = None,\n    *,\n    refine_to_word: bool = True,\n    require_exact_match: bool = True\n) -&gt; List[ProvenanceSource]:\n    # ... implementation ...\n</code></pre> <p>This method returns a list of <code>ProvenanceSource</code> objects, which contain detailed information about where the text was found, including page number, bounding box, and the surrounding context.</p>"},{"location":"concepts/provenance/#2-spatial-queries","title":"2. Spatial Queries","text":"<p>The <code>DocumentProvenanceLocator</code> supports spatial queries to find text blocks based on their location on the page:</p> <pre><code>def get_k_nearest_blocks(\n    self,\n    bbox: NormBBox,\n    page_number: int,\n    k: int,\n    granularity: BlockGranularity = \"block\"\n) -&gt; List[TextBlock]:\n    # ... implementation ...\n\ndef get_overlapping_blocks(\n    self,\n    bbox: NormBBox,\n    page_number: int,\n    granularity: BlockGranularity = \"block\"\n) -&gt; List[TextBlock]:\n    # ... implementation ...\n</code></pre> <p>These methods allow you to find text blocks that are near or overlapping with a given bounding box on a specific page.</p>"},{"location":"concepts/provenance/#usage","title":"Usage","text":""},{"location":"concepts/provenance/#recommended-usage-through-documentnode","title":"Recommended Usage: Through DocumentNode","text":"<p>The recommended way to use the <code>DocumentProvenanceLocator</code> is through the <code>DocumentNode</code> class. The <code>DocumentNode</code> provides two methods for working with the locator:</p> <ol> <li><code>locator</code> property: Lazily creates and returns the <code>DocumentProvenanceLocator</code>.</li> <li><code>refresh_locator()</code> method: Explicitly refreshes the locator for the document node.</li> </ol> <p>Here's how to use these methods:</p> <pre><code>from docprompt import load_document, DocumentNode\nfrom docprompt.tasks.ocr.gcp import GoogleOcrProvider\n\n# Load and process the document\ndocument = load_document(\"path/to/my.pdf\")\ndocument_node = DocumentNode.from_document(document)\n\n# Process the document with OCR\nprovider = GoogleOcrProvider.from_service_account_file(...)\nprovider.process_document_node(document_node)\n\n# Access the locator (creates it if it doesn't exist)\nlocator = document_node.locator\n\n# Perform a search\nresults = locator.search(\"Docprompt\")\n\n# If you need to refresh the locator (e.g., after updating OCR results)\ndocument_node.refresh_locator()\n</code></pre> <p>Note: Attempting to access the locator before OCR results are available will raise a <code>ValueError</code>.</p>"},{"location":"concepts/provenance/#alternative-standalone-usage","title":"Alternative: Standalone Usage","text":"<p>While the recommended approach is to use the locator through <code>DocumentNode</code>, you can also create and use a <code>DocumentProvenanceLocator</code> independently:</p> <pre><code>from docprompt.provenance.search import DocumentProvenanceLocator\n\n# Assuming you have a processed DocumentNode\nlocator = DocumentProvenanceLocator.from_document_node(document_node)\n\n# Now you can use the locator directly\nresults = locator.search(\"Docprompt\")\n</code></pre>"},{"location":"concepts/provenance/#searching-for-text","title":"Searching for Text","text":"<p>To search for text within the document:</p> <pre><code>results = locator.search(\"Docprompt\")\nfor result in results:\n    print(f\"Found on page {result.page_number}, bbox: {result.text_location.merged_source_block.bounding_box}\")\n</code></pre>"},{"location":"concepts/provenance/#performing-spatial-queries","title":"Performing Spatial Queries","text":"<p>To find text blocks near a specific location:</p> <pre><code>bbox = NormBBox(x0=0.1, y0=0.1, x1=0.2, y1=0.2)\nnearby_blocks = locator.get_k_nearest_blocks(bbox, page_number=1, k=5)\n</code></pre>"},{"location":"concepts/provenance/#benefits-of-using-provenance","title":"Benefits of Using Provenance","text":"<ol> <li>Accurate Text Location: Quickly find the exact location of text within a document, including page number and bounding box.</li> <li>Efficient Searching: Combine full-text search with spatial indexing for fast and accurate results.</li> <li>Flexible Granularity: Search and retrieve results at different levels of granularity (word, line, block).</li> <li>Integration with OCR: Seamlessly works with OCR results to provide comprehensive document analysis capabilities.</li> <li>Support for Complex Queries: Perform spatial queries to find text based on location within pages.</li> <li>Easy Access: Conveniently access the locator through the <code>DocumentNode</code> class, ensuring it's always available when needed.</li> </ol> <p>By leveraging the provenance functionality in Docprompt, you can build sophisticated document analysis workflows that require precise text location and contextual information retrieval.</p>"},{"location":"concepts/providers/","title":"Providers in Docprompt","text":""},{"location":"concepts/providers/#overview","title":"Overview","text":"<p>Providers in Docprompt are abstract interfaces that define how to add data to document nodes. They encapsulate various tasks such as OCR, classification, and more. The provider system is designed to be extensible, allowing users to create custom providers to add new functionality to Docprompt.</p>"},{"location":"concepts/providers/#key-concepts","title":"Key Concepts","text":""},{"location":"concepts/providers/#abstracttaskprovider","title":"AbstractTaskProvider","text":"<p>The <code>AbstractTaskProvider</code> is the base class for all providers in Docprompt. It defines the interface that all task providers must implement.</p> <pre><code>class AbstractTaskProvider(Generic[PageTaskResult]):\n    name: str\n    capabilities: List[str]\n\n    def process_document_pages(\n        self,\n        document: Document,\n        start: Optional[int] = None,\n        stop: Optional[int] = None,\n        **kwargs,\n    ) -&gt; Dict[int, PageTaskResult]:\n        raise NotImplementedError\n\n    def contribute_to_document_node(\n        self,\n        document_node: \"DocumentNode\",\n        results: Dict[int, PageTaskResult],\n    ) -&gt; None:\n        pass\n\n    def process_document_node(\n        self,\n        document_node: \"DocumentNode\",\n        start: Optional[int] = None,\n        stop: Optional[int] = None,\n        contribute_to_document: bool = True,\n        **kwargs,\n    ) -&gt; Dict[int, PageTaskResult]:\n        # ... implementation ...\n</code></pre> <p>Key features: - Generic type <code>PageTaskResult</code> allows for type-safe results - <code>capabilities</code> list defines what the provider can do - <code>process_document_pages</code> method processes pages of a document - <code>contribute_to_document_node</code> method adds results to a <code>DocumentNode</code> - <code>process_document_node</code> method combines processing and contributing results</p>"},{"location":"concepts/providers/#capabilities","title":"CAPABILITIES","text":"<p>The <code>CAPABILITIES</code> enum defines the various capabilities that a provider can have:</p> <pre><code>class CAPABILITIES(Enum):\n    PAGE_RASTERIZATION = \"page-rasterization\"\n    PAGE_LAYOUT_OCR = \"page-layout-ocr\"\n    PAGE_TEXT_OCR = \"page-text-ocr\"\n    PAGE_CLASSIFICATION = \"page-classification\"\n    PAGE_SEGMENTATION = \"page-segmentation\"\n    PAGE_VQA = \"page-vqa\"\n    PAGE_TABLE_IDENTIFICATION = \"page-table-identification\"\n    PAGE_TABLE_EXTRACTION = \"page-table-extraction\"\n</code></pre>"},{"location":"concepts/providers/#resultcontainer","title":"ResultContainer","text":"<p>The <code>ResultContainer</code> is a generic class that holds the results of a task:</p> <pre><code>class ResultContainer(BaseModel, Generic[PageOrDocumentTaskResult]):\n    results: Dict[str, PageOrDocumentTaskResult] = Field(\n        description=\"The results of the task, keyed by provider\", default_factory=dict\n    )\n\n    @property\n    def result(self):\n        return next(iter(self.results.values()), None)\n</code></pre>"},{"location":"concepts/providers/#creating-custom-providers","title":"Creating Custom Providers","text":"<p>To extend Docprompt's functionality, you can create custom providers. Here's an shortened example of a builtin OCR provider from GCP:</p> <pre><code>from docprompt.tasks.base import AbstractTaskProvider, CAPABILITIES\nfrom docprompt.schema.layout import TextBlock\nfrom pydantic import Field\n\nclass OcrPageResult(BasePageResult):\n    page_text: str = Field(description=\"The text for the entire page in reading order\")\n    word_level_blocks: List[TextBlock] = Field(default_factory=list)\n    line_level_blocks: List[TextBlock] = Field(default_factory=list)\n    block_level_blocks: List[TextBlock] = Field(default_factory=list)\n    raster_image: Optional[bytes] = Field(default=None)\n\nclass GoogleOcrProvider(AbstractTaskProvider[OcrPageResult]):\n    name = \"Google Document AI\"\n    capabilities = [\n        CAPABILITIES.PAGE_TEXT_OCR.value,\n        CAPABILITIES.PAGE_LAYOUT_OCR.value,\n        CAPABILITIES.PAGE_RASTERIZATION.value,\n    ]\n\n    def process_document_pages(\n        self,\n        document: Document,\n        start: Optional[int] = None,\n        stop: Optional[int] = None,\n        **kwargs,\n    ) -&gt; Dict[int, OcrPageResult]:\n        # Implement OCR logic here\n        pass\n\n    def contribute_to_document_node(\n        self,\n        document_node: \"DocumentNode\",\n        results: Dict[int, OcrPageResult],\n    ) -&gt; None:\n        # Add OCR results to document node\n        pass\n</code></pre>"},{"location":"concepts/providers/#usage","title":"Usage","text":"<p>Here's how you can use a provider in your Docprompt workflow:</p> <pre><code>from docprompt import load_document, DocumentNode\nfrom docprompt.providers.ocr import GoogleOcrProvider\n\n# Load a document\ndocument = load_document(\"path/to/my.pdf\")\ndocument_node = DocumentNode.from_document(document)\n\n# Create and use the OCR provider\nocr_provider = GoogleOcrProvider(...)\nocr_results = ocr_provider.process_document_node(document_node)\n\n# Access OCR results\nfor page_number, result in ocr_results.items():\n    print(f\"Page {page_number} text: {result.page_text[:100]}...\")\n</code></pre>"},{"location":"concepts/providers/#benefits-of-using-providers","title":"Benefits of Using Providers","text":"<ol> <li>Extensibility: Easily add new functionality to Docprompt by creating custom providers.</li> <li>Modularity: Each provider encapsulates a specific task, making the codebase more organized and maintainable.</li> <li>Type Safety: Generic types ensure that providers produce and consume the correct types of results.</li> <li>Standardized Interface: All providers follow the same interface, making it easy to switch between different implementations.</li> <li>Capability-based Design: Providers declare their capabilities, allowing for dynamic feature discovery and usage.</li> </ol> <p>By leveraging the provider system in Docprompt, you can create flexible and powerful document processing pipelines that can be easily extended and customized to meet your specific needs.</p>"},{"location":"guide/classify/binary/","title":"Binary Classification with DocPrompt","text":"<p>Binary classification is a fundamental task in document analysis, where you categorize pages into one of two classes. This guide will walk you through performing binary classification using DocPrompt with the Anthropic provider.</p>"},{"location":"guide/classify/binary/#setting-up","title":"Setting Up","text":"<p>First, let's import the necessary modules and set up our environment:</p> <pre><code>from docprompt import load_document, DocumentNode\nfrom docprompt.tasks.factory import AnthropicTaskProviderFactory\nfrom docprompt.tasks.classification import ClassificationInput, ClassificationTypes\n\n# Initialize the Anthropic factory\n# Make sure you have set the ANTHROPIC_API_KEY environment variable\nfactory = AnthropicTaskProviderFactory()\n\n# Create the classification provider\nclassification_provider = factory.get_page_classification_provider()\n</code></pre>"},{"location":"guide/classify/binary/#preparing-the-document","title":"Preparing the Document","text":"<p>Load your document and create a DocumentNode:</p> <pre><code>document = load_document(\"path/to/your/document.pdf\")\ndocument_node = DocumentNode.from_document(document)\n</code></pre>"},{"location":"guide/classify/binary/#configuring-the-classification-task","title":"Configuring the Classification Task","text":"<p>For binary classification, we need to create a <code>ClassificationInput</code> object. This object acts as a prompt for the model, guiding its classification decision. Here's how to set it up:</p> <pre><code>classification_input = ClassificationInput(\n    type=ClassificationTypes.BINARY,\n    instructions=\"Determine if the page contains information about financial transactions.\",\n    confidence=True  # Optional: request confidence scores\n)\n</code></pre> <p>Let's break down the <code>ClassificationInput</code>:</p> <ul> <li><code>type</code>: Set to <code>ClassificationTypes.BINARY</code> for binary classification.</li> <li><code>instructions</code>: This is crucial for binary classification. Provide clear, specific instructions for what the model should look for.</li> <li><code>confidence</code>: Set to <code>True</code> if you want confidence scores with the results.</li> </ul> <p>Note: For binary classification, you don't need to specify labels. The model will automatically use \"YES\" and \"NO\" as labels.</p>"},{"location":"guide/classify/binary/#performing-classification","title":"Performing Classification","text":"<p>Now, let's run the classification task:</p> <pre><code>results = classification_provider.process_document_node(\n    document_node,\n    classification_input\n)\n</code></pre>"},{"location":"guide/classify/binary/#interpreting-results","title":"Interpreting Results","text":"<p>The <code>results</code> dictionary contains the classification output for each page. Let's examine the results:</p> <pre><code>for page_number, result in results.items():\n    label = result.labels\n    confidence = result.score\n    print(f\"Page {page_number}:\")\n    print(f\"\\tClassification: {label} ({confidence})\")\n    print('---')\n</code></pre> <p>This will print the classification (YES or NO) for each page, along with the confidence level if requested.</p>"},{"location":"guide/classify/binary/#tips-for-effective-binary-classification","title":"Tips for Effective Binary Classification","text":"<ol> <li> <p>Clear Instructions: The <code>instructions</code> field in <code>ClassificationInput</code> is crucial. Be specific about what constitutes a \"YES\" classification.</p> </li> <li> <p>Consider Page Context: Remember that the model analyzes each page independently. If your classification requires context from multiple pages, you may need to adjust your approach.</p> </li> <li> <p>Confidence Scores: Use the <code>confidence</code> option to get an idea of how certain the model is about its classifications. This can be helpful for identifying pages that might need human review.</p> </li> <li> <p>Iterative Refinement: If you're not getting the desired results, try refining your instructions. You might need to be more specific or provide examples of what constitutes a positive classification.</p> </li> </ol>"},{"location":"guide/classify/binary/#conclusion","title":"Conclusion","text":"<p>Binary classification with DocPrompt allows you to quickly categorize pages in your documents. By leveraging the Anthropic provider and carefully crafting your <code>ClassificationInput</code>, you can achieve accurate and efficient document analysis.</p> <p>Remember that the quality of your results heavily depends on the clarity and specificity of your instructions. Experiment with different phrasings to find what works best for your specific use case.</p>"},{"location":"guide/classify/multi/","title":"Multi-Label Classification with DocPrompt: Academic Research Papers","text":"<p>In this guide, we'll use DocPrompt to classify academic research papers in a PDF document into multiple relevant fields. We'll use multi-label classification, allowing each page (paper) to be assigned to one or more categories based on its content.</p>"},{"location":"guide/classify/multi/#setting-up","title":"Setting Up","text":"<p>First, let's import the necessary modules and set up our environment:</p> <pre><code>from docprompt import load_document, DocumentNode\nfrom docprompt.tasks.factory import AnthropicTaskProviderFactory\nfrom docprompt.tasks.classification import ClassificationInput, ClassificationTypes\n\n# Initialize the Anthropic factory\n# Ensure you have set the ANTHROPIC_API_KEY environment variable\nfactory = AnthropicTaskProviderFactory()\n\n# Create the classification provider\nclassification_provider = factory.get_page_classification_provider()\n</code></pre>"},{"location":"guide/classify/multi/#preparing-the-document","title":"Preparing the Document","text":"<p>Load your collection of research papers and create a DocumentNode:</p> <pre><code>document = load_document(\"path/to/your/research_papers.pdf\")\ndocument_node = DocumentNode.from_document(document)\n</code></pre>"},{"location":"guide/classify/multi/#configuring-the-classification-task","title":"Configuring the Classification Task","text":"<p>For multi-label classification, we'll create a <code>ClassificationInput</code> object that specifies our labels and provides instructions for the model:</p> <pre><code>classification_input = ClassificationInput(\n    type=ClassificationTypes.MULTI_LABEL,\n    instructions=(\n        \"Classify the research paper on this page into one or more relevant fields \"\n        \"based on its title, abstract, methodology, and key findings. A paper may \"\n        \"belong to multiple categories if it spans multiple disciplines.\"\n    ),\n    labels=[\n        \"Machine Learning\",\n        \"Natural Language Processing\",\n        \"Computer Vision\",\n        \"Robotics\",\n        \"Data Mining\",\n        \"Cybersecurity\",\n        \"Bioinformatics\",\n        \"Quantum Computing\"\n    ],\n    descriptions=[\n        \"Algorithms and statistical models that computer systems use to perform tasks without explicit instructions\",\n        \"Processing and analyzing natural language data\",\n        \"Enabling computers to derive meaningful information from digital images, videos and other visual inputs\",\n        \"Design, construction, operation, and use of robots\",\n        \"Process of discovering patterns in large data sets\",\n        \"Protection of computer systems from theft or damage to their hardware, software, or electronic data\",\n        \"Application of computational techniques to analyze biological data\",\n        \"Computation based on the principles of quantum theory\"\n    ],\n    confidence=True  # Request confidence scores\n)\n</code></pre> <p>Let's break down the <code>ClassificationInput</code>:</p> <ul> <li><code>type</code>: Set to <code>ClassificationTypes.MULTI_LABEL</code> for multi-label classification.</li> <li><code>labels</code>: List of possible categories for our research papers.</li> <li><code>instructions</code>: Clear directions for the model on how to classify the papers, emphasizing that multiple labels can be assigned.</li> <li><code>descriptions</code>: Provide additional context for each label to improve classification accuracy.</li> <li><code>confidence</code>: Set to <code>True</code> to get confidence scores with the results.</li> </ul>"},{"location":"guide/classify/multi/#performing-classification","title":"Performing Classification","text":"<p>Now, let's run the classification task on our collection of research papers:</p> <pre><code>results = classification_provider.process_document_node(\n    document_node,\n    classification_input\n)\n</code></pre>"},{"location":"guide/classify/multi/#interpreting-results","title":"Interpreting Results","text":"<p>Let's examine the classification results for each research paper:</p> <pre><code>for page_number, result in results.items():\n    categories = result.labels\n    confidence = result.score\n    print(f\"Research Paper on Page {page_number}:\")\n    print(f\"\\tCategories: {', '.join(categories)}  ({confidence})\")\n    print('---')\n</code></pre> <p>This will print the assigned categories for each research paper, along with the confidence level.</p>"},{"location":"guide/classify/multi/#tips-for-effective-multi-label-classification","title":"Tips for Effective Multi-Label Classification","text":"<ol> <li> <p>Comprehensive Label Set: Ensure your label set covers the main topics in your domain but isn't so large that it becomes unwieldy.</p> </li> <li> <p>Clear Instructions: Emphasize in your instructions that multiple labels can and should be assigned when appropriate.</p> </li> <li> <p>Use Descriptions: The <code>descriptions</code> field helps the model understand the nuances of each category, which is especially important for interdisciplinary papers.</p> </li> <li> <p>Consider Confidence Scores: In multi-label classification, confidence scores can indicate how strongly a paper fits into each assigned category.</p> </li> <li> <p>Analyze Label Co-occurrences: Look for patterns in which labels frequently appear together to gain insights into interdisciplinary trends.</p> </li> <li> <p>Handle Outliers: If a paper doesn't fit well into any category, consider adding a catch-all category like \"Other\" or \"Interdisciplinary\" in future iterations.</p> </li> </ol>"},{"location":"guide/classify/multi/#advanced-usage-increasing-the-power","title":"Advanced Usage: Increasing the Power","text":"<p>For more control over the classification process, you can specify a beefier model from Anthropic to up the reasoning power. This can be done when setting up the task provider OR at inference time. Allowing for easy, fine-grained control of over provider defaults and runtime overrides.</p> <pre><code>classification_provider = factory.get_page_classification_provider(\n    model_name=\"claude-3-5-sonnet-20240620\"  # setup the task provider with sonnet-3.5\n)\n\nresults = classification_provider.process_document_node(\n    document_node,\n    classification_input,\n    model_name=\"claude-3-5-sonnet-20240620\" # or you can declare model name at inference time as well\n)\n</code></pre>"},{"location":"guide/classify/multi/#conclusion","title":"Conclusion","text":"<p>Multi-label classification with DocPrompt provides a powerful way to categorize complex documents like research papers that often span multiple disciplines. By carefully crafting your <code>ClassificationInput</code> with clear labels, instructions, and descriptions, you can achieve nuanced and informative document analysis.</p> <p>Remember that the quality of your results depends on the clarity of your instructions, the comprehensiveness of your label set, and the appropriateness of your descriptions. Experiment with different configurations to find what works best for your specific use case.</p> <p>This approach can be adapted to other multi-label classification tasks, such as categorizing news articles by multiple topics, classifying products by multiple features, or tagging images with multiple attributes.</p>"},{"location":"guide/classify/single/","title":"Single-Label Classification with DocPrompt: Recipe Categories","text":"<p>In this guide, we'll use DocPrompt to classify recipes in a PDF document into distinct meal categories. We'll use single-label classification, meaning each page (recipe) will be assigned to one category: Breakfast, Lunch, Dinner, or Dessert.</p>"},{"location":"guide/classify/single/#setting-up","title":"Setting Up","text":"<p>First, let's import the necessary modules and set up our environment:</p> <pre><code>from docprompt import load_document, DocumentNode\nfrom docprompt.tasks.factory import AnthropicTaskProviderFactory\nfrom docprompt.tasks.classification import ClassificationInput, ClassificationTypes\n\n# Initialize the Anthropic factory\n# Ensure you have set the ANTHROPIC_API_KEY environment variable\nfactory = AnthropicTaskProviderFactory()\n\n# Create the classification provider\nclassification_provider = factory.get_page_classification_provider()\n</code></pre>"},{"location":"guide/classify/single/#preparing-the-document","title":"Preparing the Document","text":"<p>Load your recipe book PDF and create a DocumentNode:</p> <pre><code>document = load_document(\"path/to/your/recipe_book.pdf\")\ndocument_node = DocumentNode.from_document(document)\n</code></pre>"},{"location":"guide/classify/single/#configuring-the-classification-task","title":"Configuring the Classification Task","text":"<p>For single-label classification, we'll create a <code>ClassificationInput</code> object that specifies our labels and provides instructions for the model:</p> <pre><code>classification_input = ClassificationInput(\n    type=ClassificationTypes.SINGLE_LABEL,\n    labels=[\"Breakfast\", \"Lunch\", \"Dinner\", \"Dessert\"],\n    instructions=\"Classify the recipe on this page into one of the given meal categories based on its ingredients, cooking methods, and typical serving time.\",\n    descriptions=[\n        \"Morning meals, often including eggs, cereals, or pastries\",\n        \"Midday meals, typically lighter fare like sandwiches or salads\",\n        \"Evening meals, often the most substantial meal of the day\",\n        \"Sweet treats typically served after a meal or as a snack\"\n    ],\n    confidence=True  # Request confidence scores\n)\n</code></pre> <p>Let's break down the <code>ClassificationInput</code>:</p> <ul> <li><code>type</code>: Set to <code>ClassificationTypes.SINGLE_LABEL</code> for single-label classification.</li> <li><code>labels</code>: List of possible categories for our recipes.</li> <li><code>instructions</code>: Clear directions for the model on how to classify the recipes.</li> <li><code>descriptions</code>: (Optional) Provide additional context for each label to improve classification accuracy.<ul> <li>Note that the <code>descriptions</code> array must be the same length as the <code>labels</code> array.</li> </ul> </li> <li><code>confidence</code>: Set to <code>True</code> to get confidence scores with the results.</li> </ul>"},{"location":"guide/classify/single/#performing-classification","title":"Performing Classification","text":"<p>Now, let's run the classification task on our recipe book:</p> <pre><code>results = classification_provider.process_document_node(\n    document_node,\n    classification_input\n)\n</code></pre>"},{"location":"guide/classify/single/#interpreting-results","title":"Interpreting Results","text":"<p>Let's examine the classification results for each recipe:</p> <pre><code>for page_number, result in results.items():\n    category = result.labels\n    confidence = result.score\n\n    print(f\"Recipe on Page {page_number}:\")\n    print(f\"\\tCategory: {category} ({confidence})\")\n    print('---')\n</code></pre> <p>This will print the assigned category (Breakfast, Lunch, Dinner, or Dessert) for each recipe, along with the confidence level.</p>"},{"location":"guide/classify/single/#tips-for-effective-single-label-classification","title":"Tips for Effective Single-Label Classification","text":"<ol> <li> <p>Comprehensive Labels: Ensure your label set covers all possible categories without overlap.</p> </li> <li> <p>Clear Instructions: Provide specific criteria for each category in your instructions. For recipes, mention considering ingredients, cooking methods, and typical serving times.</p> </li> <li> <p>Use Descriptions: The <code>descriptions</code> field can help the model understand nuances between categories, especially for edge cases like brunch recipes.</p> </li> <li> <p>Consider Confidence Scores: Low confidence scores might indicate recipes that don't clearly fit into one category, such as versatile dishes that could be served at multiple meals.</p> </li> <li> <p>Handling Edge Cases: If you encounter many low-confidence classifications, you might need to refine your categories or instructions. For example, you might add an \"Anytime\" category for versatile recipes.</p> </li> </ol>"},{"location":"guide/classify/single/#advanced-usage-customizing-the-model","title":"Advanced Usage: Customizing the Model","text":"<p>If you need to experiment with different LLM models, based on the complexity of your task, you may control the model_name parameter to the classification provider:</p> <pre><code>haiku_classification_provider = factory.get_page_classification_provider(\n    model_name=\"claude-3-haiku-20240307\"\n)\n\nsonnet_classification_provider = factory.get_page_classification_provider(\n    model_name=\"claude-3-5-sonnet-20240620\"\n)\n</code></pre>"},{"location":"guide/classify/single/#conclusion","title":"Conclusion","text":"<p>Single-label classification with DocPrompt provides a powerful way to categorize pages in your documents, such as recipes in a cookbook. By carefully crafting your <code>ClassificationInput</code> with clear labels, instructions, and descriptions, you can achieve accurate and efficient document analysis.</p> <p>Remember that the quality of your results depends on the clarity of your instructions and the appropriateness of your label set. Experiment with different phrasings and label combinations to find what works best for your specific use case.</p> <p>This approach can be easily adapted to other single-label classification tasks, such as categorizing scientific papers by field, sorting legal documents by type, or classifying news articles by topic.</p>"},{"location":"guide/ocr/advanced_search/","title":"Lightning-Fast Document Search \ud83d\udd25\ud83d\ude80","text":"<p>Ever wished you could search through OCR-processed documents at the speed of light? Look no further! DocPrompt's Provenance Locator, powered by Rust, offers blazingly fast text search capabilities that will revolutionize your document processing workflows.</p>"},{"location":"guide/ocr/advanced_search/#the-power-of-rust-powered-search","title":"The Power of Rust-Powered Search","text":"<p>DocPrompt's <code>DocumentProvenanceLocator</code> is not your average search tool. Implemented in Rust and leveraging the power of <code>tantivy</code> and <code>rtree</code>, it provides:</p> <ul> <li>\u26a1 Lightning-fast full-text search</li> <li>\ud83c\udfaf Precise text location within documents</li> <li>\ud83e\udde0 Smart granularity refinement (word, line, block)</li> <li>\ud83d\uddfa\ufe0f Spatial querying capabilities</li> </ul> <p>Let's dive into how you can harness this power!</p>"},{"location":"guide/ocr/advanced_search/#setting-up-the-locator","title":"Setting Up the Locator","text":"<p>First, let's create a <code>DocumentProvenanceLocator</code> from a processed <code>DocumentNode</code>:</p> <pre><code>from docprompt import load_document, DocumentNode\nfrom docprompt.tasks.factory import GCPTaskProviderFactory\n\n# Load and process the document\ndocument = load_document(\"path/to/your/document.pdf\")\ndocument_node = DocumentNode.from_document(document)\n\n# Process with OCR (assuming you've set up the GCP factory)\ngcp_factory = GCPTaskProviderFactory(service_account_file=\"path/to/credentials.json\")\nocr_provider = gcp_factory.get_page_ocr_provider(\n    project_id=\"your-project-id\",\n    processor_id=\"your-processor-id\"\n)\nocr_results = ocr_provider.process_document_node(document_node)\n\n# Create the locator\nlocator = document_node.locator\n</code></pre>"},{"location":"guide/ocr/advanced_search/#searching-at-the-speed-of-rust","title":"Searching at the Speed of Rust \ud83d\udd25","text":"<p>Now that we have our locator, let's see it in action:</p> <pre><code># Perform a simple search\nresults = locator.search(\"DocPrompt\")\n\nfor result in results:\n    print(f\"Found on page {result.page_number}\")\n    print(f\"Text: {result.text}\")\n    print(f\"Bounding box: {result.text_location.merged_source_block.bounding_box}\")\n    print(\"---\")\n</code></pre> <p>This search operation happens in milliseconds, even for large documents, thanks to the Rust-powered backend!</p>"},{"location":"guide/ocr/advanced_search/#advanced-search-capabilities","title":"Advanced Search Capabilities","text":""},{"location":"guide/ocr/advanced_search/#refining-to-word-level","title":"Refining to Word Level","text":"<p>DocPrompt can automatically refine search results to the word level:</p> <pre><code>refined_results = locator.search(\"DocPrompt\", refine_to_word=True)\n</code></pre> <p>This gives you pinpoint accuracy in locating text within your document.</p>"},{"location":"guide/ocr/advanced_search/#page-specific-search","title":"Page-Specific Search","text":"<p>Need to search on a specific page? No problem:</p> <pre><code>page_5_results = locator.search(\"DocPrompt\", page_number=5)\n</code></pre>"},{"location":"guide/ocr/advanced_search/#best-match-search","title":"Best Match Search","text":"<p>Find the best matches based on different criteria:</p> <pre><code>best_short_matches = locator.search_n_best(\"DocPrompt\", n=3, mode=\"shortest_text\")\n\nbest_long_matches = locator.search_n_best(\"DocPrompt\", n=3, mode=\"longest_text\")\n\nbest_overall_matches = locator.search_n_best(\"DocPrompt\", n=3, mode=\"highest_score\")\n</code></pre>"},{"location":"guide/ocr/advanced_search/#spatial-queries-beyond-text-search","title":"Spatial Queries: Beyond Text Search \ud83d\uddfa\ufe0f","text":"<p>DocPrompt's locator isn't just fast\u2014it's spatially aware! You can perform queries based on document layout:</p> <pre><code>from docprompt.schema.layout import NormBBox\n\n# Get blocks near a specific area on page 1\nbbox = NormBBox(x0=0.1, top=0.1, x1=0.2, bottom=0.2)\nnearby_blocks = locator.get_k_nearest_blocks(bbox, page_number=1, k=5)\n\n# Get overlapping blocks\noverlapping_blocks = locator.get_overlapping_blocks(bbox, page_number=1)\n</code></pre> <p>This spatial awareness opens up possibilities for advanced document analysis and data extraction!</p>"},{"location":"guide/ocr/advanced_search/#conclusion-search-at-the-speed-of-thought","title":"Conclusion: Search at the Speed of Thought \ud83e\udde0\ud83d\udca8","text":"<p>DocPrompt's <code>DocumentProvenanceLocator</code> brings unprecedented speed and precision to document search and analysis. By leveraging the power of Rust, it offers:</p> <ol> <li>Lightning-fast full-text search</li> <li>Precise text location within documents</li> <li>Advanced spatial querying capabilities</li> <li>Scalability for large documents and datasets</li> </ol> <p>Whether you're building a document analysis pipeline, a search system, or any text-based application, DocPrompt's Provenance Locator offers the speed and accuracy you need to stay ahead of the game.</p>"},{"location":"guide/ocr/advanced_workflows/","title":"Advanced Document Analysis:","text":""},{"location":"guide/ocr/advanced_workflows/#detecting-potential-conflicts-of-interest-in-10-k-reports","title":"Detecting Potential Conflicts of Interest in 10-K Reports","text":"<p>In this guide, we'll demonstrate how to use DocPrompt's powerful OCR and search capabilities to analyze 10-K reports for potential conflicts of interest. We'll search for mentions of company names and executive names, then identify instances where they appear in close proximity within the document.</p>"},{"location":"guide/ocr/advanced_workflows/#setup","title":"Setup","text":"<p>First, let's set up our environment and process the document:</p> <pre><code>from docprompt import load_document, DocumentNode\nfrom docprompt.tasks.factory import GCPTaskProviderFactory\nfrom docprompt.schema.layout import NormBBox\nfrom itertools import product\n\n# Load and process the document\ndocument = load_document(\"path/to/10k_report.pdf\")\ndocument_node = DocumentNode.from_document(document)\n\n# Perform OCR\ngcp_factory = GCPTaskProviderFactory(service_account_file=\"path/to/credentials.json\")\nocr_provider = gcp_factory.get_page_ocr_provider(project_id=\"your-project-id\", processor_id=\"your-processor-id\")\nocr_provider.process_document_node(document_node)\n\n# Create the locator\nlocator = document_node.locator\n\n# Define entities to search for\ncompany_names = [\"SubsidiaryA\", \"PartnerB\", \"CompetitorC\"]\nexecutive_names = [\"John Doe\", \"Jane Smith\", \"Alice Johnson\"]\n</code></pre>"},{"location":"guide/ocr/advanced_workflows/#searching-for-entities","title":"Searching for Entities","text":"<p>Now, let's use DocPrompt's fast search capabilities to find all mentions of companies and executives. By leveraging the speed of the rust powered locator, along with python's builtin comprehension, we can execute our set of queries over the several-hundred page document in a matter of miliseconds.</p> <pre><code>company_results = {\n    company: locator.search(company)\n    for company in company_names\n}\n\nexecutive_results = {\n    executive: locator.search(executive)\n    for executive in executive_names\n}\n</code></pre>"},{"location":"guide/ocr/advanced_workflows/#detecting-proximity","title":"Detecting Proximity","text":"<p>Next, we'll check for instances where company names and executive names appear in close proximity:</p> <pre><code>def check_proximity(bbox1, bbox2, threshold=0.1):\n    left_collision = abs(bbox1.x0 - bbox2.x0) &lt; threshold\n    top_collision = abs(bbox1.top - bbox2.top) &lt; threshold\n\n    return left_collision and top_collision\n\npotential_conflicts = []\n\nfor company, exec_name in product(company_names, executive_names):\n    c_result = company_results[company]\n    e_result = exexecutive_results[exec_name]\n\n    # Check if the two results appear on the same page\n    if c_result.page_number == e_result.page_number:\n        c_bbox = c_result.text_location.merged_source_block.bounding_box\n        e_bbox = e_result.text_location.merged_source_block.bounding_box\n\n        # If they do, check if the bounding boxes break our threshold\n        if check_proximity(c_bbox, e_bbox):\n            potential_conflicts.append({\n                'company': company,\n                'executive': exec_name,\n                'page': c_result.page_number,\n                'company_bbox': c_bbox,\n                'exec_bbox': e_bbox\n            })\n</code></pre>"},{"location":"guide/ocr/advanced_workflows/#analyzing-results","title":"Analyzing Results","text":"<p>Finally, let's analyze and display our results:</p> <pre><code>print(f\"Found {len(potential_conflicts)} potential conflicts of interest:\")\n\nfor conflict in potential_conflicts:\n    print(f\"\\nPotential conflict on page {conflict['page']}:\")\n    print(f\"  Company: {conflict['company']}\")\n    print(f\"  Executive: {conflict['executive']}\")\n\n    # Get surrounding context\n    context_bbox = NormBBox(\n        x0=min(conflict['company_bbox'].x0, conflict['exec_bbox'].x0) - 0.05,\n        top=min(conflict['company_bbox'].top, conflict['exec_bbox'].top) - 0.05,\n        x1=max(conflict['company_bbox'].x1, conflict['exec_bbox'].x1) + 0.05,\n        bottom=max(conflict['company_bbox'].bottom, conflict['exec_bbox'].bottom) + 0.05\n    )\n\n    context_blocks = locator.get_overlapping_blocks(context_bbox, conflict['page'])\n\n    print(\"  Context:\")\n    for block in context_blocks:\n        print(f\"    {block.text}\")\n</code></pre> <p>This refined approach demonstrates several key features of DocPrompt:</p> <ol> <li> <p>Fast and Accurate Search: We use the <code>DocumentProvenanceLocator</code> to quickly find all mentions of companies and executives across the entire document.</p> </li> <li> <p>Spatial Analysis: By leveraging the bounding box information, we can determine when two entities are mentioned in close proximity on the page.</p> </li> <li> <p>Contextual Information: We use spatial queries to extract the surrounding text, providing context for each potential conflict of interest.</p> </li> <li> <p>Scalability: This approach can easily handle multiple companies and executives, making it suitable for analyzing large, complex documents.</p> </li> </ol> <p>By combining these capabilities, DocPrompt enables efficient and thorough analysis of 10-K reports, helping to identify potential conflicts of interest that might otherwise be overlooked in manual review processes.</p>"},{"location":"guide/ocr/basic_usage/","title":"Basic OCR Usage with Docprompt","text":"<p>This guide will walk you through the basics of performing Optical Character Recognition (OCR) using Docprompt. You'll learn how to set up the OCR provider, process a document, and access the results.</p>"},{"location":"guide/ocr/basic_usage/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have:</p> <ol> <li>Installed Docprompt with OCR support: <code>pip install \"docprompt[google]\"</code></li> <li>A Google Cloud Platform account with Document AI API enabled</li> <li>A GCP service account key file</li> </ol>"},{"location":"guide/ocr/basic_usage/#setting-up-the-ocr-provider","title":"Setting Up the OCR Provider","text":"<p>First, let's set up the Google OCR provider:</p> <pre><code>from docprompt.tasks.factory import GCPTaskProviderFactory\n\n# Initialize the GCP Task Provider Factory\ngcp_factory = GCPTaskProviderFactory(\n    service_account_file=\"path/to/your/service_account_key.json\"\n)\n\n# Create the OCR provider\nocr_provider = gcp_factory.get_page_ocr_provider(\n    project_id=\"your-gcp-project-id\",\n    processor_id=\"your-document-ai-processor-id\"\n)\n</code></pre>"},{"location":"guide/ocr/basic_usage/#loading-and-processing-a-document","title":"Loading and Processing a Document","text":"<p>Now, let's load a document and process it using OCR:</p> <pre><code>from docprompt import load_document, DocumentNode\n\n# Load the document\ndocument = load_document(\"path/to/your/document.pdf\")\ndocument_node = DocumentNode.from_document(document)\n\n# Process the document\nocr_results = ocr_provider.process_document_node(document_node)\n</code></pre>"},{"location":"guide/ocr/basic_usage/#accessing-ocr-results","title":"Accessing OCR Results","text":"<p>After processing, you can access the OCR results in various ways:</p>"},{"location":"guide/ocr/basic_usage/#1-page-level-text","title":"1. Page-level Text","text":"<p>To get the full text of a specific page:</p> <pre><code>page_number = 1  # Pages are 1-indexed\npage_text = ocr_results[page_number].page_text\nprint(f\"Text on page {page_number}:\\n{page_text[:500]}...\")  # Print first 500 characters\n</code></pre>"},{"location":"guide/ocr/basic_usage/#2-words-lines-and-blocks","title":"2. Words, Lines, and Blocks","text":"<p>Docprompt provides access to words, lines, and blocks (paragraphs) extracted from the document:</p> <pre><code># Get the first page's result\nfirst_page_result = ocr_results[1]\n\n# Print the first 5 words on the page\nprint(\"First 5 words:\")\nfor word in first_page_result.word_level_blocks[:5]:\n    print(f\"Word: {word.text}, Confidence: {word.metadata.confidence}\")\n\n# Print the first line on the page\nprint(\"\\nFirst line:\")\nif first_page_result.line_level_blocks:\n    first_line = first_page_result.line_level_blocks[0]\n    print(f\"Line: {first_line.text}\")\n\n# Print the first block (paragraph) on the page\nprint(\"\\nFirst block:\")\nif first_page_result.block_level_blocks:\n    first_block = first_page_result.block_level_blocks[0]\n    print(f\"Block: {first_block.text[:100]}...\")  # Print first 100 characters\n</code></pre>"},{"location":"guide/ocr/basic_usage/#3-bounding-boxes","title":"3. Bounding Boxes","text":"<p>Each word, line, and block comes with bounding box information:</p> <pre><code># Get bounding box for the first word\nif first_page_result.word_level_blocks:\n    first_word = first_page_result.word_level_blocks[0]\n    bbox = first_word.bounding_box\n    print(f\"\\nBounding box for '{first_word.text}':\")\n    print(f\"Top-left: ({bbox.x0}, {bbox.top})\")\n    print(f\"Bottom-right: ({bbox.x1}, {bbox.bottom})\")\n</code></pre>"},{"location":"guide/ocr/basic_usage/#conclusion","title":"Conclusion","text":"<p>You've now learned the basics of performing OCR with Docprompt. This includes setting up the OCR provider, processing a document, and accessing the results at different levels of granularity.</p> <p>For more advanced usage, including customizing OCR settings, using other providers, and leveraging the powerful search capabilities, check out our other guides:</p> <ul> <li>Customizing OCR Providers and Settings</li> <li>Advanced Text Search with Provenance Locators</li> <li>Building OCR-based Workflows</li> </ul>"},{"location":"guide/ocr/provider_config/","title":"Customizing OCR Providers","text":"<p>Docprompt uses a factory pattern to manage credentials and create task providers efficiently. This guide will demonstrate how to configure and customize OCR providers, focusing on Amazon Textract and Google Cloud Platform (GCP) as examples.</p>"},{"location":"guide/ocr/provider_config/#understanding-the-factory-pattern","title":"Understanding the Factory Pattern","text":"<p>Docprompt uses task provider factories to manage credentials and create providers for various tasks. This approach allows for:</p> <ol> <li>Centralized credential management</li> <li>Easy creation of multiple task providers from a single backend</li> <li>Separation of provider-specific and task-specific configurations</li> </ol> <p>Here's a simplified example of how the factory pattern works:</p> <pre><code>from docprompt.tasks.factory import GCPTaskProviderFactory, AmazonTaskProviderFactory\n\n# Create a GCP factory\ngcp_factory = GCPTaskProviderFactory(\n    service_account_file=\"path/to/service_account.json\"\n)\n\n# Create an Amazon factory\namazon_factory = AmazonTaskProviderFactory(\n    aws_access_key_id=\"YOUR_ACCESS_KEY\",\n    aws_secret_access_key=\"YOUR_SECRET_KEY\",\n    region_name=\"us-west-2\"\n)\n</code></pre>"},{"location":"guide/ocr/provider_config/#creating-ocr-providers","title":"Creating OCR Providers","text":"<p>Once you have a factory, you can create OCR providers with task-specific configurations:</p> <pre><code># Create a GCP OCR provider\ngcp_ocr_provider = gcp_factory.get_page_ocr_provider(\n    project_id=\"YOUR_PROJECT_ID\",\n    processor_id=\"YOUR_PROCESSOR_ID\",\n    max_workers=4,\n    return_images=True\n)\n\n# Create an Amazon Textract provider\namazon_ocr_provider = amazon_factory.get_page_ocr_provider(\n    max_workers=4,\n    exclude_bounding_poly=True\n)\n</code></pre>"},{"location":"guide/ocr/provider_config/#understanding-provider-configuration","title":"Understanding Provider Configuration","text":"<p>When configuring OCR providers, you'll encounter two types of parameters:</p> <ol> <li> <p>Docprompt generic parameters: These are common across different providers and control Docprompt's behavior.</p> <ul> <li><code>max_workers</code>: Controls concurrency for processing large documents</li> <li><code>exclude_bounding_poly</code>: Reduces memory usage by excluding detailed polygon data</li> </ul> </li> <li> <p>Provider-specific parameters: These are unique to each backend and control provider-specific features. For example, if using GCP as an OCR provider, you must specify <code>project_id</code>, <code>processor_id</code> and you may optionally set <code>return_image_quality_scores</code>.</p> </li> </ol>"},{"location":"guide/ocr/provider_config/#provider-specific-features-and-limitations","title":"Provider-Specific Features and Limitations","text":""},{"location":"guide/ocr/provider_config/#google-cloud-platform-gcp","title":"Google Cloud Platform (GCP)","text":"<ul> <li>Offers advanced layout analysis and image quality scoring</li> <li>Supports returning rasterized images of processed pages</li> <li>Requires GCP-specific project and processor IDs</li> </ul> <p>Example configuration: <pre><code>gcp_ocr_provider = gcp_factory.get_page_ocr_provider(\n    project_id=\"YOUR_PROJECT_ID\",\n    processor_id=\"YOUR_PROCESSOR_ID\",\n    max_workers=4,  # Docprompt generic\n    return_images=True,  # GCP-specific\n    return_image_quality_scores=True  # GCP-specific\n)\n</code></pre></p>"},{"location":"guide/ocr/provider_config/#amazon-textract","title":"Amazon Textract","text":"<ul> <li>Focuses on text extraction and layout analysis</li> <li>Provides confidence scores for extracted text</li> <li>Does not support returning rasterized images</li> </ul> <p>Example configuration: <pre><code>amazon_ocr_provider = amazon_factory.get_page_ocr_provider(\n    max_workers=4,  # Docprompt generic\n    exclude_bounding_poly=True  # Docprompt generic\n)\n</code></pre></p>"},{"location":"guide/ocr/provider_config/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Use factories for credential management: This centralizes authentication and makes it easier to switch between providers.</p> </li> <li> <p>Consult provider documentation: Always refer to the latest documentation from AWS or GCP for the most up-to-date information on their OCR services.</p> </li> <li> <p>Check Docprompt API reference: Review Docprompt's API documentation for each provider to understand available configurations.</p> </li> <li> <p>Optimize for your use case: Configure providers based on your specific needs, balancing performance and feature requirements.</p> </li> </ol>"},{"location":"guide/ocr/provider_config/#conclusion","title":"Conclusion","text":"<p>Understanding the factory pattern and the distinction between Docprompt generic and provider-specific parameters is key to effectively configuring OCR providers in Docprompt. While this guide provides an overview using Amazon Textract and GCP as examples, the principles apply to other providers as well. Always consult the specific provider's documentation and Docprompt's API reference for the most current and detailed information.</p>"},{"location":"guide/table_extraction/extract_tables/","title":"Table Extraction with DocPrompt: Invoice Parsing","text":"<p>DocPrompt can be used to extract tables from documents with high accuracy using visual large language models, such as GPT-4 Vision or Anthropic's Claude 3. In this guide, we'll demonstrate how to extract tables from invoices using DocPrompt.</p>"},{"location":"guide/table_extraction/extract_tables/#setting-up","title":"Setting Up","text":"<p>First, let's import the necessary modules and set up our environment:</p> <pre><code>from docprompt import load_document_node, DocumentNode\nfrom docprompt.tasks.factory import AnthropicTaskProviderFactory\nfrom docprompt.tasks.table_extraction import TableExtractionInput\n\n# Initialize the Anthropic factory\n# Ensure you have set the ANTHROPIC_API_KEY environment variable\nfactory = AnthropicTaskProviderFactory()\n\n# Create the table extraction provider\ntable_extraction_provider = factory.get_page_table_extraction_provider()\n</code></pre>"},{"location":"guide/table_extraction/extract_tables/#preparing-the-document","title":"Preparing the Document","text":"<p>Load a DocumentNode from a path</p> <pre><code>document_node = load_document_node(\"path/to/your/invoice.pdf\")\n</code></pre>"},{"location":"guide/table_extraction/extract_tables/#performing-table-extraction","title":"Performing Table Extraction","text":"<p>Now, let's run the table extraction task on our invoice:</p> <pre><code>results = table_extraction_provider.process_document_node(document_node) # Sync\n\nasync_results = await table_extraction_provider.aprocess_document_node(document_node)\n</code></pre> <p>Alternatively, we can do table extraction async as well</p>"},{"location":"guide/table_extraction/extract_tables/#interpreting-results","title":"Interpreting Results","text":"<p>Let's examine the extracted tables from a pretend invoice:</p> <pre><code>for page_number, result in results.items():\n    print(f\"Tables extracted from Page {page_number}:\")\n    for i, table in enumerate(result.tables, 1):\n        print(f\"\\nTable {i}:\")\n        print(f\"Title: {table.title}\")\n        print(\"Headers:\")\n        print(\", \".join(header.text for header in table.headers))\n        print(\"Rows:\")\n        for row in table.rows:\n            print(\", \".join(cell.text for cell in row.cells))\n    print('---')\n</code></pre> <p>This will print the extracted tables, including headers and rows, for each page of the invoice.</p>"},{"location":"guide/table_extraction/extract_tables/#increasing-accuracy","title":"Increasing Accuracy","text":"<p>In Anthropic's case, the default is <code>\"claude-3-haiku-20240307\"</code>. This performs with high accuracy, and is over 5x cheaper than table extraction using Azure Document Intelligence.</p> <p>In use-cases where accuracy is paramount however, it may be worthwhile to set the provider to a more powerful model.</p> <pre><code>table_extraction_provider = factory.get_page_table_extraction_provider(\n    model_name=\"claude-3-5-sonnet-20240620\"  # setup the task provider with Sonnet 35\n)\n\nresults = table_extraction_provider.process_document_node(\n    document_node,\n    table_extraction_input,\n    model_name=\"claude-3-5-sonnet-20240620\"  # or declare model name at inference time\n)\n</code></pre> <p>As Large Language Models steadily get cheaper and more capable, your inference costs will drop inevitably. The beauty of progress!</p>"},{"location":"guide/table_extraction/extract_tables/#resolving-bounding-boxes","title":"Resolving Bounding Boxes","text":"<p>Coming Soon</p> <p>In some scenarios, you may want the exact bounding boxes of the various rows, columns, and cells. If you've processed OCR results through Docprompt, this is possible by specifying an additional argument in <code>process_document_node</code></p> <pre><code>results = table_extraction_provider.process_document_node(\n    document_node,\n    table_extraction_input,\n    model_name=\"claude-3-5-sonnet-20240620\",  # or declare model name at inference time\n    resolve_bounding_boxes=True\n)\n</code></pre> <p>If you've collected and stored OCR results on the DocumentNode, this will use word-level bounding boxes coupled with the Docprompt search engine to determine the bounding boxes of the resulting tables, where possible.</p>"},{"location":"guide/table_extraction/extract_tables/#conclusion","title":"Conclusion","text":"<p>Table extraction with DocPrompt provides a powerful way to automatically parse structured data from any documents containing tabular information in just a few lines of code.</p> <p>The quality of your results depends on the model and the complexity of the table layouts. Experiment with different configurations and post-processing steps to find what works best for your specific use case.</p> <p>When combining with other tasks such as classification, layout analysis and markerization, you can build powerful document processing pipelines in just a few steps.</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>_decorators</li> <li>_exec<ul> <li>ghostscript</li> <li>tesseract</li> </ul> </li> <li>_pdfium</li> <li>contrib<ul> <li>parser_bot</li> </ul> </li> <li>provenance<ul> <li>search</li> <li>source</li> <li>util</li> </ul> </li> <li>rasterize</li> <li>schema<ul> <li>document</li> <li>layout</li> <li>pipeline<ul> <li>metadata</li> <li>node<ul> <li>base</li> <li>collection</li> <li>document</li> <li>image</li> <li>page</li> <li>typing</li> </ul> </li> <li>rasterizer</li> </ul> </li> </ul> </li> <li>storage</li> <li>tasks<ul> <li>base</li> <li>capabilities</li> <li>classification<ul> <li>anthropic</li> <li>base</li> </ul> </li> <li>credentials</li> <li>factory</li> <li>markerize<ul> <li>anthropic</li> <li>base</li> </ul> </li> <li>message</li> <li>ocr<ul> <li>amazon</li> <li>base</li> <li>gcp</li> <li>result</li> <li>tesseract</li> </ul> </li> <li>parser</li> <li>result</li> <li>table_extraction<ul> <li>anthropic</li> <li>base</li> <li>schema</li> </ul> </li> <li>util</li> </ul> </li> <li>utils<ul> <li>async_utils</li> <li>compressor</li> <li>date_extraction</li> <li>inference</li> <li>masking<ul> <li>image</li> </ul> </li> <li>splitter</li> <li>util</li> </ul> </li> </ul>"},{"location":"reference/_decorators/","title":"_decorators","text":""},{"location":"reference/_pdfium/","title":"_pdfium","text":""},{"location":"reference/_pdfium/#docprompt._pdfium.chunk_iterable","title":"<code>chunk_iterable(iterable, chunk_size)</code>","text":"<p>Splits an iterable into chunks of specified size, distributing the remainder evenly.</p> <p>Parameters:</p> Name Type Description Default <code>iterable</code> <code>Iterable[T]</code> <p>The iterable to be chunked.</p> required <code>chunk_size</code> <code>int</code> <p>The desired size of each chunk.</p> required <p>Returns:</p> Type Description <code>List[List[T]]</code> <p>List[List[T]]: A list of lists, where each sublist is a chunk.</p> Source code in <code>docprompt/_pdfium.py</code> <pre><code>def chunk_iterable(iterable: Iterable[T], chunk_size: int) -&gt; List[List[T]]:\n    \"\"\"\n    Splits an iterable into chunks of specified size, distributing the remainder evenly.\n\n    Args:\n        iterable (Iterable[T]): The iterable to be chunked.\n        chunk_size (int): The desired size of each chunk.\n\n    Returns:\n        List[List[T]]: A list of lists, where each sublist is a chunk.\n    \"\"\"\n    # Convert the iterable to a list\n    items = list(iterable)\n    total_items = len(items)\n\n    # Calculate the number of chunks needed\n    num_chunks = (total_items + chunk_size - 1) // chunk_size\n\n    # Calculate the ideal size of each chunk\n    ideal_chunk_size = total_items // num_chunks\n    remainder = total_items % num_chunks\n\n    # Create the chunks\n    chunks = []\n    start = 0\n    for i in range(num_chunks):\n        end = start + ideal_chunk_size + (1 if i &lt; remainder else 0)\n        chunks.append(items[start:end])\n        start = end\n\n    return chunks\n</code></pre>"},{"location":"reference/_pdfium/#docprompt._pdfium.get_pdfium_document","title":"<code>get_pdfium_document(fp, password=None)</code>","text":"<p>Loads a PDF document with a lock to prevent race conditions in threaded environments</p> Source code in <code>docprompt/_pdfium.py</code> <pre><code>@contextmanager\ndef get_pdfium_document(\n    fp: Union[PathLike, Path, bytes, str], password: Optional[str] = None\n):\n    \"\"\"\n    Loads a PDF document with a lock to prevent race conditions in threaded environments\n    \"\"\"\n    with PDFIUM_LOAD_LOCK:\n        pdf = pdfium.PdfDocument(fp, password=password, autoclose=False)\n    try:\n        yield pdf\n    finally:\n        pdf.close()\n</code></pre>"},{"location":"reference/_pdfium/#docprompt._pdfium.rasterize_page_with_pdfium","title":"<code>rasterize_page_with_pdfium(fp, page_number, *, return_mode='pil', post_process_fn=None, **kwargs)</code>","text":"<p>Rasterizes a page of a PDF document</p> Source code in <code>docprompt/_pdfium.py</code> <pre><code>def rasterize_page_with_pdfium(\n    fp: Union[PathLike, Path, bytes],\n    page_number: int,\n    *,\n    return_mode: Literal[\"pil\", \"bytes\"] = \"pil\",\n    post_process_fn: Optional[Callable[[Image.Image], Image.Image]] = None,\n    **kwargs,\n) -&gt; Union[Image.Image, bytes]:\n    \"\"\"\n    Rasterizes a page of a PDF document\n    \"\"\"\n    with get_pdfium_document(fp) as pdf:\n        return _render_job(\n            page_number - 1,\n            pdf,\n            kwargs,\n            return_mode=return_mode,\n            post_process_fn=post_process_fn,\n        )\n</code></pre>"},{"location":"reference/_pdfium/#docprompt._pdfium.rasterize_pdf_with_pdfium","title":"<code>rasterize_pdf_with_pdfium(fp, password=None, *, return_mode='pil', post_process_fn=None, **kwargs)</code>","text":"<p>Rasterizes an entire PDF using PDFium and a pool of workers</p> Source code in <code>docprompt/_pdfium.py</code> <pre><code>def rasterize_pdf_with_pdfium(\n    fp: Union[PathLike, Path, bytes],\n    password: Optional[str] = None,\n    *,\n    return_mode: Literal[\"pil\", \"bytes\"] = \"pil\",\n    post_process_fn: Optional[Callable[[Image.Image], Image.Image]] = None,\n    **kwargs,\n) -&gt; List[Union[Image.Image, bytes]]:\n    \"\"\"\n    Rasterizes an entire PDF using PDFium and a pool of workers\n    \"\"\"\n    with get_pdfium_document(fp, password=password) as pdf:\n        total_pages = len(pdf)\n\n    max_workers = min(mp.cpu_count(), total_pages)\n\n    ctx = mp.get_context(\"spawn\")\n\n    with potential_temporary_file(fp) as temp_fp:\n        initargs = (\n            None,\n            temp_fp,\n            password,\n            False,\n            kwargs,\n            return_mode,\n            post_process_fn,\n        )\n\n        with ft.ProcessPoolExecutor(\n            max_workers=max_workers,\n            initializer=_render_parallel_init,\n            initargs=initargs,\n            mp_context=ctx,\n        ) as executor:\n            results = executor.map(\n                _render_parallel_job, range(total_pages), chunksize=1\n            )\n\n        return list(results)\n</code></pre>"},{"location":"reference/_pdfium/#docprompt._pdfium.rasterize_pdfs_with_pdfium","title":"<code>rasterize_pdfs_with_pdfium(fps, passwords=None, *, return_mode='pil', post_process_fn=None, **kwargs)</code>","text":"<p>Like 'rasterize_pdf_with_pdfium', but optimized for multiple PDFs by loading all PDF's into the workers memory space</p> Source code in <code>docprompt/_pdfium.py</code> <pre><code>def rasterize_pdfs_with_pdfium(\n    fps: List[Union[PathLike, Path, bytes]],\n    passwords: Optional[List[str]] = None,\n    *,\n    return_mode: Literal[\"pil\", \"bytes\"] = \"pil\",\n    post_process_fn: Optional[Callable[[Image.Image], Image.Image]] = None,\n    **kwargs,\n) -&gt; Dict[int, Dict[int, Union[Image.Image, bytes]]]:\n    \"\"\"\n    Like 'rasterize_pdf_with_pdfium', but optimized for multiple PDFs by loading all PDF's into the workers memory space\n    \"\"\"\n    if passwords and len(passwords) != len(fps):\n        raise ValueError(\n            \"If specifying passwords, must provide one for each PDF. Use None for no password.\"\n        )\n    passwords = passwords or [None] * len(fps)\n\n    ctx = mp.get_context(\"spawn\")\n\n    with tempfile.TemporaryDirectory(prefix=\"docprompt_raster_tmp\") as tempdir:\n        writable_fps = _get_writable_temp_fp_paths(fps, tempdir)\n        page_counts = _get_page_counts_from_pdfs(writable_fps)\n        total_to_process = sum(page_counts)\n\n        max_workers = min(mp.cpu_count(), total_to_process)\n\n        pdf_page_map = dict(enumerate(page_counts))\n        name_to_idx = {fp: i for i, fp in enumerate(writable_fps)}\n\n        core_pdf_assignments = distribute_pdfs(pdf_page_map, max_workers)\n\n        with mp.Manager() as manager:\n            mp_queue = manager.Queue()\n\n            processes = []\n\n            with tqdm(total=total_to_process, desc=\"Rasterizing PDF's\") as pbar:\n                for core_id, pdf_page_map in core_pdf_assignments.items():\n                    data = {\n                        (writable_fps[i], passwords[i]): pages\n                        for i, pages in pdf_page_map.items()\n                    }\n\n                    p = ctx.Process(\n                        target=process_work,\n                        args=(data, post_process_fn, return_mode, mp_queue),\n                    )\n                    p.start()\n                    processes.append(p)\n\n                results: Dict[int, Dict[int, Union[Image.Image, bytes]]] = {}\n\n                while any(p.is_alive() for p in processes) or not mp_queue.empty():\n                    try:\n                        pdf, page, result = mp_queue.get(timeout=0.5)\n                        i = name_to_idx[pdf]\n                        results.setdefault(i, {})[page] = result\n                        pbar.update(1)\n                    except queue.Empty:\n                        pass\n\n    return results\n</code></pre>"},{"location":"reference/rasterize/","title":"rasterize","text":""},{"location":"reference/rasterize/#docprompt.rasterize.estimate_png_byte_size","title":"<code>estimate_png_byte_size(image, assummed_compression_ratio=4.0, overhead_bytes=1024)</code>","text":"<p>Provides an estimate of the size of a PNG image given the uncompressed size and an assumed compression ratio.</p> <p>The default compression ratio of 4.0 is based on the assumption that the image is a document, and represents a pessimistic estimate.</p> Source code in <code>docprompt/rasterize.py</code> <pre><code>def estimate_png_byte_size(\n    image: Image.Image,\n    assummed_compression_ratio: float = 4.0,\n    overhead_bytes: int = 1024,\n) -&gt; int:\n    \"\"\"\n    Provides an estimate of the size of a PNG image given the uncompressed size and an assumed compression ratio.\n\n    The default compression ratio of 4.0 is based on the assumption that the image is a document, and represents a\n    pessimistic estimate.\n    \"\"\"\n    width, height = image.size\n    mode = image.mode\n\n    # Determine bytes per pixel based on image mode\n    if mode == \"1\":\n        bytes_per_pixel = 1 / 8  # 1 bit per pixel\n    elif mode == \"L\":\n        bytes_per_pixel = 1  # 1 byte per pixel\n    elif mode == \"LA\":\n        bytes_per_pixel = 2  # 2 bytes per pixel\n    elif mode == \"RGB\":\n        bytes_per_pixel = 3  # 3 bytes per pixel\n    elif mode == \"RGBA\":\n        bytes_per_pixel = 4  # 4 bytes per pixel\n    else:\n        raise ValueError(f\"Unsupported image mode: {mode}\")\n\n    uncompressed_size = width * height * bytes_per_pixel\n    compressed_size = uncompressed_size / assummed_compression_ratio\n\n    return int(compressed_size + overhead_bytes)\n</code></pre>"},{"location":"reference/rasterize/#docprompt.rasterize.mask_image_from_bboxes","title":"<code>mask_image_from_bboxes(image, bboxes, *, mask_color='black')</code>","text":"<p>Given a set of normalized bounding boxes, masks the image. :param image: PIL Image object or bytes object representing an image. :param bboxes: Iterable of NormBBox objects. :param mask_color: Color used for the mask, can be a string (e.g., \"black\") or a tuple (e.g., (0, 0, 0)).</p> Source code in <code>docprompt/rasterize.py</code> <pre><code>def mask_image_from_bboxes(\n    image: PILOrBytes,\n    bboxes: Iterable[NormBBox],\n    *,\n    mask_color: Union[str, int] = \"black\",\n):\n    \"\"\"\n    Given a set of normalized bounding boxes, masks the image.\n    :param image: PIL Image object or bytes object representing an image.\n    :param bboxes: Iterable of NormBBox objects.\n    :param mask_color: Color used for the mask, can be a string (e.g., \"black\") or a tuple (e.g., (0, 0, 0)).\n    \"\"\"\n    # Convert bytes image to PIL Image if necessary\n    if isinstance(image, bytes):\n        image = load_image_from_bytes(image)\n\n    # Get image dimensions\n    width, height = image.size\n\n    # Create a drawing context\n    draw = ImageDraw.Draw(image)\n\n    # Draw rectangles over the specified bounding boxes\n    for bbox in bboxes:\n        # Convert normalized coordinates to absolute coordinates\n        absolute_bbox = (\n            bbox.x0 * width,\n            bbox.top * height,\n            bbox.x1 * width,\n            bbox.bottom * height,\n        )\n        # Draw rectangle\n        draw.rectangle(absolute_bbox, fill=mask_color)\n\n    return image\n</code></pre>"},{"location":"reference/rasterize/#docprompt.rasterize.resize_image_to_fize_size_limit","title":"<code>resize_image_to_fize_size_limit(image, max_file_size_bytes, *, resize_mode='thumbnail', resize_step_size=0.1, allow_channel_reduction=True, image_convert_mode='L')</code>","text":"<p>Incrementally resizes an image until it is under a certain file size</p> Source code in <code>docprompt/rasterize.py</code> <pre><code>def resize_image_to_fize_size_limit(\n    image: PILOrBytes,\n    max_file_size_bytes: int,\n    *,\n    resize_mode: ResizeModes = \"thumbnail\",\n    resize_step_size: float = 0.1,\n    allow_channel_reduction: bool = True,\n    image_convert_mode: str = \"L\",\n) -&gt; Image.Image:\n    \"\"\"\n    Incrementally resizes an image until it is under a certain file size\n    \"\"\"\n    if resize_step_size &lt;= 0 or resize_step_size &gt;= 0.5:\n        raise ValueError(\"resize_step_size must be between 0 and 0.5\")\n\n    if isinstance(image, bytes):\n        image = load_image_from_bytes(image)\n\n    estimated_bytes = estimate_png_byte_size(image)\n\n    if estimated_bytes &lt; max_file_size_bytes:\n        return image\n\n    # Convert image to the desired mode if it has multiple channels\n    if allow_channel_reduction and image.mode in [\"LA\", \"RGBA\"]:\n        image = image.convert(image_convert_mode)\n\n        if estimate_png_byte_size(image) &lt; max_file_size_bytes:\n            return image\n\n    step_count = 0\n    working_image = image.copy()\n\n    while estimated_bytes &gt; max_file_size_bytes:\n        new_width = int(image.width * (1 - resize_step_size * step_count))\n        new_height = int(image.height * (1 - resize_step_size * step_count))\n\n        if new_width &lt;= 200 or new_height &lt;= 200:\n            logger.warning(\n                f\"Image could not be resized to under {max_file_size_bytes} bytes. Reached {estimated_bytes} bytes.\"\n            )\n            break\n\n        if resize_mode == \"thumbnail\":\n            working_image.thumbnail((new_width, new_height))\n        elif resize_mode == \"resize\":\n            working_image = working_image.resize((new_width, new_height))\n\n        estimated_bytes = estimate_png_byte_size(working_image)\n\n        if estimated_bytes &lt; max_file_size_bytes:\n            return working_image\n\n        step_count += 1\n\n    return working_image\n</code></pre>"},{"location":"reference/storage/","title":"storage","text":"<p>The File System class provides a wrapper around FSSpec with the required operations for reading, writing, and deleting files from an arbitrary file system backend.</p> <p>When instantiating the model, you must review the <code>fsspec</code> documentation to determine what additional environment and credential kwargs you must provide, based on your specific selection of file system.</p>"},{"location":"reference/storage/#docprompt.storage.FileSidecarsPathManager","title":"<code>FileSidecarsPathManager</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>The FileSidecarsPathManager provides a wrapper around fsspec to provide a clean interface for reading and writing sidecar directories for storing document nodes.</p> <p>Attributes:</p> Name Type Description <code>base_path</code> <code>str</code> <p>The base path for the sidecar files.</p> <code>file_hash</code> <code>str</code> <p>The hash of the file to be stored.</p> <code>pdf</code> <code>str</code> <p>The path for the PDF file.</p> <code>metadata</code> <code>str</code> <p>The path for the metadata file.</p> <code>page_metadata</code> <code>str</code> <p>The path for the page metadata file.</p> Source code in <code>docprompt/storage.py</code> <pre><code>class FileSidecarsPathManager(BaseModel):\n    \"\"\"The FileSidecarsPathManager provides a wrapper around fsspec to provide a clean interface for\n    reading and writing sidecar directories for storing document nodes.\n\n    Attributes:\n        base_path (str): The base path for the sidecar files.\n        file_hash (str): The hash of the file to be stored.\n        pdf (str): The path for the PDF file.\n        metadata (str): The path for the metadata file.\n        page_metadata (str): The path for the page metadata file.\n    \"\"\"\n\n    base_path: str = Field(...)\n    file_hash: str = Field(...)\n\n    @computed_field\n    @property\n    def pdf(self) -&gt; str:\n        \"\"\"The path for the PDF file.\"\"\"\n        return f\"{self.base_path}/{self.file_hash}/base.pdf\"\n\n    @computed_field\n    @property\n    def metadata(self) -&gt; str:\n        \"\"\"The path for the metadata file.\"\"\"\n        return f\"{self.base_path}/{self.file_hash}/base.json\"\n\n    @computed_field\n    @property\n    def page_metadata(self) -&gt; str:\n        \"\"\"The path for the page metadata file.\"\"\"\n        return f\"{self.base_path}/{self.file_hash}/pages.json\"\n</code></pre>"},{"location":"reference/storage/#docprompt.storage.FileSidecarsPathManager.metadata","title":"<code>metadata: str</code>  <code>property</code>","text":"<p>The path for the metadata file.</p>"},{"location":"reference/storage/#docprompt.storage.FileSidecarsPathManager.page_metadata","title":"<code>page_metadata: str</code>  <code>property</code>","text":"<p>The path for the page metadata file.</p>"},{"location":"reference/storage/#docprompt.storage.FileSidecarsPathManager.pdf","title":"<code>pdf: str</code>  <code>property</code>","text":"<p>The path for the PDF file.</p>"},{"location":"reference/storage/#docprompt.storage.FileSystemAnnotation","title":"<code>FileSystemAnnotation</code>","text":"<p>FileSystemType is a custom type for fsspec.AbstractFileSystem instances.</p> <p>This class handles defining the necessary information for pydantic to be able to validate and fsspec file system as a field in a Pydanitc model.</p> Source code in <code>docprompt/storage.py</code> <pre><code>class FileSystemAnnotation:\n    \"\"\"FileSystemType is a custom type for fsspec.AbstractFileSystem instances.\n\n    This class handles defining the necessary information for pydantic to be able to validate\n    and fsspec file system as a field in a Pydanitc model.\n    \"\"\"\n\n    @classmethod\n    def __get_validators__(cls):\n        yield cls.validate\n\n    @classmethod\n    def validate(cls, v):\n        \"\"\"Validate that it is a valid fsspec file system.\"\"\"\n        if not isinstance(v, fsspec.AbstractFileSystem):\n            raise TypeError(\"Must be an fsspec.AbstractFileSystem instance\")\n        return v\n\n    @classmethod\n    def __get_pydantic_core_schema__(\n        cls, _source_type: Any, _handler: Any\n    ) -&gt; core_schema.CoreSchema:\n        \"\"\"Get the Pydantic Core Schema for the FileSystemType.\"\"\"\n        return core_schema.no_info_after_validator_function(\n            cls.validate,\n            core_schema.any_schema(),\n            serialization=core_schema.to_string_ser_schema(),\n        )\n</code></pre>"},{"location":"reference/storage/#docprompt.storage.FileSystemAnnotation.__get_pydantic_core_schema__","title":"<code>__get_pydantic_core_schema__(_source_type, _handler)</code>  <code>classmethod</code>","text":"<p>Get the Pydantic Core Schema for the FileSystemType.</p> Source code in <code>docprompt/storage.py</code> <pre><code>@classmethod\ndef __get_pydantic_core_schema__(\n    cls, _source_type: Any, _handler: Any\n) -&gt; core_schema.CoreSchema:\n    \"\"\"Get the Pydantic Core Schema for the FileSystemType.\"\"\"\n    return core_schema.no_info_after_validator_function(\n        cls.validate,\n        core_schema.any_schema(),\n        serialization=core_schema.to_string_ser_schema(),\n    )\n</code></pre>"},{"location":"reference/storage/#docprompt.storage.FileSystemAnnotation.validate","title":"<code>validate(v)</code>  <code>classmethod</code>","text":"<p>Validate that it is a valid fsspec file system.</p> Source code in <code>docprompt/storage.py</code> <pre><code>@classmethod\ndef validate(cls, v):\n    \"\"\"Validate that it is a valid fsspec file system.\"\"\"\n    if not isinstance(v, fsspec.AbstractFileSystem):\n        raise TypeError(\"Must be an fsspec.AbstractFileSystem instance\")\n    return v\n</code></pre>"},{"location":"reference/storage/#docprompt.storage.FileSystemManager","title":"<code>FileSystemManager</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>The FileSystemManager provides a wrapper around fsspec to provide a clean interface for reading and writing sidecar directories for storing document nodes.</p> Source code in <code>docprompt/storage.py</code> <pre><code>class FileSystemManager(BaseModel):\n    \"\"\"The FileSystemManager provides a wrapper around fsspec to provide a clean interface for\n    reading and writing sidecar directories for storing document nodes.\n    \"\"\"\n\n    path: str = Field(...)\n    fs: FileSystemAnnotation = Field(...)\n    fs_kwargs: Dict[str, Any] = Field(...)\n\n    def __init__(self, url: str, **kwargs: Any) -&gt; None:\n        \"\"\"Initialize the FileSystemManager with the path and file system backend.\"\"\"\n        super().__init__(path=url, fs_kwargs=kwargs)\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def validate_filesystem_protocol_and_kwargs(cls, data: Any) -&gt; Any:\n        \"\"\"Validate that the path is a valid filesystem path.\"\"\"\n\n        path = data.get(\"path\", None)\n        fs_kwargs = data.get(\"fs_kwargs\", {})\n\n        # Validate that the path is a valid filesystem path\n        file_system = fsspec.url_to_fs(path, **fs_kwargs)\n\n        # Set the data values on the model\n        data[\"fs\"] = file_system[0]\n        data[\"path\"] = file_system[1]\n        data[\"fs_kwargs\"] = fs_kwargs\n\n        return data\n\n    def get_pdf_name(self, file_hash: str) -&gt; FileSidecarsPathManager:\n        \"\"\"Get the file manager for a specific file hash.\"\"\"\n        path_manager = FileSidecarsPathManager(base_path=self.path, file_hash=file_hash)\n        return os.path.basename(path_manager.pdf)\n\n    def _write(self, value: bytes, *args, **kwargs) -&gt; None:\n        \"\"\"Write a value to the file system.\n\n        Args:\n            value: The value to write.\n            *args: `fsspec.open` positional arguments.\n            **kwargs: `fsspec.open` keyword arguments.\n        \"\"\"\n        # Make sure the path exists\n        parent_dir = os.path.dirname(args[0])\n\n        kwargs = {**self.fs_kwargs, **kwargs}\n\n        if not self.fs.exists(parent_dir, **kwargs):\n            self.fs.mkdirs(parent_dir, **kwargs)\n\n        with self.fs.open(*args, **kwargs) as f:\n            f.write(value)\n\n    def _read(self, *args, **kwargs) -&gt; bytes:\n        \"\"\"A wrapper for reading with fsspec.\n\n        Args:\n            *args: `fsspec.open` positional arguments.\n            **kwargs: `fsspec.open` keyword arguments.\n\n        Returns:\n            bytes: The read value.\n        \"\"\"\n\n        kwargs = {**self.fs_kwargs, **kwargs}\n\n        with self.fs.open(*args, **kwargs) as f:\n            return f.read()\n\n    def _delete(self, path: str, **kwargs):\n        \"\"\"Delete a file from the file system.\n\n        Args:\n            path (str): The path to the file to delete.\n            **kwargs: Additional keyword arguments to pass to the file system.\n        \"\"\"\n\n        kwargs = {**self.fs_kwargs, **kwargs}\n\n        if self.fs.exists(path, **kwargs):\n            self.fs.rm(path, **kwargs)\n\n    def write(\n        self,\n        pdf_bytes: bytes,\n        metadata_bytes: Optional[bytes] = None,\n        page_metadata_bytes: Optional[bytes] = None,\n        encrypt: bool = False,\n        compress: bool = False,\n        **kwargs,\n    ) -&gt; FileSidecarsPathManager:\n        \"\"\"Write a sidecar to the filesystem.\"\"\"\n        from docprompt.utils.util import hash_from_bytes\n\n        file_hash = hash_from_bytes(pdf_bytes)\n\n        # Craete the sidecar manager\n        path_manager = FileSidecarsPathManager(base_path=self.path, file_hash=file_hash)\n\n        if encrypt:\n            warnings.warn(\"Encryption is not yet supported for the FileSystemManager.\")\n\n        if compress:\n            warnings.warn(\"Compression is not yet supported for the FileSystemManager.\")\n\n        kwargs = {**self.fs_kwargs, **kwargs}\n\n        # Write the file\n        self._write(pdf_bytes, path_manager.pdf, \"wb\", **kwargs)\n\n        # If the metadata is provided, we want to write it\n        if metadata_bytes is not None:\n            self._write(metadata_bytes, path_manager.metadata, \"wb\", **kwargs)\n\n        # Otherwise, we need to clear the metadata file, so that the node is read\n        # without any metadata (overwriting any existing metadata)\n        else:\n            # Check if a metadata file exists\n            self._delete(path_manager.metadata, **kwargs)\n\n        if page_metadata_bytes is not None:\n            self._write(page_metadata_bytes, path_manager.page_metadata, \"wb\", **kwargs)\n\n        # Otherwise, we need to clear the page metadata file, so that the node is read\n        # without any metadata (overwriting any existing metadata)\n        else:\n            self._delete(path_manager.page_metadata, **kwargs)\n\n        return path_manager\n\n    def read(\n        self, file_hash: str, **kwargs\n    ) -&gt; Tuple[bytes, Union[bytes, None], Union[bytes, None]]:\n        \"\"\"Read a pair of sidecar files from the filesystem.\"\"\"\n\n        # Craete the sidecar manager\n        path_manager = FileSidecarsPathManager(base_path=self.path, file_hash=file_hash)\n\n        # Read the PDF file\n        pdf_bytes = self._read(path_manager.pdf, \"rb\", **kwargs)\n\n        try:\n            metadata_bytes = self._read(path_manager.metadata, \"rb\", **kwargs)\n        except FileNotFoundError:\n            metadata_bytes = None\n\n        try:\n            page_metadata_bytes = self._read(path_manager.page_metadata, \"rb\", **kwargs)\n        except FileNotFoundError:\n            page_metadata_bytes = None\n\n        return pdf_bytes, metadata_bytes, page_metadata_bytes\n</code></pre>"},{"location":"reference/storage/#docprompt.storage.FileSystemManager.__init__","title":"<code>__init__(url, **kwargs)</code>","text":"<p>Initialize the FileSystemManager with the path and file system backend.</p> Source code in <code>docprompt/storage.py</code> <pre><code>def __init__(self, url: str, **kwargs: Any) -&gt; None:\n    \"\"\"Initialize the FileSystemManager with the path and file system backend.\"\"\"\n    super().__init__(path=url, fs_kwargs=kwargs)\n</code></pre>"},{"location":"reference/storage/#docprompt.storage.FileSystemManager.get_pdf_name","title":"<code>get_pdf_name(file_hash)</code>","text":"<p>Get the file manager for a specific file hash.</p> Source code in <code>docprompt/storage.py</code> <pre><code>def get_pdf_name(self, file_hash: str) -&gt; FileSidecarsPathManager:\n    \"\"\"Get the file manager for a specific file hash.\"\"\"\n    path_manager = FileSidecarsPathManager(base_path=self.path, file_hash=file_hash)\n    return os.path.basename(path_manager.pdf)\n</code></pre>"},{"location":"reference/storage/#docprompt.storage.FileSystemManager.read","title":"<code>read(file_hash, **kwargs)</code>","text":"<p>Read a pair of sidecar files from the filesystem.</p> Source code in <code>docprompt/storage.py</code> <pre><code>def read(\n    self, file_hash: str, **kwargs\n) -&gt; Tuple[bytes, Union[bytes, None], Union[bytes, None]]:\n    \"\"\"Read a pair of sidecar files from the filesystem.\"\"\"\n\n    # Craete the sidecar manager\n    path_manager = FileSidecarsPathManager(base_path=self.path, file_hash=file_hash)\n\n    # Read the PDF file\n    pdf_bytes = self._read(path_manager.pdf, \"rb\", **kwargs)\n\n    try:\n        metadata_bytes = self._read(path_manager.metadata, \"rb\", **kwargs)\n    except FileNotFoundError:\n        metadata_bytes = None\n\n    try:\n        page_metadata_bytes = self._read(path_manager.page_metadata, \"rb\", **kwargs)\n    except FileNotFoundError:\n        page_metadata_bytes = None\n\n    return pdf_bytes, metadata_bytes, page_metadata_bytes\n</code></pre>"},{"location":"reference/storage/#docprompt.storage.FileSystemManager.validate_filesystem_protocol_and_kwargs","title":"<code>validate_filesystem_protocol_and_kwargs(data)</code>  <code>classmethod</code>","text":"<p>Validate that the path is a valid filesystem path.</p> Source code in <code>docprompt/storage.py</code> <pre><code>@model_validator(mode=\"before\")\n@classmethod\ndef validate_filesystem_protocol_and_kwargs(cls, data: Any) -&gt; Any:\n    \"\"\"Validate that the path is a valid filesystem path.\"\"\"\n\n    path = data.get(\"path\", None)\n    fs_kwargs = data.get(\"fs_kwargs\", {})\n\n    # Validate that the path is a valid filesystem path\n    file_system = fsspec.url_to_fs(path, **fs_kwargs)\n\n    # Set the data values on the model\n    data[\"fs\"] = file_system[0]\n    data[\"path\"] = file_system[1]\n    data[\"fs_kwargs\"] = fs_kwargs\n\n    return data\n</code></pre>"},{"location":"reference/storage/#docprompt.storage.FileSystemManager.write","title":"<code>write(pdf_bytes, metadata_bytes=None, page_metadata_bytes=None, encrypt=False, compress=False, **kwargs)</code>","text":"<p>Write a sidecar to the filesystem.</p> Source code in <code>docprompt/storage.py</code> <pre><code>def write(\n    self,\n    pdf_bytes: bytes,\n    metadata_bytes: Optional[bytes] = None,\n    page_metadata_bytes: Optional[bytes] = None,\n    encrypt: bool = False,\n    compress: bool = False,\n    **kwargs,\n) -&gt; FileSidecarsPathManager:\n    \"\"\"Write a sidecar to the filesystem.\"\"\"\n    from docprompt.utils.util import hash_from_bytes\n\n    file_hash = hash_from_bytes(pdf_bytes)\n\n    # Craete the sidecar manager\n    path_manager = FileSidecarsPathManager(base_path=self.path, file_hash=file_hash)\n\n    if encrypt:\n        warnings.warn(\"Encryption is not yet supported for the FileSystemManager.\")\n\n    if compress:\n        warnings.warn(\"Compression is not yet supported for the FileSystemManager.\")\n\n    kwargs = {**self.fs_kwargs, **kwargs}\n\n    # Write the file\n    self._write(pdf_bytes, path_manager.pdf, \"wb\", **kwargs)\n\n    # If the metadata is provided, we want to write it\n    if metadata_bytes is not None:\n        self._write(metadata_bytes, path_manager.metadata, \"wb\", **kwargs)\n\n    # Otherwise, we need to clear the metadata file, so that the node is read\n    # without any metadata (overwriting any existing metadata)\n    else:\n        # Check if a metadata file exists\n        self._delete(path_manager.metadata, **kwargs)\n\n    if page_metadata_bytes is not None:\n        self._write(page_metadata_bytes, path_manager.page_metadata, \"wb\", **kwargs)\n\n    # Otherwise, we need to clear the page metadata file, so that the node is read\n    # without any metadata (overwriting any existing metadata)\n    else:\n        self._delete(path_manager.page_metadata, **kwargs)\n\n    return path_manager\n</code></pre>"},{"location":"reference/_exec/","title":"Index","text":""},{"location":"reference/_exec/ghostscript/","title":"ghostscript","text":""},{"location":"reference/_exec/tesseract/","title":"tesseract","text":""},{"location":"reference/contrib/","title":"Index","text":""},{"location":"reference/contrib/parser_bot/","title":"parser_bot","text":""},{"location":"reference/provenance/","title":"Index","text":""},{"location":"reference/provenance/#docprompt.provenance.PageTextLocation","title":"<code>PageTextLocation</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Specifies the location of a piece of text in a page</p> Source code in <code>docprompt/provenance/source.py</code> <pre><code>class PageTextLocation(BaseModel):\n    \"\"\"\n    Specifies the location of a piece of text in a page\n    \"\"\"\n\n    source_blocks: List[TextBlock] = Field(\n        description=\"The source text blocks\", repr=False\n    )\n    text: str  # Sometimes the source text is less than the textblock's text.\n    score: float\n    granularity: Literal[\"word\", \"line\", \"block\"] = \"block\"\n\n    merged_source_block: Optional[TextBlock] = Field(default=None)\n</code></pre>"},{"location":"reference/provenance/#docprompt.provenance.ProvenanceSource","title":"<code>ProvenanceSource</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Bundled with some data, specifies exactly where a piece of verbatim text came from in a document.</p> Source code in <code>docprompt/provenance/source.py</code> <pre><code>class ProvenanceSource(BaseModel):\n    \"\"\"\n    Bundled with some data, specifies exactly where a piece of verbatim text came from\n    in a document.\n    \"\"\"\n\n    document_name: str\n    page_number: PositiveInt\n    text_location: Optional[PageTextLocation] = None\n\n    @computed_field  # type: ignore\n    @property\n    def source_block(self) -&gt; Optional[TextBlock]:\n        if self.text_location:\n            if self.text_location.merged_source_block:\n                return self.text_location.merged_source_block\n            if self.text_location.source_blocks:\n                return self.text_location.source_blocks[0]\n\n            return None\n\n    @property\n    def text(self) -&gt; str:\n        if self.text_location:\n            return \"\\n\".join([block.text for block in self.text_location.source_blocks])\n\n        return \"\"\n</code></pre>"},{"location":"reference/provenance/#docprompt.provenance.search","title":"<code>search</code>","text":""},{"location":"reference/provenance/#docprompt.provenance.search.DocumentProvenanceLocator","title":"<code>DocumentProvenanceLocator</code>  <code>dataclass</code>","text":"Source code in <code>docprompt/provenance/search.py</code> <pre><code>@dataclass\nclass DocumentProvenanceLocator:\n    document_name: str\n    search_index: \"tantivy.Index\"\n    block_mapping: Dict[int, OcrPageResult] = field(repr=False)\n    geo_index: DocumentProvenanceGeoMap = field(repr=False)\n\n    @classmethod\n    def from_document_node(cls, document_node: \"DocumentNode\"):\n        # TODO: See if we can remove the ocr_results attribute from the\n        # PageNode and just use the metadata.task_result[\"&lt;provider&gt;_ocr\"],\n        # result of the OCR task instead.\n\n        index = create_tantivy_document_wise_block_index()\n        block_mapping_dict = {}\n        geo_index_dict: DocumentProvenanceGeoMap = {}\n\n        writer = index.writer()\n\n        for page_node in document_node.page_nodes:\n            if (\n                not page_node.ocr_results.result\n                or not page_node.ocr_results.result.block_level_blocks\n            ):\n                continue\n\n            ocr_result = page_node.ocr_results.result\n\n            for idx, text_block in enumerate(ocr_result.block_level_blocks):\n                writer.add_document(\n                    tantivy.Document(\n                        page_number=page_node.page_number,\n                        block_type=text_block.type,\n                        block_page_idx=idx,\n                        content=text_block.text,\n                    )\n                )\n\n            for granularity in [\"word\", \"line\", \"block\"]:\n                text_blocks = getattr(ocr_result, f\"{granularity}_level_blocks\", [])\n\n                bounding_boxes = [text_block.bounding_box for text_block in text_blocks]\n\n                if bounding_boxes:\n                    r_tree = RTreeIndex(\n                        insert_generator(bounding_boxes), fill_factor=0.9\n                    )\n                else:\n                    r_tree = RTreeIndex()\n\n                if page_node.page_number not in geo_index_dict:\n                    geo_index_dict[page_node.page_number] = {}\n\n                geo_index_dict[page_node.page_number][granularity] = r_tree  # type: ignore\n\n            block_mapping_dict[page_node.page_number] = ocr_result\n\n        writer.commit()\n        index.reload()\n\n        return cls(\n            document_name=document_node.document.name,\n            search_index=index,\n            block_mapping=block_mapping_dict,\n            geo_index=geo_index_dict,\n        )\n\n    def _construct_tantivy_query(\n        self, query: str, page_number: Optional[int] = None\n    ) -&gt; tantivy.Query:\n        query = preprocess_query_text(query)\n\n        if page_number is None:\n            return self.search_index.parse_query(f'content:\"{query}\"')\n        else:\n            return self.search_index.parse_query(\n                f'(page_number:{page_number}) AND content:\"{query}\"'\n            )\n\n    def get_k_nearest_blocks(\n        self,\n        bbox: NormBBox,\n        page_number: int,\n        k: int,\n        granularity: BlockGranularity = \"block\",\n    ) -&gt; List[TextBlock]:\n        \"\"\"\n        Get the k nearest text blocks to a given bounding box\n        \"\"\"\n        search_tuple = construct_valid_rtree_tuple(bbox)\n\n        word_level_bbox_indices = list(\n            self.geo_index[page_number][granularity].nearest(\n                search_tuple, num_results=k\n            )\n        )\n\n        block_mapping = self.block_mapping[page_number]\n\n        nearest_blocks = [\n            getattr(block_mapping, granularity + \"s\")[idx]\n            for idx in word_level_bbox_indices\n        ]\n\n        nearest_blocks.sort(key=lambda x: (x.bounding_box.top, x.bounding_box.x0))\n\n        return [x for x in nearest_blocks if x.bounding_box != bbox]\n\n    def get_overlapping_blocks(\n        self, bbox: NormBBox, page_number: int, granularity: BlockGranularity = \"block\"\n    ) -&gt; List[TextBlock]:\n        \"\"\"\n        Get the text blocks that overlap with a given bounding box\n        \"\"\"\n        search_tuple = construct_valid_rtree_tuple(bbox)\n\n        bbox_indices = list(\n            self.geo_index[page_number][granularity].intersection(search_tuple)\n        )\n\n        block_mapping = self.block_mapping[page_number]\n\n        overlapping_blocks = [\n            getattr(block_mapping, f\"{granularity}_level_blocks\")[idx]\n            for idx in bbox_indices\n        ]\n\n        overlapping_blocks.sort(key=lambda x: (x.bounding_box.top, x.bounding_box.x0))\n\n        return [x for x in overlapping_blocks if x.bounding_box != bbox]\n\n    def search_raw(self, raw_query: str) -&gt; List[str]:\n        \"\"\"\n        Search for a piece of text using a raw query\n\n        Args:\n            query: The text to search for\n            page_number: The page number to search on\n        \"\"\"\n        parsed_query = self.search_index.parse_query(raw_query)\n\n        searcher = self.search_index.searcher()\n\n        search_results = searcher.search(parsed_query, limit=100)\n\n        results = []\n\n        for score, doc_address in search_results.hits:\n            doc = searcher.doc(doc_address)\n\n            result_page_number = doc[\"page_number\"][0]\n            result_block_page_idx = doc[\"block_page_idx\"][0]\n            block_mapping = self.block_mapping[result_page_number]\n\n            source_block: TextBlock = block_mapping.block_level_blocks[\n                result_block_page_idx\n            ]\n\n            results.append(source_block.text)\n\n        return results\n\n    def refine_query_to_word_level(\n        self, query: str, page_number: int, enclosing_block: TextBlock\n    ):\n        \"\"\"\n        Refine a query to the word level\n        \"\"\"\n        search_tuple = construct_valid_rtree_tuple(enclosing_block.bounding_box)\n\n        word_level_bbox_indices = list(\n            self.geo_index[page_number][\"word\"].intersection(search_tuple)\n        )\n        word_level_blocks_in_original_bbox = [\n            self.block_mapping[page_number].word_level_blocks[idx]\n            for idx in word_level_bbox_indices\n        ]\n\n        refine_result = refine_block_to_word_level(\n            source_block=enclosing_block,\n            intersecting_word_level_blocks=word_level_blocks_in_original_bbox,\n            query=query,\n        )\n\n        return refine_result\n\n    def search(\n        self,\n        query: str,\n        page_number: Optional[int] = None,\n        *,\n        refine_to_word: bool = True,\n        require_exact_match: bool = True,\n    ) -&gt; List[ProvenanceSource]:\n        \"\"\"\n        Search for a piece of text in the document and return the source of it\n\n        Args:\n            query: The text to search for\n            page_number: The page number to search on\n            refine_to_word: Whether to refine the search to the word level\n            require_exact_match: Whether to require null results if `refine_to_word` is True and no exact match is found\n        \"\"\"\n        search_query = self._construct_tantivy_query(query, page_number)\n\n        searcher = self.search_index.searcher()\n\n        search_results = searcher.search(search_query, limit=100)\n\n        results = []\n\n        for score, doc_address in search_results.hits:\n            doc = searcher.doc(doc_address)\n\n            result_page_number = doc[\"page_number\"][0]\n            result_block_page_idx = doc[\"block_page_idx\"][0]\n            block_mapping = self.block_mapping[result_page_number]\n\n            source_block: TextBlock = block_mapping.block_level_blocks[\n                result_block_page_idx\n            ]\n\n            source_blocks = [source_block]\n            principal_block = source_block\n\n            if refine_to_word:\n                refine_result = self.refine_query_to_word_level(\n                    query=query,\n                    page_number=result_page_number,\n                    enclosing_block=source_block,\n                )\n\n                if refine_result is not None:\n                    principal_block, source_blocks = refine_result\n                elif require_exact_match:\n                    continue\n\n            source = ProvenanceSource(\n                document_name=self.document_name,\n                page_number=result_page_number,\n                text_location=PageTextLocation(\n                    source_blocks=source_blocks,\n                    text=query,\n                    score=score,\n                    granularity=\"block\",\n                    merged_source_block=principal_block,\n                ),\n            )\n            results.append(source)\n\n        results.sort(key=lambda x: x.page_number)\n\n        return results\n\n    def search_n_best(\n        self, query: str, n: int = 3, mode: SearchBestModes = \"shortest_text\"\n    ) -&gt; List[ProvenanceSource]:\n        results = self.search(query)\n\n        if not results:\n            return []\n\n        if mode == \"shortest_text\":\n            score_func = lambda x: len(x.source_block.text)  # noqa: E731\n        elif mode == \"longest_text\":\n            score_func = lambda x: -len(x[0].source_block.text)  # noqa: E731\n        elif mode == \"highest_score\":\n            score_func = lambda x: x[1]  # noqa: E731\n        else:\n            raise ValueError(f\"Unknown mode {mode}\")\n\n        results.sort(key=score_func)\n\n        return results[:n]\n</code></pre>"},{"location":"reference/provenance/#docprompt.provenance.search.DocumentProvenanceLocator.get_k_nearest_blocks","title":"<code>get_k_nearest_blocks(bbox, page_number, k, granularity='block')</code>","text":"<p>Get the k nearest text blocks to a given bounding box</p> Source code in <code>docprompt/provenance/search.py</code> <pre><code>def get_k_nearest_blocks(\n    self,\n    bbox: NormBBox,\n    page_number: int,\n    k: int,\n    granularity: BlockGranularity = \"block\",\n) -&gt; List[TextBlock]:\n    \"\"\"\n    Get the k nearest text blocks to a given bounding box\n    \"\"\"\n    search_tuple = construct_valid_rtree_tuple(bbox)\n\n    word_level_bbox_indices = list(\n        self.geo_index[page_number][granularity].nearest(\n            search_tuple, num_results=k\n        )\n    )\n\n    block_mapping = self.block_mapping[page_number]\n\n    nearest_blocks = [\n        getattr(block_mapping, granularity + \"s\")[idx]\n        for idx in word_level_bbox_indices\n    ]\n\n    nearest_blocks.sort(key=lambda x: (x.bounding_box.top, x.bounding_box.x0))\n\n    return [x for x in nearest_blocks if x.bounding_box != bbox]\n</code></pre>"},{"location":"reference/provenance/#docprompt.provenance.search.DocumentProvenanceLocator.get_overlapping_blocks","title":"<code>get_overlapping_blocks(bbox, page_number, granularity='block')</code>","text":"<p>Get the text blocks that overlap with a given bounding box</p> Source code in <code>docprompt/provenance/search.py</code> <pre><code>def get_overlapping_blocks(\n    self, bbox: NormBBox, page_number: int, granularity: BlockGranularity = \"block\"\n) -&gt; List[TextBlock]:\n    \"\"\"\n    Get the text blocks that overlap with a given bounding box\n    \"\"\"\n    search_tuple = construct_valid_rtree_tuple(bbox)\n\n    bbox_indices = list(\n        self.geo_index[page_number][granularity].intersection(search_tuple)\n    )\n\n    block_mapping = self.block_mapping[page_number]\n\n    overlapping_blocks = [\n        getattr(block_mapping, f\"{granularity}_level_blocks\")[idx]\n        for idx in bbox_indices\n    ]\n\n    overlapping_blocks.sort(key=lambda x: (x.bounding_box.top, x.bounding_box.x0))\n\n    return [x for x in overlapping_blocks if x.bounding_box != bbox]\n</code></pre>"},{"location":"reference/provenance/#docprompt.provenance.search.DocumentProvenanceLocator.refine_query_to_word_level","title":"<code>refine_query_to_word_level(query, page_number, enclosing_block)</code>","text":"<p>Refine a query to the word level</p> Source code in <code>docprompt/provenance/search.py</code> <pre><code>def refine_query_to_word_level(\n    self, query: str, page_number: int, enclosing_block: TextBlock\n):\n    \"\"\"\n    Refine a query to the word level\n    \"\"\"\n    search_tuple = construct_valid_rtree_tuple(enclosing_block.bounding_box)\n\n    word_level_bbox_indices = list(\n        self.geo_index[page_number][\"word\"].intersection(search_tuple)\n    )\n    word_level_blocks_in_original_bbox = [\n        self.block_mapping[page_number].word_level_blocks[idx]\n        for idx in word_level_bbox_indices\n    ]\n\n    refine_result = refine_block_to_word_level(\n        source_block=enclosing_block,\n        intersecting_word_level_blocks=word_level_blocks_in_original_bbox,\n        query=query,\n    )\n\n    return refine_result\n</code></pre>"},{"location":"reference/provenance/#docprompt.provenance.search.DocumentProvenanceLocator.search","title":"<code>search(query, page_number=None, *, refine_to_word=True, require_exact_match=True)</code>","text":"<p>Search for a piece of text in the document and return the source of it</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The text to search for</p> required <code>page_number</code> <code>Optional[int]</code> <p>The page number to search on</p> <code>None</code> <code>refine_to_word</code> <code>bool</code> <p>Whether to refine the search to the word level</p> <code>True</code> <code>require_exact_match</code> <code>bool</code> <p>Whether to require null results if <code>refine_to_word</code> is True and no exact match is found</p> <code>True</code> Source code in <code>docprompt/provenance/search.py</code> <pre><code>def search(\n    self,\n    query: str,\n    page_number: Optional[int] = None,\n    *,\n    refine_to_word: bool = True,\n    require_exact_match: bool = True,\n) -&gt; List[ProvenanceSource]:\n    \"\"\"\n    Search for a piece of text in the document and return the source of it\n\n    Args:\n        query: The text to search for\n        page_number: The page number to search on\n        refine_to_word: Whether to refine the search to the word level\n        require_exact_match: Whether to require null results if `refine_to_word` is True and no exact match is found\n    \"\"\"\n    search_query = self._construct_tantivy_query(query, page_number)\n\n    searcher = self.search_index.searcher()\n\n    search_results = searcher.search(search_query, limit=100)\n\n    results = []\n\n    for score, doc_address in search_results.hits:\n        doc = searcher.doc(doc_address)\n\n        result_page_number = doc[\"page_number\"][0]\n        result_block_page_idx = doc[\"block_page_idx\"][0]\n        block_mapping = self.block_mapping[result_page_number]\n\n        source_block: TextBlock = block_mapping.block_level_blocks[\n            result_block_page_idx\n        ]\n\n        source_blocks = [source_block]\n        principal_block = source_block\n\n        if refine_to_word:\n            refine_result = self.refine_query_to_word_level(\n                query=query,\n                page_number=result_page_number,\n                enclosing_block=source_block,\n            )\n\n            if refine_result is not None:\n                principal_block, source_blocks = refine_result\n            elif require_exact_match:\n                continue\n\n        source = ProvenanceSource(\n            document_name=self.document_name,\n            page_number=result_page_number,\n            text_location=PageTextLocation(\n                source_blocks=source_blocks,\n                text=query,\n                score=score,\n                granularity=\"block\",\n                merged_source_block=principal_block,\n            ),\n        )\n        results.append(source)\n\n    results.sort(key=lambda x: x.page_number)\n\n    return results\n</code></pre>"},{"location":"reference/provenance/#docprompt.provenance.search.DocumentProvenanceLocator.search_raw","title":"<code>search_raw(raw_query)</code>","text":"<p>Search for a piece of text using a raw query</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <p>The text to search for</p> required <code>page_number</code> <p>The page number to search on</p> required Source code in <code>docprompt/provenance/search.py</code> <pre><code>def search_raw(self, raw_query: str) -&gt; List[str]:\n    \"\"\"\n    Search for a piece of text using a raw query\n\n    Args:\n        query: The text to search for\n        page_number: The page number to search on\n    \"\"\"\n    parsed_query = self.search_index.parse_query(raw_query)\n\n    searcher = self.search_index.searcher()\n\n    search_results = searcher.search(parsed_query, limit=100)\n\n    results = []\n\n    for score, doc_address in search_results.hits:\n        doc = searcher.doc(doc_address)\n\n        result_page_number = doc[\"page_number\"][0]\n        result_block_page_idx = doc[\"block_page_idx\"][0]\n        block_mapping = self.block_mapping[result_page_number]\n\n        source_block: TextBlock = block_mapping.block_level_blocks[\n            result_block_page_idx\n        ]\n\n        results.append(source_block.text)\n\n    return results\n</code></pre>"},{"location":"reference/provenance/#docprompt.provenance.source","title":"<code>source</code>","text":""},{"location":"reference/provenance/#docprompt.provenance.source.PageTextLocation","title":"<code>PageTextLocation</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Specifies the location of a piece of text in a page</p> Source code in <code>docprompt/provenance/source.py</code> <pre><code>class PageTextLocation(BaseModel):\n    \"\"\"\n    Specifies the location of a piece of text in a page\n    \"\"\"\n\n    source_blocks: List[TextBlock] = Field(\n        description=\"The source text blocks\", repr=False\n    )\n    text: str  # Sometimes the source text is less than the textblock's text.\n    score: float\n    granularity: Literal[\"word\", \"line\", \"block\"] = \"block\"\n\n    merged_source_block: Optional[TextBlock] = Field(default=None)\n</code></pre>"},{"location":"reference/provenance/#docprompt.provenance.source.ProvenanceSource","title":"<code>ProvenanceSource</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Bundled with some data, specifies exactly where a piece of verbatim text came from in a document.</p> Source code in <code>docprompt/provenance/source.py</code> <pre><code>class ProvenanceSource(BaseModel):\n    \"\"\"\n    Bundled with some data, specifies exactly where a piece of verbatim text came from\n    in a document.\n    \"\"\"\n\n    document_name: str\n    page_number: PositiveInt\n    text_location: Optional[PageTextLocation] = None\n\n    @computed_field  # type: ignore\n    @property\n    def source_block(self) -&gt; Optional[TextBlock]:\n        if self.text_location:\n            if self.text_location.merged_source_block:\n                return self.text_location.merged_source_block\n            if self.text_location.source_blocks:\n                return self.text_location.source_blocks[0]\n\n            return None\n\n    @property\n    def text(self) -&gt; str:\n        if self.text_location:\n            return \"\\n\".join([block.text for block in self.text_location.source_blocks])\n\n        return \"\"\n</code></pre>"},{"location":"reference/provenance/#docprompt.provenance.util","title":"<code>util</code>","text":""},{"location":"reference/provenance/#docprompt.provenance.util.insert_generator","title":"<code>insert_generator(bboxes, data=None)</code>","text":"<p>Make an iterator that yields tuples of (id, bbox, data) for insertion into an RTree index which improves performance massively.</p> Source code in <code>docprompt/provenance/util.py</code> <pre><code>def insert_generator(bboxes: List[NormBBox], data: Optional[Iterable[Any]] = None):\n    \"\"\"\n    Make an iterator that yields tuples of (id, bbox, data) for insertion into an RTree index\n    which improves performance massively.\n    \"\"\"\n    data = data or [None] * len(bboxes)\n\n    for idx, (bbox, data_item) in enumerate(zip(bboxes, data)):\n        yield (idx, construct_valid_rtree_tuple(bbox), data_item)\n</code></pre>"},{"location":"reference/provenance/#docprompt.provenance.util.preprocess_query_text","title":"<code>preprocess_query_text(text)</code>","text":"<p>Improve matching ability by applying some preprocessing to the query text.</p> Source code in <code>docprompt/provenance/util.py</code> <pre><code>def preprocess_query_text(text: str) -&gt; str:\n    \"\"\"\n    Improve matching ability by applying some preprocessing to the query text.\n    \"\"\"\n    for regex in _prefix_regexs:\n        text = regex.sub(\"\", text)\n\n    text = text.strip()\n\n    text = text.replace('\"', \"\")\n\n    return text\n</code></pre>"},{"location":"reference/provenance/#docprompt.provenance.util.refine_block_to_word_level","title":"<code>refine_block_to_word_level(source_block, intersecting_word_level_blocks, query)</code>","text":"<p>Create a new text block by merging the intersecting word level blocks that match the query.</p> Source code in <code>docprompt/provenance/util.py</code> <pre><code>def refine_block_to_word_level(\n    source_block: TextBlock,\n    intersecting_word_level_blocks: List[TextBlock],\n    query: str,\n):\n    \"\"\"\n    Create a new text block by merging the intersecting word level blocks that\n    match the query.\n\n    \"\"\"\n    intersecting_word_level_blocks.sort(\n        key=lambda x: (x.bounding_box.top, x.bounding_box.x0)\n    )\n\n    tokenized_query = word_tokenize(query)\n\n    if len(tokenized_query) == 1:\n        fuzzified = default_process(tokenized_query[0])\n        for word_level_block in intersecting_word_level_blocks:\n            if fuzz.ratio(fuzzified, default_process(word_level_block.text)) &gt; 87.5:\n                return word_level_block, [word_level_block]\n    else:\n        fuzzified_word_level_texts = [\n            default_process(word_level_block.text)\n            for word_level_block in intersecting_word_level_blocks\n        ]\n\n        # Populate the block mapping\n        token_block_mapping = defaultdict(set)\n\n        first_word = tokenized_query[0]\n        last_word = tokenized_query[-1]\n\n        for token in tokenized_query:\n            fuzzified_token = default_process(token)\n            for i, word_level_block in enumerate(intersecting_word_level_blocks):\n                if fuzz.ratio(fuzzified_token, fuzzified_word_level_texts[i]) &gt; 87.5:\n                    token_block_mapping[token].add(i)\n\n        graph = networkx.DiGraph()\n        prev = tokenized_query[0]\n\n        for i in token_block_mapping[prev]:\n            graph.add_node(i)\n\n        for token in tokenized_query[1:]:\n            for prev_block in token_block_mapping[prev]:\n                for block in sorted(token_block_mapping[token]):\n                    if block &gt; prev_block:\n                        weight = (\n                            (block - prev_block) ** 2\n                        )  # Square the distance to penalize large jumps, which encourages reading order\n                        graph.add_edge(prev_block, block, weight=weight)\n\n            prev = token\n\n        # Get every combination of first and last word\n        first_word_blocks = token_block_mapping[first_word]\n        last_word_blocks = token_block_mapping[last_word]\n\n        combinations = sorted(\n            [(x, y) for x in first_word_blocks for y in last_word_blocks if x &lt; y],\n            key=lambda x: abs(x[1] - x[0]),\n        )\n\n        for start, end in combinations:\n            try:\n                path = networkx.shortest_path(graph, start, end, weight=\"weight\")\n            except networkx.NetworkXNoPath:\n                continue\n            except Exception:\n                continue\n\n            matching_blocks = [intersecting_word_level_blocks[i] for i in path]\n\n            merged_bbox = NormBBox.combine(\n                *[word_level_block.bounding_box for word_level_block in matching_blocks]\n            )\n\n            merged_text = \"\"\n\n            for word_level_block in matching_blocks:\n                merged_text += word_level_block.text\n                if not word_level_block.text.endswith(\" \"):\n                    merged_text += \" \"  # Ensure there is a space between words\n\n            return (\n                TextBlock(\n                    text=merged_text,\n                    type=\"block\",\n                    bounding_box=merged_bbox,\n                    metadata=source_block.metadata,\n                ),\n                matching_blocks,\n            )\n</code></pre>"},{"location":"reference/provenance/#docprompt.provenance.util.word_tokenize","title":"<code>word_tokenize(text)</code>","text":"<p>Tokenize a string into words.</p> Source code in <code>docprompt/provenance/util.py</code> <pre><code>def word_tokenize(text: str) -&gt; List[str]:\n    \"\"\"\n    Tokenize a string into words.\n    \"\"\"\n    return re.split(r\"\\s+\", text)\n</code></pre>"},{"location":"reference/provenance/search/","title":"search","text":""},{"location":"reference/provenance/search/#docprompt.provenance.search.DocumentProvenanceLocator","title":"<code>DocumentProvenanceLocator</code>  <code>dataclass</code>","text":"Source code in <code>docprompt/provenance/search.py</code> <pre><code>@dataclass\nclass DocumentProvenanceLocator:\n    document_name: str\n    search_index: \"tantivy.Index\"\n    block_mapping: Dict[int, OcrPageResult] = field(repr=False)\n    geo_index: DocumentProvenanceGeoMap = field(repr=False)\n\n    @classmethod\n    def from_document_node(cls, document_node: \"DocumentNode\"):\n        # TODO: See if we can remove the ocr_results attribute from the\n        # PageNode and just use the metadata.task_result[\"&lt;provider&gt;_ocr\"],\n        # result of the OCR task instead.\n\n        index = create_tantivy_document_wise_block_index()\n        block_mapping_dict = {}\n        geo_index_dict: DocumentProvenanceGeoMap = {}\n\n        writer = index.writer()\n\n        for page_node in document_node.page_nodes:\n            if (\n                not page_node.ocr_results.result\n                or not page_node.ocr_results.result.block_level_blocks\n            ):\n                continue\n\n            ocr_result = page_node.ocr_results.result\n\n            for idx, text_block in enumerate(ocr_result.block_level_blocks):\n                writer.add_document(\n                    tantivy.Document(\n                        page_number=page_node.page_number,\n                        block_type=text_block.type,\n                        block_page_idx=idx,\n                        content=text_block.text,\n                    )\n                )\n\n            for granularity in [\"word\", \"line\", \"block\"]:\n                text_blocks = getattr(ocr_result, f\"{granularity}_level_blocks\", [])\n\n                bounding_boxes = [text_block.bounding_box for text_block in text_blocks]\n\n                if bounding_boxes:\n                    r_tree = RTreeIndex(\n                        insert_generator(bounding_boxes), fill_factor=0.9\n                    )\n                else:\n                    r_tree = RTreeIndex()\n\n                if page_node.page_number not in geo_index_dict:\n                    geo_index_dict[page_node.page_number] = {}\n\n                geo_index_dict[page_node.page_number][granularity] = r_tree  # type: ignore\n\n            block_mapping_dict[page_node.page_number] = ocr_result\n\n        writer.commit()\n        index.reload()\n\n        return cls(\n            document_name=document_node.document.name,\n            search_index=index,\n            block_mapping=block_mapping_dict,\n            geo_index=geo_index_dict,\n        )\n\n    def _construct_tantivy_query(\n        self, query: str, page_number: Optional[int] = None\n    ) -&gt; tantivy.Query:\n        query = preprocess_query_text(query)\n\n        if page_number is None:\n            return self.search_index.parse_query(f'content:\"{query}\"')\n        else:\n            return self.search_index.parse_query(\n                f'(page_number:{page_number}) AND content:\"{query}\"'\n            )\n\n    def get_k_nearest_blocks(\n        self,\n        bbox: NormBBox,\n        page_number: int,\n        k: int,\n        granularity: BlockGranularity = \"block\",\n    ) -&gt; List[TextBlock]:\n        \"\"\"\n        Get the k nearest text blocks to a given bounding box\n        \"\"\"\n        search_tuple = construct_valid_rtree_tuple(bbox)\n\n        word_level_bbox_indices = list(\n            self.geo_index[page_number][granularity].nearest(\n                search_tuple, num_results=k\n            )\n        )\n\n        block_mapping = self.block_mapping[page_number]\n\n        nearest_blocks = [\n            getattr(block_mapping, granularity + \"s\")[idx]\n            for idx in word_level_bbox_indices\n        ]\n\n        nearest_blocks.sort(key=lambda x: (x.bounding_box.top, x.bounding_box.x0))\n\n        return [x for x in nearest_blocks if x.bounding_box != bbox]\n\n    def get_overlapping_blocks(\n        self, bbox: NormBBox, page_number: int, granularity: BlockGranularity = \"block\"\n    ) -&gt; List[TextBlock]:\n        \"\"\"\n        Get the text blocks that overlap with a given bounding box\n        \"\"\"\n        search_tuple = construct_valid_rtree_tuple(bbox)\n\n        bbox_indices = list(\n            self.geo_index[page_number][granularity].intersection(search_tuple)\n        )\n\n        block_mapping = self.block_mapping[page_number]\n\n        overlapping_blocks = [\n            getattr(block_mapping, f\"{granularity}_level_blocks\")[idx]\n            for idx in bbox_indices\n        ]\n\n        overlapping_blocks.sort(key=lambda x: (x.bounding_box.top, x.bounding_box.x0))\n\n        return [x for x in overlapping_blocks if x.bounding_box != bbox]\n\n    def search_raw(self, raw_query: str) -&gt; List[str]:\n        \"\"\"\n        Search for a piece of text using a raw query\n\n        Args:\n            query: The text to search for\n            page_number: The page number to search on\n        \"\"\"\n        parsed_query = self.search_index.parse_query(raw_query)\n\n        searcher = self.search_index.searcher()\n\n        search_results = searcher.search(parsed_query, limit=100)\n\n        results = []\n\n        for score, doc_address in search_results.hits:\n            doc = searcher.doc(doc_address)\n\n            result_page_number = doc[\"page_number\"][0]\n            result_block_page_idx = doc[\"block_page_idx\"][0]\n            block_mapping = self.block_mapping[result_page_number]\n\n            source_block: TextBlock = block_mapping.block_level_blocks[\n                result_block_page_idx\n            ]\n\n            results.append(source_block.text)\n\n        return results\n\n    def refine_query_to_word_level(\n        self, query: str, page_number: int, enclosing_block: TextBlock\n    ):\n        \"\"\"\n        Refine a query to the word level\n        \"\"\"\n        search_tuple = construct_valid_rtree_tuple(enclosing_block.bounding_box)\n\n        word_level_bbox_indices = list(\n            self.geo_index[page_number][\"word\"].intersection(search_tuple)\n        )\n        word_level_blocks_in_original_bbox = [\n            self.block_mapping[page_number].word_level_blocks[idx]\n            for idx in word_level_bbox_indices\n        ]\n\n        refine_result = refine_block_to_word_level(\n            source_block=enclosing_block,\n            intersecting_word_level_blocks=word_level_blocks_in_original_bbox,\n            query=query,\n        )\n\n        return refine_result\n\n    def search(\n        self,\n        query: str,\n        page_number: Optional[int] = None,\n        *,\n        refine_to_word: bool = True,\n        require_exact_match: bool = True,\n    ) -&gt; List[ProvenanceSource]:\n        \"\"\"\n        Search for a piece of text in the document and return the source of it\n\n        Args:\n            query: The text to search for\n            page_number: The page number to search on\n            refine_to_word: Whether to refine the search to the word level\n            require_exact_match: Whether to require null results if `refine_to_word` is True and no exact match is found\n        \"\"\"\n        search_query = self._construct_tantivy_query(query, page_number)\n\n        searcher = self.search_index.searcher()\n\n        search_results = searcher.search(search_query, limit=100)\n\n        results = []\n\n        for score, doc_address in search_results.hits:\n            doc = searcher.doc(doc_address)\n\n            result_page_number = doc[\"page_number\"][0]\n            result_block_page_idx = doc[\"block_page_idx\"][0]\n            block_mapping = self.block_mapping[result_page_number]\n\n            source_block: TextBlock = block_mapping.block_level_blocks[\n                result_block_page_idx\n            ]\n\n            source_blocks = [source_block]\n            principal_block = source_block\n\n            if refine_to_word:\n                refine_result = self.refine_query_to_word_level(\n                    query=query,\n                    page_number=result_page_number,\n                    enclosing_block=source_block,\n                )\n\n                if refine_result is not None:\n                    principal_block, source_blocks = refine_result\n                elif require_exact_match:\n                    continue\n\n            source = ProvenanceSource(\n                document_name=self.document_name,\n                page_number=result_page_number,\n                text_location=PageTextLocation(\n                    source_blocks=source_blocks,\n                    text=query,\n                    score=score,\n                    granularity=\"block\",\n                    merged_source_block=principal_block,\n                ),\n            )\n            results.append(source)\n\n        results.sort(key=lambda x: x.page_number)\n\n        return results\n\n    def search_n_best(\n        self, query: str, n: int = 3, mode: SearchBestModes = \"shortest_text\"\n    ) -&gt; List[ProvenanceSource]:\n        results = self.search(query)\n\n        if not results:\n            return []\n\n        if mode == \"shortest_text\":\n            score_func = lambda x: len(x.source_block.text)  # noqa: E731\n        elif mode == \"longest_text\":\n            score_func = lambda x: -len(x[0].source_block.text)  # noqa: E731\n        elif mode == \"highest_score\":\n            score_func = lambda x: x[1]  # noqa: E731\n        else:\n            raise ValueError(f\"Unknown mode {mode}\")\n\n        results.sort(key=score_func)\n\n        return results[:n]\n</code></pre>"},{"location":"reference/provenance/search/#docprompt.provenance.search.DocumentProvenanceLocator.get_k_nearest_blocks","title":"<code>get_k_nearest_blocks(bbox, page_number, k, granularity='block')</code>","text":"<p>Get the k nearest text blocks to a given bounding box</p> Source code in <code>docprompt/provenance/search.py</code> <pre><code>def get_k_nearest_blocks(\n    self,\n    bbox: NormBBox,\n    page_number: int,\n    k: int,\n    granularity: BlockGranularity = \"block\",\n) -&gt; List[TextBlock]:\n    \"\"\"\n    Get the k nearest text blocks to a given bounding box\n    \"\"\"\n    search_tuple = construct_valid_rtree_tuple(bbox)\n\n    word_level_bbox_indices = list(\n        self.geo_index[page_number][granularity].nearest(\n            search_tuple, num_results=k\n        )\n    )\n\n    block_mapping = self.block_mapping[page_number]\n\n    nearest_blocks = [\n        getattr(block_mapping, granularity + \"s\")[idx]\n        for idx in word_level_bbox_indices\n    ]\n\n    nearest_blocks.sort(key=lambda x: (x.bounding_box.top, x.bounding_box.x0))\n\n    return [x for x in nearest_blocks if x.bounding_box != bbox]\n</code></pre>"},{"location":"reference/provenance/search/#docprompt.provenance.search.DocumentProvenanceLocator.get_overlapping_blocks","title":"<code>get_overlapping_blocks(bbox, page_number, granularity='block')</code>","text":"<p>Get the text blocks that overlap with a given bounding box</p> Source code in <code>docprompt/provenance/search.py</code> <pre><code>def get_overlapping_blocks(\n    self, bbox: NormBBox, page_number: int, granularity: BlockGranularity = \"block\"\n) -&gt; List[TextBlock]:\n    \"\"\"\n    Get the text blocks that overlap with a given bounding box\n    \"\"\"\n    search_tuple = construct_valid_rtree_tuple(bbox)\n\n    bbox_indices = list(\n        self.geo_index[page_number][granularity].intersection(search_tuple)\n    )\n\n    block_mapping = self.block_mapping[page_number]\n\n    overlapping_blocks = [\n        getattr(block_mapping, f\"{granularity}_level_blocks\")[idx]\n        for idx in bbox_indices\n    ]\n\n    overlapping_blocks.sort(key=lambda x: (x.bounding_box.top, x.bounding_box.x0))\n\n    return [x for x in overlapping_blocks if x.bounding_box != bbox]\n</code></pre>"},{"location":"reference/provenance/search/#docprompt.provenance.search.DocumentProvenanceLocator.refine_query_to_word_level","title":"<code>refine_query_to_word_level(query, page_number, enclosing_block)</code>","text":"<p>Refine a query to the word level</p> Source code in <code>docprompt/provenance/search.py</code> <pre><code>def refine_query_to_word_level(\n    self, query: str, page_number: int, enclosing_block: TextBlock\n):\n    \"\"\"\n    Refine a query to the word level\n    \"\"\"\n    search_tuple = construct_valid_rtree_tuple(enclosing_block.bounding_box)\n\n    word_level_bbox_indices = list(\n        self.geo_index[page_number][\"word\"].intersection(search_tuple)\n    )\n    word_level_blocks_in_original_bbox = [\n        self.block_mapping[page_number].word_level_blocks[idx]\n        for idx in word_level_bbox_indices\n    ]\n\n    refine_result = refine_block_to_word_level(\n        source_block=enclosing_block,\n        intersecting_word_level_blocks=word_level_blocks_in_original_bbox,\n        query=query,\n    )\n\n    return refine_result\n</code></pre>"},{"location":"reference/provenance/search/#docprompt.provenance.search.DocumentProvenanceLocator.search","title":"<code>search(query, page_number=None, *, refine_to_word=True, require_exact_match=True)</code>","text":"<p>Search for a piece of text in the document and return the source of it</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The text to search for</p> required <code>page_number</code> <code>Optional[int]</code> <p>The page number to search on</p> <code>None</code> <code>refine_to_word</code> <code>bool</code> <p>Whether to refine the search to the word level</p> <code>True</code> <code>require_exact_match</code> <code>bool</code> <p>Whether to require null results if <code>refine_to_word</code> is True and no exact match is found</p> <code>True</code> Source code in <code>docprompt/provenance/search.py</code> <pre><code>def search(\n    self,\n    query: str,\n    page_number: Optional[int] = None,\n    *,\n    refine_to_word: bool = True,\n    require_exact_match: bool = True,\n) -&gt; List[ProvenanceSource]:\n    \"\"\"\n    Search for a piece of text in the document and return the source of it\n\n    Args:\n        query: The text to search for\n        page_number: The page number to search on\n        refine_to_word: Whether to refine the search to the word level\n        require_exact_match: Whether to require null results if `refine_to_word` is True and no exact match is found\n    \"\"\"\n    search_query = self._construct_tantivy_query(query, page_number)\n\n    searcher = self.search_index.searcher()\n\n    search_results = searcher.search(search_query, limit=100)\n\n    results = []\n\n    for score, doc_address in search_results.hits:\n        doc = searcher.doc(doc_address)\n\n        result_page_number = doc[\"page_number\"][0]\n        result_block_page_idx = doc[\"block_page_idx\"][0]\n        block_mapping = self.block_mapping[result_page_number]\n\n        source_block: TextBlock = block_mapping.block_level_blocks[\n            result_block_page_idx\n        ]\n\n        source_blocks = [source_block]\n        principal_block = source_block\n\n        if refine_to_word:\n            refine_result = self.refine_query_to_word_level(\n                query=query,\n                page_number=result_page_number,\n                enclosing_block=source_block,\n            )\n\n            if refine_result is not None:\n                principal_block, source_blocks = refine_result\n            elif require_exact_match:\n                continue\n\n        source = ProvenanceSource(\n            document_name=self.document_name,\n            page_number=result_page_number,\n            text_location=PageTextLocation(\n                source_blocks=source_blocks,\n                text=query,\n                score=score,\n                granularity=\"block\",\n                merged_source_block=principal_block,\n            ),\n        )\n        results.append(source)\n\n    results.sort(key=lambda x: x.page_number)\n\n    return results\n</code></pre>"},{"location":"reference/provenance/search/#docprompt.provenance.search.DocumentProvenanceLocator.search_raw","title":"<code>search_raw(raw_query)</code>","text":"<p>Search for a piece of text using a raw query</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <p>The text to search for</p> required <code>page_number</code> <p>The page number to search on</p> required Source code in <code>docprompt/provenance/search.py</code> <pre><code>def search_raw(self, raw_query: str) -&gt; List[str]:\n    \"\"\"\n    Search for a piece of text using a raw query\n\n    Args:\n        query: The text to search for\n        page_number: The page number to search on\n    \"\"\"\n    parsed_query = self.search_index.parse_query(raw_query)\n\n    searcher = self.search_index.searcher()\n\n    search_results = searcher.search(parsed_query, limit=100)\n\n    results = []\n\n    for score, doc_address in search_results.hits:\n        doc = searcher.doc(doc_address)\n\n        result_page_number = doc[\"page_number\"][0]\n        result_block_page_idx = doc[\"block_page_idx\"][0]\n        block_mapping = self.block_mapping[result_page_number]\n\n        source_block: TextBlock = block_mapping.block_level_blocks[\n            result_block_page_idx\n        ]\n\n        results.append(source_block.text)\n\n    return results\n</code></pre>"},{"location":"reference/provenance/source/","title":"source","text":""},{"location":"reference/provenance/source/#docprompt.provenance.source.PageTextLocation","title":"<code>PageTextLocation</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Specifies the location of a piece of text in a page</p> Source code in <code>docprompt/provenance/source.py</code> <pre><code>class PageTextLocation(BaseModel):\n    \"\"\"\n    Specifies the location of a piece of text in a page\n    \"\"\"\n\n    source_blocks: List[TextBlock] = Field(\n        description=\"The source text blocks\", repr=False\n    )\n    text: str  # Sometimes the source text is less than the textblock's text.\n    score: float\n    granularity: Literal[\"word\", \"line\", \"block\"] = \"block\"\n\n    merged_source_block: Optional[TextBlock] = Field(default=None)\n</code></pre>"},{"location":"reference/provenance/source/#docprompt.provenance.source.ProvenanceSource","title":"<code>ProvenanceSource</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Bundled with some data, specifies exactly where a piece of verbatim text came from in a document.</p> Source code in <code>docprompt/provenance/source.py</code> <pre><code>class ProvenanceSource(BaseModel):\n    \"\"\"\n    Bundled with some data, specifies exactly where a piece of verbatim text came from\n    in a document.\n    \"\"\"\n\n    document_name: str\n    page_number: PositiveInt\n    text_location: Optional[PageTextLocation] = None\n\n    @computed_field  # type: ignore\n    @property\n    def source_block(self) -&gt; Optional[TextBlock]:\n        if self.text_location:\n            if self.text_location.merged_source_block:\n                return self.text_location.merged_source_block\n            if self.text_location.source_blocks:\n                return self.text_location.source_blocks[0]\n\n            return None\n\n    @property\n    def text(self) -&gt; str:\n        if self.text_location:\n            return \"\\n\".join([block.text for block in self.text_location.source_blocks])\n\n        return \"\"\n</code></pre>"},{"location":"reference/provenance/util/","title":"util","text":""},{"location":"reference/provenance/util/#docprompt.provenance.util.insert_generator","title":"<code>insert_generator(bboxes, data=None)</code>","text":"<p>Make an iterator that yields tuples of (id, bbox, data) for insertion into an RTree index which improves performance massively.</p> Source code in <code>docprompt/provenance/util.py</code> <pre><code>def insert_generator(bboxes: List[NormBBox], data: Optional[Iterable[Any]] = None):\n    \"\"\"\n    Make an iterator that yields tuples of (id, bbox, data) for insertion into an RTree index\n    which improves performance massively.\n    \"\"\"\n    data = data or [None] * len(bboxes)\n\n    for idx, (bbox, data_item) in enumerate(zip(bboxes, data)):\n        yield (idx, construct_valid_rtree_tuple(bbox), data_item)\n</code></pre>"},{"location":"reference/provenance/util/#docprompt.provenance.util.preprocess_query_text","title":"<code>preprocess_query_text(text)</code>","text":"<p>Improve matching ability by applying some preprocessing to the query text.</p> Source code in <code>docprompt/provenance/util.py</code> <pre><code>def preprocess_query_text(text: str) -&gt; str:\n    \"\"\"\n    Improve matching ability by applying some preprocessing to the query text.\n    \"\"\"\n    for regex in _prefix_regexs:\n        text = regex.sub(\"\", text)\n\n    text = text.strip()\n\n    text = text.replace('\"', \"\")\n\n    return text\n</code></pre>"},{"location":"reference/provenance/util/#docprompt.provenance.util.refine_block_to_word_level","title":"<code>refine_block_to_word_level(source_block, intersecting_word_level_blocks, query)</code>","text":"<p>Create a new text block by merging the intersecting word level blocks that match the query.</p> Source code in <code>docprompt/provenance/util.py</code> <pre><code>def refine_block_to_word_level(\n    source_block: TextBlock,\n    intersecting_word_level_blocks: List[TextBlock],\n    query: str,\n):\n    \"\"\"\n    Create a new text block by merging the intersecting word level blocks that\n    match the query.\n\n    \"\"\"\n    intersecting_word_level_blocks.sort(\n        key=lambda x: (x.bounding_box.top, x.bounding_box.x0)\n    )\n\n    tokenized_query = word_tokenize(query)\n\n    if len(tokenized_query) == 1:\n        fuzzified = default_process(tokenized_query[0])\n        for word_level_block in intersecting_word_level_blocks:\n            if fuzz.ratio(fuzzified, default_process(word_level_block.text)) &gt; 87.5:\n                return word_level_block, [word_level_block]\n    else:\n        fuzzified_word_level_texts = [\n            default_process(word_level_block.text)\n            for word_level_block in intersecting_word_level_blocks\n        ]\n\n        # Populate the block mapping\n        token_block_mapping = defaultdict(set)\n\n        first_word = tokenized_query[0]\n        last_word = tokenized_query[-1]\n\n        for token in tokenized_query:\n            fuzzified_token = default_process(token)\n            for i, word_level_block in enumerate(intersecting_word_level_blocks):\n                if fuzz.ratio(fuzzified_token, fuzzified_word_level_texts[i]) &gt; 87.5:\n                    token_block_mapping[token].add(i)\n\n        graph = networkx.DiGraph()\n        prev = tokenized_query[0]\n\n        for i in token_block_mapping[prev]:\n            graph.add_node(i)\n\n        for token in tokenized_query[1:]:\n            for prev_block in token_block_mapping[prev]:\n                for block in sorted(token_block_mapping[token]):\n                    if block &gt; prev_block:\n                        weight = (\n                            (block - prev_block) ** 2\n                        )  # Square the distance to penalize large jumps, which encourages reading order\n                        graph.add_edge(prev_block, block, weight=weight)\n\n            prev = token\n\n        # Get every combination of first and last word\n        first_word_blocks = token_block_mapping[first_word]\n        last_word_blocks = token_block_mapping[last_word]\n\n        combinations = sorted(\n            [(x, y) for x in first_word_blocks for y in last_word_blocks if x &lt; y],\n            key=lambda x: abs(x[1] - x[0]),\n        )\n\n        for start, end in combinations:\n            try:\n                path = networkx.shortest_path(graph, start, end, weight=\"weight\")\n            except networkx.NetworkXNoPath:\n                continue\n            except Exception:\n                continue\n\n            matching_blocks = [intersecting_word_level_blocks[i] for i in path]\n\n            merged_bbox = NormBBox.combine(\n                *[word_level_block.bounding_box for word_level_block in matching_blocks]\n            )\n\n            merged_text = \"\"\n\n            for word_level_block in matching_blocks:\n                merged_text += word_level_block.text\n                if not word_level_block.text.endswith(\" \"):\n                    merged_text += \" \"  # Ensure there is a space between words\n\n            return (\n                TextBlock(\n                    text=merged_text,\n                    type=\"block\",\n                    bounding_box=merged_bbox,\n                    metadata=source_block.metadata,\n                ),\n                matching_blocks,\n            )\n</code></pre>"},{"location":"reference/provenance/util/#docprompt.provenance.util.word_tokenize","title":"<code>word_tokenize(text)</code>","text":"<p>Tokenize a string into words.</p> Source code in <code>docprompt/provenance/util.py</code> <pre><code>def word_tokenize(text: str) -&gt; List[str]:\n    \"\"\"\n    Tokenize a string into words.\n    \"\"\"\n    return re.split(r\"\\s+\", text)\n</code></pre>"},{"location":"reference/schema/","title":"Index","text":""},{"location":"reference/schema/#docprompt.schema.document","title":"<code>document</code>","text":""},{"location":"reference/schema/#docprompt.schema.document.PdfDocument","title":"<code>PdfDocument</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a PDF document</p> Source code in <code>docprompt/schema/document.py</code> <pre><code>class PdfDocument(BaseModel):\n    \"\"\"\n    Represents a PDF document\n    \"\"\"\n\n    name: str = Field(description=\"The name of the document\")\n    file_bytes: bytes = Field(description=\"The bytes of the document\", repr=False)\n    file_path: Optional[str] = None\n\n    password: Optional[SecretStr] = None\n\n    def __len__(self):\n        return self.num_pages\n\n    def __hash__(self):\n        return hash(self.document_hash)\n\n    @computed_field\n    @cached_property\n    def page_count(self) -&gt; PositiveInt:\n        from docprompt.utils.util import get_page_count\n\n        return get_page_count(self.file_bytes)\n\n    @property\n    def num_pages(self):\n        return self.page_count\n\n    @property\n    def bytes_per_page(self):\n        return len(self.file_bytes) / self.num_pages\n\n    @computed_field\n    @cached_property\n    def document_hash(self) -&gt; str:\n        from docprompt.utils.util import hash_from_bytes\n\n        return hash_from_bytes(self.file_bytes)\n\n    @field_serializer(\"file_bytes\")\n    def serialize_file_bytes(self, v: bytes, _info):\n        compressed = gzip.compress(v)\n\n        return base64.b64encode(compressed).decode(\"utf-8\")\n\n    @field_validator(\"file_bytes\")\n    def validate_file_bytes(cls, v: bytes):\n        if not isinstance(v, bytes):\n            raise ValueError(\"File bytes must be bytes\")\n\n        if len(v) == 0:\n            raise ValueError(\"File bytes must not be empty\")\n\n        if filetype.guess_mime(v) == \"text/plain\":\n            v = base64.b64decode(v, validate=True)\n\n        if filetype.guess_mime(v) == \"application/gzip\":\n            v = gzip.decompress(v)\n\n        if filetype.guess_mime(v) != \"application/pdf\":\n            raise ValueError(\"File bytes must be a PDF\")\n\n        return v\n\n    @classmethod\n    def from_path(cls, file_path: Union[PathLike, str]):\n        file_path = Path(file_path)\n\n        if not file_path.is_file():\n            raise ValueError(f\"File path {file_path} is not a file\")\n\n        file_bytes = file_path.read_bytes()\n\n        return cls(name=file_path.name, file_path=str(file_path), file_bytes=file_bytes)\n\n    @classmethod\n    def from_bytes(cls, file_bytes: bytes, name: Optional[str] = None):\n        if name is None:\n            name = f\"PDF-{datetime.now().isoformat()}.pdf\"\n\n        return cls(name=name, file_bytes=file_bytes)\n\n    def get_bytes(self) -&gt; bytes:\n        return self.file_bytes  # Deprecated\n\n    @property\n    def path(self):\n        return self.file_path\n\n    def get_page_render_size(\n        self, page_number: int, dpi: int = DEFAULT_DPI\n    ) -&gt; Tuple[int, int]:\n        \"\"\"\n        Returns the render size of a page in pixels\n        \"\"\"\n        return get_page_render_size_from_bytes(self.get_bytes(), page_number, dpi=dpi)\n\n    def to_compressed_bytes(self, compression_kwargs: dict = {}) -&gt; bytes:\n        \"\"\"\n        Compresses the document using Ghostscript\n        \"\"\"\n        with self.as_tempfile() as temp_path:\n            return compress_pdf_to_bytes(temp_path, **compression_kwargs)\n\n    def rasterize_page(\n        self,\n        page_number: int,\n        *,\n        dpi: int = DEFAULT_DPI,\n        downscale_size: Optional[Tuple[int, int]] = None,\n        resize_mode: ResizeModes = \"thumbnail\",\n        max_file_size_bytes: Optional[int] = None,\n        resize_aspect_ratios: Optional[Iterable[AspectRatioRule]] = None,\n        do_convert: bool = False,\n        image_convert_mode: str = \"L\",\n        do_quantize: bool = False,\n        quantize_color_count: int = 8,\n        return_mode: Literal[\"pil\", \"bytes\"] = \"bytes\",\n    ):\n        \"\"\"\n        Rasterizes a page of the document using Pdfium\n        \"\"\"\n        if page_number &lt;= 0 or page_number &gt; self.num_pages:\n            raise ValueError(f\"Page number must be between 0 and {self.num_pages}\")\n\n        post_process_fn = None\n\n        if any(\n            (\n                downscale_size,\n                max_file_size_bytes,\n                resize_aspect_ratios,\n                do_convert,\n                do_quantize,\n            )\n        ):\n            post_process_fn = partial(\n                process_raster_image,\n                resize_width=downscale_size[0] if downscale_size else None,\n                resize_height=downscale_size[1] if downscale_size else None,\n                resize_mode=resize_mode,\n                resize_aspect_ratios=resize_aspect_ratios,\n                do_convert=do_convert,\n                image_convert_mode=image_convert_mode,\n                do_quantize=do_quantize,\n                quantize_color_count=quantize_color_count,\n                max_file_size_bytes=max_file_size_bytes,\n            )\n\n        rastered = rasterize_page_with_pdfium(\n            self.file_bytes,\n            page_number,\n            return_mode=return_mode,\n            post_process_fn=post_process_fn,\n            scale=(1 / 72) * dpi,\n        )\n\n        return rastered\n\n    def rasterize_page_to_data_uri(\n        self,\n        page_number: int,\n        *,\n        dpi: int = DEFAULT_DPI,\n        downscale_size: Optional[Tuple[int, int]] = None,\n        resize_mode: ResizeModes = \"thumbnail\",\n        max_file_size_bytes: Optional[int] = None,\n        resize_aspect_ratios: Optional[Iterable[AspectRatioRule]] = None,\n        do_convert: bool = False,\n        image_convert_mode: str = \"L\",\n        do_quantize: bool = False,\n        quantize_color_count: int = 8,\n        render_grayscale: bool = False,\n    ) -&gt; str:\n        \"\"\"\n        Rasterizes a page of the document using Pdfium and returns a data URI, which can\n        be embedded into HTML or passed to large language models\n        \"\"\"\n        image_bytes = self.rasterize_page(\n            page_number,\n            dpi=dpi,\n            downscale_size=downscale_size,\n            do_convert=do_convert,\n            image_convert_mode=image_convert_mode,\n            do_quantize=do_quantize,\n            quantize_color_count=quantize_color_count,\n            resize_mode=resize_mode,\n            max_file_size_bytes=max_file_size_bytes,\n            resize_aspect_ratios=resize_aspect_ratios,\n            return_mode=\"bytes\",\n        )\n        return f\"data:image/png;base64,{base64.b64encode(image_bytes).decode('utf-8')}\"\n\n    def rasterize_pdf(\n        self,\n        dpi: int = DEFAULT_DPI,\n        downscale_size: Optional[Tuple[int, int]] = None,\n        resize_mode: ResizeModes = \"thumbnail\",\n        max_file_size_bytes: Optional[int] = None,\n        resize_aspect_ratios: Optional[Iterable[AspectRatioRule]] = None,\n        do_convert: bool = False,\n        image_convert_mode: str = \"L\",\n        do_quantize: bool = False,\n        quantize_color_count: int = 8,\n        return_mode: Literal[\"pil\", \"bytes\"] = \"bytes\",\n        render_grayscale: bool = False,\n    ) -&gt; Dict[int, bytes]:\n        \"\"\"\n        Rasterizes the entire document using Pdfium\n        \"\"\"\n        result = {}\n\n        post_process_fn = None\n\n        if any(\n            (\n                downscale_size,\n                max_file_size_bytes,\n                resize_aspect_ratios,\n                do_convert,\n                do_quantize,\n            )\n        ):\n            post_process_fn = partial(\n                process_raster_image,\n                resize_width=downscale_size[0] if downscale_size else None,\n                resize_height=downscale_size[1] if downscale_size else None,\n                resize_mode=resize_mode,\n                resize_aspect_ratios=resize_aspect_ratios,\n                do_convert=do_convert,\n                image_convert_mode=image_convert_mode,\n                do_quantize=do_quantize,\n                quantize_color_count=quantize_color_count,\n                max_file_size_bytes=max_file_size_bytes,\n            )\n\n        for idx, rastered in enumerate(\n            rasterize_pdf_with_pdfium(\n                self.file_bytes,\n                scale=(1 / 72) * dpi,\n                grayscale=render_grayscale,\n                return_mode=return_mode,\n                post_process_fn=post_process_fn,\n            )\n        ):\n            result[idx + 1] = rastered\n\n        return result\n\n    def split(self, start: Optional[int] = None, stop: Optional[int] = None):\n        \"\"\"\n        Splits a document into multiple documents\n        \"\"\"\n        if start is None and stop is None:\n            raise ValueError(\"Must specify either start or stop\")\n\n        start = start or 0\n\n        from docprompt.utils.splitter import split_pdf_to_bytes\n\n        split_bytes = split_pdf_to_bytes(\n            self.file_bytes, start_page=start, stop_page=stop\n        )\n\n        return Document.from_bytes(split_bytes, name=self.name)\n\n    def as_tempfile(self, **kwargs):\n        \"\"\"\n        Returns a tempfile of the document\n        \"\"\"\n\n        @contextmanager\n        def tempfile_context() -&gt; Generator[str, None, None]:\n            tempfile_kwargs = {\"mode\": \"wb\", \"delete\": True, \"suffix\": \".pdf\", **kwargs}\n\n            with tempfile.NamedTemporaryFile(**tempfile_kwargs) as f:\n                f.write(self.file_bytes)\n                f.flush()\n                yield f.name\n\n        return tempfile_context()\n\n    def write_to_path(self, path: Union[PathLike, str], **kwargs):\n        \"\"\"\n        Writes the document to a path\n        \"\"\"\n        path = Path(path)\n\n        if path.is_dir():\n            path = path / self.name\n\n        with path.open(\"wb\") as f:\n            f.write(self.file_bytes)\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.document.PdfDocument.as_tempfile","title":"<code>as_tempfile(**kwargs)</code>","text":"<p>Returns a tempfile of the document</p> Source code in <code>docprompt/schema/document.py</code> <pre><code>def as_tempfile(self, **kwargs):\n    \"\"\"\n    Returns a tempfile of the document\n    \"\"\"\n\n    @contextmanager\n    def tempfile_context() -&gt; Generator[str, None, None]:\n        tempfile_kwargs = {\"mode\": \"wb\", \"delete\": True, \"suffix\": \".pdf\", **kwargs}\n\n        with tempfile.NamedTemporaryFile(**tempfile_kwargs) as f:\n            f.write(self.file_bytes)\n            f.flush()\n            yield f.name\n\n    return tempfile_context()\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.document.PdfDocument.get_page_render_size","title":"<code>get_page_render_size(page_number, dpi=DEFAULT_DPI)</code>","text":"<p>Returns the render size of a page in pixels</p> Source code in <code>docprompt/schema/document.py</code> <pre><code>def get_page_render_size(\n    self, page_number: int, dpi: int = DEFAULT_DPI\n) -&gt; Tuple[int, int]:\n    \"\"\"\n    Returns the render size of a page in pixels\n    \"\"\"\n    return get_page_render_size_from_bytes(self.get_bytes(), page_number, dpi=dpi)\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.document.PdfDocument.rasterize_page","title":"<code>rasterize_page(page_number, *, dpi=DEFAULT_DPI, downscale_size=None, resize_mode='thumbnail', max_file_size_bytes=None, resize_aspect_ratios=None, do_convert=False, image_convert_mode='L', do_quantize=False, quantize_color_count=8, return_mode='bytes')</code>","text":"<p>Rasterizes a page of the document using Pdfium</p> Source code in <code>docprompt/schema/document.py</code> <pre><code>def rasterize_page(\n    self,\n    page_number: int,\n    *,\n    dpi: int = DEFAULT_DPI,\n    downscale_size: Optional[Tuple[int, int]] = None,\n    resize_mode: ResizeModes = \"thumbnail\",\n    max_file_size_bytes: Optional[int] = None,\n    resize_aspect_ratios: Optional[Iterable[AspectRatioRule]] = None,\n    do_convert: bool = False,\n    image_convert_mode: str = \"L\",\n    do_quantize: bool = False,\n    quantize_color_count: int = 8,\n    return_mode: Literal[\"pil\", \"bytes\"] = \"bytes\",\n):\n    \"\"\"\n    Rasterizes a page of the document using Pdfium\n    \"\"\"\n    if page_number &lt;= 0 or page_number &gt; self.num_pages:\n        raise ValueError(f\"Page number must be between 0 and {self.num_pages}\")\n\n    post_process_fn = None\n\n    if any(\n        (\n            downscale_size,\n            max_file_size_bytes,\n            resize_aspect_ratios,\n            do_convert,\n            do_quantize,\n        )\n    ):\n        post_process_fn = partial(\n            process_raster_image,\n            resize_width=downscale_size[0] if downscale_size else None,\n            resize_height=downscale_size[1] if downscale_size else None,\n            resize_mode=resize_mode,\n            resize_aspect_ratios=resize_aspect_ratios,\n            do_convert=do_convert,\n            image_convert_mode=image_convert_mode,\n            do_quantize=do_quantize,\n            quantize_color_count=quantize_color_count,\n            max_file_size_bytes=max_file_size_bytes,\n        )\n\n    rastered = rasterize_page_with_pdfium(\n        self.file_bytes,\n        page_number,\n        return_mode=return_mode,\n        post_process_fn=post_process_fn,\n        scale=(1 / 72) * dpi,\n    )\n\n    return rastered\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.document.PdfDocument.rasterize_page_to_data_uri","title":"<code>rasterize_page_to_data_uri(page_number, *, dpi=DEFAULT_DPI, downscale_size=None, resize_mode='thumbnail', max_file_size_bytes=None, resize_aspect_ratios=None, do_convert=False, image_convert_mode='L', do_quantize=False, quantize_color_count=8, render_grayscale=False)</code>","text":"<p>Rasterizes a page of the document using Pdfium and returns a data URI, which can be embedded into HTML or passed to large language models</p> Source code in <code>docprompt/schema/document.py</code> <pre><code>def rasterize_page_to_data_uri(\n    self,\n    page_number: int,\n    *,\n    dpi: int = DEFAULT_DPI,\n    downscale_size: Optional[Tuple[int, int]] = None,\n    resize_mode: ResizeModes = \"thumbnail\",\n    max_file_size_bytes: Optional[int] = None,\n    resize_aspect_ratios: Optional[Iterable[AspectRatioRule]] = None,\n    do_convert: bool = False,\n    image_convert_mode: str = \"L\",\n    do_quantize: bool = False,\n    quantize_color_count: int = 8,\n    render_grayscale: bool = False,\n) -&gt; str:\n    \"\"\"\n    Rasterizes a page of the document using Pdfium and returns a data URI, which can\n    be embedded into HTML or passed to large language models\n    \"\"\"\n    image_bytes = self.rasterize_page(\n        page_number,\n        dpi=dpi,\n        downscale_size=downscale_size,\n        do_convert=do_convert,\n        image_convert_mode=image_convert_mode,\n        do_quantize=do_quantize,\n        quantize_color_count=quantize_color_count,\n        resize_mode=resize_mode,\n        max_file_size_bytes=max_file_size_bytes,\n        resize_aspect_ratios=resize_aspect_ratios,\n        return_mode=\"bytes\",\n    )\n    return f\"data:image/png;base64,{base64.b64encode(image_bytes).decode('utf-8')}\"\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.document.PdfDocument.rasterize_pdf","title":"<code>rasterize_pdf(dpi=DEFAULT_DPI, downscale_size=None, resize_mode='thumbnail', max_file_size_bytes=None, resize_aspect_ratios=None, do_convert=False, image_convert_mode='L', do_quantize=False, quantize_color_count=8, return_mode='bytes', render_grayscale=False)</code>","text":"<p>Rasterizes the entire document using Pdfium</p> Source code in <code>docprompt/schema/document.py</code> <pre><code>def rasterize_pdf(\n    self,\n    dpi: int = DEFAULT_DPI,\n    downscale_size: Optional[Tuple[int, int]] = None,\n    resize_mode: ResizeModes = \"thumbnail\",\n    max_file_size_bytes: Optional[int] = None,\n    resize_aspect_ratios: Optional[Iterable[AspectRatioRule]] = None,\n    do_convert: bool = False,\n    image_convert_mode: str = \"L\",\n    do_quantize: bool = False,\n    quantize_color_count: int = 8,\n    return_mode: Literal[\"pil\", \"bytes\"] = \"bytes\",\n    render_grayscale: bool = False,\n) -&gt; Dict[int, bytes]:\n    \"\"\"\n    Rasterizes the entire document using Pdfium\n    \"\"\"\n    result = {}\n\n    post_process_fn = None\n\n    if any(\n        (\n            downscale_size,\n            max_file_size_bytes,\n            resize_aspect_ratios,\n            do_convert,\n            do_quantize,\n        )\n    ):\n        post_process_fn = partial(\n            process_raster_image,\n            resize_width=downscale_size[0] if downscale_size else None,\n            resize_height=downscale_size[1] if downscale_size else None,\n            resize_mode=resize_mode,\n            resize_aspect_ratios=resize_aspect_ratios,\n            do_convert=do_convert,\n            image_convert_mode=image_convert_mode,\n            do_quantize=do_quantize,\n            quantize_color_count=quantize_color_count,\n            max_file_size_bytes=max_file_size_bytes,\n        )\n\n    for idx, rastered in enumerate(\n        rasterize_pdf_with_pdfium(\n            self.file_bytes,\n            scale=(1 / 72) * dpi,\n            grayscale=render_grayscale,\n            return_mode=return_mode,\n            post_process_fn=post_process_fn,\n        )\n    ):\n        result[idx + 1] = rastered\n\n    return result\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.document.PdfDocument.split","title":"<code>split(start=None, stop=None)</code>","text":"<p>Splits a document into multiple documents</p> Source code in <code>docprompt/schema/document.py</code> <pre><code>def split(self, start: Optional[int] = None, stop: Optional[int] = None):\n    \"\"\"\n    Splits a document into multiple documents\n    \"\"\"\n    if start is None and stop is None:\n        raise ValueError(\"Must specify either start or stop\")\n\n    start = start or 0\n\n    from docprompt.utils.splitter import split_pdf_to_bytes\n\n    split_bytes = split_pdf_to_bytes(\n        self.file_bytes, start_page=start, stop_page=stop\n    )\n\n    return Document.from_bytes(split_bytes, name=self.name)\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.document.PdfDocument.to_compressed_bytes","title":"<code>to_compressed_bytes(compression_kwargs={})</code>","text":"<p>Compresses the document using Ghostscript</p> Source code in <code>docprompt/schema/document.py</code> <pre><code>def to_compressed_bytes(self, compression_kwargs: dict = {}) -&gt; bytes:\n    \"\"\"\n    Compresses the document using Ghostscript\n    \"\"\"\n    with self.as_tempfile() as temp_path:\n        return compress_pdf_to_bytes(temp_path, **compression_kwargs)\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.document.PdfDocument.write_to_path","title":"<code>write_to_path(path, **kwargs)</code>","text":"<p>Writes the document to a path</p> Source code in <code>docprompt/schema/document.py</code> <pre><code>def write_to_path(self, path: Union[PathLike, str], **kwargs):\n    \"\"\"\n    Writes the document to a path\n    \"\"\"\n    path = Path(path)\n\n    if path.is_dir():\n        path = path / self.name\n\n    with path.open(\"wb\") as f:\n        f.write(self.file_bytes)\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.document.get_page_render_size_from_bytes","title":"<code>get_page_render_size_from_bytes(file_bytes, page_number, dpi=DEFAULT_DPI)</code>","text":"<p>Returns the render size of a page in pixels</p> Source code in <code>docprompt/schema/document.py</code> <pre><code>def get_page_render_size_from_bytes(\n    file_bytes: bytes, page_number: int, dpi: int = DEFAULT_DPI\n):\n    \"\"\"\n    Returns the render size of a page in pixels\n    \"\"\"\n\n    with get_pdfium_document(file_bytes) as pdf:\n        page = pdf.get_page(page_number)\n\n        mediabox = page.get_mediabox()\n\n        base_width = int(mediabox[2] - mediabox[0])\n        base_height = int(mediabox[3] - mediabox[1])\n\n        width = int(base_width * dpi / 72)\n        height = int(base_height * dpi / 72)\n\n        return width, height\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.layout","title":"<code>layout</code>","text":""},{"location":"reference/schema/#docprompt.schema.layout.BoundingPoly","title":"<code>BoundingPoly</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a normalized bounding poly with each value in the range [0, 1]</p> <p>Used for higher order shapes like polygons on a page</p> Source code in <code>docprompt/schema/layout.py</code> <pre><code>class BoundingPoly(BaseModel):\n    \"\"\"\n    Represents a normalized bounding poly with each value in the range [0, 1]\n\n    Used for higher order shapes like polygons on a page\n    \"\"\"\n\n    normalized_vertices: List[Point]\n\n    def __getitem__(self, index):\n        return self.normalized_vertices[index]\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.layout.NormBBox","title":"<code>NormBBox</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a normalized bounding box with each value in the range [0, 1]</p> <p>Where x1 &gt; x0 and bottom &gt; top</p> Source code in <code>docprompt/schema/layout.py</code> <pre><code>class NormBBox(BaseModel):\n    \"\"\"\n    Represents a normalized bounding box with each value in the range [0, 1]\n\n    Where x1 &gt; x0 and bottom &gt; top\n    \"\"\"\n\n    x0: BoundedFloat\n    top: BoundedFloat\n    x1: BoundedFloat\n    bottom: BoundedFloat\n\n    model_config: ConfigDict = {\"json_encoders\": {float: lambda v: round(v, 5)}}\n\n    def as_tuple(self):\n        return (self.x0, self.top, self.x1, self.bottom)\n\n    def __getitem__(self, index):\n        # Lots of if statements to prevent new allocations\n        if index &gt; 3:\n            raise IndexError(\"Index out of range\")\n\n        if index == 0:\n            return self.x0\n        elif index == 1:\n            return self.top\n        elif index == 2:\n            return self.x1\n        elif index == 3:\n            return self.bottom\n\n    def __eq__(self, other):\n        if not isinstance(other, NormBBox):\n            return False\n\n        return self.as_tuple() == other.as_tuple()\n\n    def __hash__(self):\n        return hash(self.as_tuple())\n\n    def __and__(self, other):\n        if not isinstance(other, NormBBox):\n            raise TypeError(\"Can only compute intersection with NormBBox\")\n        # Compute the intersection of two bounding boxes\n        new_x0 = max(self.x0, other.x0)\n        new_top = max(self.top, other.top)\n        new_x1 = min(self.x1, other.x1)\n        new_bottom = min(self.bottom, other.bottom)\n\n        # Check if there is an actual intersection and if the resulting bounding box is valid\n        if new_x0 &lt;= new_x1 and new_top &lt;= new_bottom:\n            return NormBBox(x0=new_x0, top=new_top, x1=new_x1, bottom=new_bottom)\n        else:\n            # Return an empty or non-existent bounding box representation\n            return None\n\n    def __add__(self, other):\n        if not isinstance(other, NormBBox):\n            raise TypeError(\"Can only add NormBBox to NormBBox\")\n\n        return NormBBox(\n            x0=min(self.x0, other.x0),\n            top=min(self.top, other.top),\n            x1=max(self.x1, other.x1),\n            bottom=max(self.bottom, other.bottom),\n        )\n\n    def __contains__(self, other):\n        return (\n            self.x0 &lt;= other.x0\n            and self.top &lt;= other.top\n            and self.x1 &gt;= other.x1\n            and self.bottom &gt;= other.bottom\n        )\n\n    def intersection_over_union(self, other):\n        if not isinstance(other, NormBBox):\n            raise TypeError(\"Can only compute IOU with NormBBox\")\n\n        # Compute the intersection\n        intersection_bbox = self &amp; other\n\n        if intersection_bbox:\n            intersection_area = intersection_bbox.area\n            union_area = self.area + other.area - intersection_area\n            return intersection_area / union_area\n\n        return 0  # No intersection\n\n    def x_overlap(self, other):\n        \"\"\"\n        Get the overlap, between 0 and 1, of the x-axis of two bounding boxes\n        \"\"\"\n        return max(0, min(self.x1, other.x1) - max(self.x0, other.x0))\n\n    def y_overlap(self, other):\n        \"\"\"\n        Get the overlap, between 0 and 1, of the y-axis of two bounding boxes\n        \"\"\"\n        return max(0, min(self.bottom, other.bottom) - max(self.top, other.top))\n\n    @classmethod\n    def combine(cls, *bboxes: \"NormBBox\"):\n        \"\"\"\n        Combines multiple bounding boxes into a single bounding box\n        \"\"\"\n        if len(bboxes) == 0:\n            raise ValueError(\"Must provide at least one bounding box\")\n\n        if len(bboxes) == 1:\n            return bboxes[0]\n\n        working_bbox = bboxes[0]\n        for bbox in bboxes[1:]:\n            working_bbox = working_bbox + bbox\n\n        return working_bbox\n\n    @classmethod\n    def from_bounding_poly(cls, bounding_poly: \"BoundingPoly\"):\n        \"\"\"\n        Returns a NormBBox from a BoundingPoly\n        \"\"\"\n        if len(bounding_poly.normalized_vertices) != 4:\n            raise ValueError(\n                \"BoundingPoly must have 4 vertices for NormBBox conversion\"\n            )\n\n        (\n            top_left,\n            top_right,\n            bottom_right,\n            bottom_left,\n        ) = bounding_poly.normalized_vertices\n\n        return cls(\n            x0=top_left.x,\n            top=top_left.y,\n            x1=bottom_right.x,\n            bottom=bottom_right.y,\n        )\n\n    @property\n    def width(self):\n        return self.x1 - self.x0\n\n    @property\n    def height(self):\n        return self.bottom - self.top\n\n    @property\n    def area(self):\n        return self.width * self.height\n\n    @property\n    def centroid(self):\n        return (self.x0 + self.x1) / 2, (self.top + self.bottom) / 2\n\n    @property\n    def y_center(self):\n        return (self.top + self.bottom) / 2\n\n    @property\n    def x_center(self):\n        return (self.x0 + self.x1) / 2\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.layout.NormBBox.combine","title":"<code>combine(*bboxes)</code>  <code>classmethod</code>","text":"<p>Combines multiple bounding boxes into a single bounding box</p> Source code in <code>docprompt/schema/layout.py</code> <pre><code>@classmethod\ndef combine(cls, *bboxes: \"NormBBox\"):\n    \"\"\"\n    Combines multiple bounding boxes into a single bounding box\n    \"\"\"\n    if len(bboxes) == 0:\n        raise ValueError(\"Must provide at least one bounding box\")\n\n    if len(bboxes) == 1:\n        return bboxes[0]\n\n    working_bbox = bboxes[0]\n    for bbox in bboxes[1:]:\n        working_bbox = working_bbox + bbox\n\n    return working_bbox\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.layout.NormBBox.from_bounding_poly","title":"<code>from_bounding_poly(bounding_poly)</code>  <code>classmethod</code>","text":"<p>Returns a NormBBox from a BoundingPoly</p> Source code in <code>docprompt/schema/layout.py</code> <pre><code>@classmethod\ndef from_bounding_poly(cls, bounding_poly: \"BoundingPoly\"):\n    \"\"\"\n    Returns a NormBBox from a BoundingPoly\n    \"\"\"\n    if len(bounding_poly.normalized_vertices) != 4:\n        raise ValueError(\n            \"BoundingPoly must have 4 vertices for NormBBox conversion\"\n        )\n\n    (\n        top_left,\n        top_right,\n        bottom_right,\n        bottom_left,\n    ) = bounding_poly.normalized_vertices\n\n    return cls(\n        x0=top_left.x,\n        top=top_left.y,\n        x1=bottom_right.x,\n        bottom=bottom_right.y,\n    )\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.layout.NormBBox.x_overlap","title":"<code>x_overlap(other)</code>","text":"<p>Get the overlap, between 0 and 1, of the x-axis of two bounding boxes</p> Source code in <code>docprompt/schema/layout.py</code> <pre><code>def x_overlap(self, other):\n    \"\"\"\n    Get the overlap, between 0 and 1, of the x-axis of two bounding boxes\n    \"\"\"\n    return max(0, min(self.x1, other.x1) - max(self.x0, other.x0))\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.layout.NormBBox.y_overlap","title":"<code>y_overlap(other)</code>","text":"<p>Get the overlap, between 0 and 1, of the y-axis of two bounding boxes</p> Source code in <code>docprompt/schema/layout.py</code> <pre><code>def y_overlap(self, other):\n    \"\"\"\n    Get the overlap, between 0 and 1, of the y-axis of two bounding boxes\n    \"\"\"\n    return max(0, min(self.bottom, other.bottom) - max(self.top, other.top))\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.layout.Point","title":"<code>Point</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a normalized bounding box with each value in the range [0, 1]</p> Source code in <code>docprompt/schema/layout.py</code> <pre><code>class Point(BaseModel):\n    \"\"\"\n    Represents a normalized bounding box with each value in the range [0, 1]\n    \"\"\"\n\n    model_config: ConfigDict = {\"json_encoders\": {float: lambda v: round(v, 5)}}\n\n    x: BoundedFloat\n    y: BoundedFloat\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.layout.TextBlock","title":"<code>TextBlock</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a single block of text, with its bounding box. The bounding box is a tuple of (x0, top, x1, bottom) and is normalized to the page size.</p> Source code in <code>docprompt/schema/layout.py</code> <pre><code>class TextBlock(BaseModel):\n    \"\"\"\n    Represents a single block of text, with its bounding box.\n    The bounding box is a tuple of (x0, top, x1, bottom) and\n    is normalized to the page size.\n    \"\"\"\n\n    model_config: ConfigDict = {\"json_encoders\": {float: lambda v: round(v, 5)}}\n\n    text: str\n    type: SegmentLevels\n    source: TextblockSource = Field(\n        default=\"derived\", description=\"The source of the text block\"\n    )\n\n    # Layout information\n    bounding_box: NormBBox = Field(default=None, repr=False)\n    bounding_poly: Optional[BoundingPoly] = Field(default=None, repr=False)\n    text_spans: Optional[List[TextSpan]] = Field(default=None, repr=False)\n\n    metadata: Optional[TextBlockMetadata] = Field(default_factory=TextBlockMetadata)\n\n    def __getitem__(self, index):\n        return getattr(self, index)\n\n    def __hash__(self):\n        return hash((self.text, self.bounding_box.as_tuple()))\n\n    @property\n    def confidence(self):\n        return self.metadata.confidence\n\n    @property\n    def direction(self):\n        return self.metadata.direction\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.pipeline","title":"<code>pipeline</code>","text":""},{"location":"reference/schema/#docprompt.schema.pipeline.BaseMetadata","title":"<code>BaseMetadata</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>MutableMapping</code>, <code>Generic[TMetadataOwner]</code></p> <p>The base metadata class is utilized for defining a basic yet flexible interface for metadata attached to various fields.</p> The metadata class can be used in two ways <ol> <li>As a dictionary-like object, where metadata is stored in the <code>extra</code> field.</li> <li>As a sub-classed model, where metadata is stored in the fields of the model.</li> </ol> <p>When used out of the box, the metadata class will adobpt dictionary-like behavior. You may easily access different fields of the metadata as if it were a dictionary: <pre><code># Instantiate it with any kwargs you like\nmetadata = BaseMetadata(foo-'bar', cow='moo')\n\nmetadata[\"foo\"]  # \"bar\"\nmetadata[\"cow\"]  # \"moo\"\n\n# Update the value of the key\nmetadata[\"foo\"] = \"fighters\"\n\n# Set new key-value pairs\nmetadata['sheep'] = 'baa'\n</code></pre></p> <p>Otherwise, you may sub-class the metadata class in order to create a more strictly typed metadata model. This is useful when you want to enforce a specific structure for your metadata.</p> <pre><code>class CustomMetadata(BaseMetadata):\n    foo: str\n    cow: str\n\n# Instantiate it with the required fields\nmetadata = CustomMetadata(foo='bar', cow='moo')\n\nmetadata.foo  # \"bar\"\nmetadata.cow  # \"moo\"\n\n# Update the value of the key\nmetadata.foo = \"fighters\"\n\n# Use the extra field to store dynamic metadata\nmetadata.extra['sheep'] = 'baa'\n</code></pre> <p>Additionally, the task results descriptor allows for controlled and easy access to the task results of various tasks that are run on the parent node.</p> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>class BaseMetadata(BaseModel, MutableMapping, Generic[TMetadataOwner]):\n    \"\"\"\n    The base metadata class is utilized for defining a basic yet flexible interface\n    for metadata attached to various fields.\n\n    The metadata class can be used in two ways:\n        1. As a dictionary-like object, where metadata is stored in the `extra` field.\n        2. As a sub-classed model, where metadata is stored in the fields of the model.\n\n    When used out of the box, the metadata class will adobpt dictionary-like behavior. You\n    may easily access different fields of the metadata as if it were a dictionary:\n    ```python\n    # Instantiate it with any kwargs you like\n    metadata = BaseMetadata(foo-'bar', cow='moo')\n\n    metadata[\"foo\"]  # \"bar\"\n    metadata[\"cow\"]  # \"moo\"\n\n    # Update the value of the key\n    metadata[\"foo\"] = \"fighters\"\n\n    # Set new key-value pairs\n    metadata['sheep'] = 'baa'\n    ```\n\n    Otherwise, you may sub-class the metadata class in order to create a more strictly typed\n    metadata model. This is useful when you want to enforce a specific structure for your metadata.\n\n    ```python\n    class CustomMetadata(BaseMetadata):\n        foo: str\n        cow: str\n\n    # Instantiate it with the required fields\n    metadata = CustomMetadata(foo='bar', cow='moo')\n\n    metadata.foo  # \"bar\"\n    metadata.cow  # \"moo\"\n\n    # Update the value of the key\n    metadata.foo = \"fighters\"\n\n    # Use the extra field to store dynamic metadata\n    metadata.extra['sheep'] = 'baa'\n    ```\n\n    Additionally, the task results descriptor allows for controlled and easy access to the task results\n    of various tasks that are run on the parent node.\n    \"\"\"\n\n    extra: Dict[str, Any] = Field(..., default_factory=dict, repr=False)\n\n    _task_results: TaskResultsDescriptor = PrivateAttr(\n        default_factory=TaskResultsDescriptor\n    )\n\n    _owner: TMetadataOwner = PrivateAttr()\n\n    @property\n    def task_results(self) -&gt; TaskResultsDescriptor:\n        \"\"\"Return the task results descriptor.\"\"\"\n        return self._task_results.__get__(self)\n\n    @task_results.setter\n    def task_results(self, value: Any) -&gt; None:\n        \"\"\"This will raise an error, as we do not want to set the task results directly.\n\n        NOTE: This implementation is here purely to avoid the task_results property from being\n        overwritten by accident.\n        \"\"\"\n        self._task_results.__set__(self, value)\n\n    @property\n    def owner(self) -&gt; TMetadataOwner:\n        \"\"\"Return the owner of the metadata.\"\"\"\n        return self._owner\n\n    @owner.setter\n    def owner(self, owner: TMetadataOwner) -&gt; None:\n        \"\"\"Return the owner of the metadata.\"\"\"\n        self._owner = owner\n\n    @classmethod\n    def from_owner(cls, owner: TMetadataOwner, **data) -&gt; BaseMetadata:\n        \"\"\"Create a new instance of the metadata class with the owner set.\"\"\"\n        metadata = cls(**data)\n        metadata.owner = owner\n        return metadata\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def validate_data_fields_from_annotations(cls, data: Any) -&gt; Any:\n        \"\"\"Validate the data fields from the annotations.\"\"\"\n\n        # We want to make sure that we combine the `extra` metdata along with any\n        # other specific fields that are defined in the metadata.\n        extra = data.pop(\"extra\", {})\n        assert isinstance(extra, dict), \"The `extra` field must be a dictionary.\"\n        data = {**data, **extra}\n\n        # If the model has been sub-classed, then all of our fields must be\n        # validated by the pydantic model.\n        if cls._is_field_typed():\n            # We will get the fields out of extra and set them as potential fields to\n            # validate. They will be ignored if they are not defined in the model, but it\n            # allows for a more flexible way to define metadata.\n            # Otherwise, what ever is in the `extra` field will be stroed in the `extra` field.\n            return {**data, \"extra\": extra}\n\n        # Otherwise, we are using our mock-dict implentation, so we store our\n        # metadata in the `extra` field.\n        return {\"extra\": data}\n\n    @classmethod\n    def _is_field_typed(cls):\n        \"\"\"\n        Check if the metadata model is field typed.\n\n        This is used to determine if the metadata model is a dictionary-like model,\n        or a more strictly typed model.\n        \"\"\"\n        if set([\"extra\"]) != set(cls.model_fields.keys()):\n            return True\n\n        return False\n\n    def __repr__(self):\n        \"\"\"\n        Provide a string representation of the metadata.\n\n        This only works for the base metadata model. If sub-classed, this will raise an error,\n        unless overridden, as BaseModel's do not have a __repr__ method.\n        \"\"\"\n        if self._is_field_typed():\n            return super().__repr__()\n\n        # Otherwise, we are deailing with dictornary-like metadata\n        return json.dumps(self.extra)\n\n    def __getitem__(self, name):\n        \"\"\"\n        Provide dictionary functionlaity to the metadata class.\n\n        This only works for the base metadata model. If sub-classed, this will raise an error,\n        unless overridden, as BaseModel's do not have an __getitem__ method.\n        \"\"\"\n        if not self._is_field_typed():\n            return self.extra[name]\n\n        raise AttributeError(\n            f\"'{self.__class__.__name__}' object has no attribute '{name}'\"\n        )\n\n    def __setitem__(self, name, value):\n        \"\"\"\n        Provide dictionary functionality to the metadata class.\n\n        This only works for the base metadata model. If sub-classed, this will raise an error,\n        unless overridden, as BaseModel's do not have an __setitem__ method.\n        \"\"\"\n        if not self._is_field_typed():\n            self.extra[name] = value\n        else:\n            raise AttributeError(\n                f\"'{self.__class__.__name__}' object has no attribute '{name}'\"\n            )\n\n    def __delitem__(self, name):\n        \"\"\"\n        Provide dictionary functionality to the metadata class.\n\n        This only works for the base metadata model. If sub-classed, this will raise an error,\n        unless overridden, as BaseModel's do not have an __delitem__ method.\n        \"\"\"\n        if not self._is_field_typed():\n            del self.extra[name]\n        else:\n            raise AttributeError(\n                f\"'{self.__class__.__name__}' object has no attribute '{name}'\"\n            )\n\n    def __iter__(self):\n        \"\"\"\n        Iterate over the keys in the metadata.\n\n        This only works for the base metadata model. If sub-classed, this will raise an error,\n        unless overridden, as BaseModel's do not have an __iter__ method.\n        \"\"\"\n        if self._is_field_typed():\n            raise AttributeError(f\"'{self.__class__.__name__}' object is not iterable\")\n\n        return iter(self.extra)\n\n    def __len__(self):\n        \"\"\"\n        Get the number of keys in the metadata.\n\n        This only works for the base metadata model. If sub-classed, this will raise an error,\n        unless overridden, as BaseModel's do not have a __len__ method.\n        \"\"\"\n        if self._is_field_typed():\n            raise AttributeError(\n                f\"'{self.__class__.__name__}' object has no attribute '__len__'\"\n            )\n\n        return len(self.extra)\n\n    def __getattr__(self, name):\n        \"\"\"\n        Allow for getting of attributes on the metadata class.\n\n        The attributes are retrieved through the following heirarchy:\n            - If the model is sub-classed, it will be retrieved as normal.\n            - Otherwise, if the attribute is private, it will be retrieved as normal.\n            - Finally, if we are getting a public attribute on the base metadata class,\n                we use the extra field.\n            - If the key is not set in the `extra` dictionary, we resort back to just\n            trying to get the field.\n                - This is when we grab the `owner` or `task_result` attribuite.\n        \"\"\"\n        if self._is_field_typed():\n            return super().__getattr__(name)\n\n        if name.startswith(\"_\"):\n            return super().__getattr__(name)\n\n        # Attempt to retreieve the attr from the `extra` field\n        try:\n            return self.extra.get(name)\n\n        except KeyError:\n            # This is for grabbing properties on the base metadata class\n            return super().__getattr__(name)\n\n    def __setattr__(self, name: str, value: Any) -&gt; None:\n        \"\"\"\n        Allow for setting of attributes on the metadata class.\n\n        The attributes are set through the following heirarchy:\n            - If the model is sub-classed, it will be set as normal.\n            - Otherwise, if the attribute is private, it will be set as normal.\n            - Finally, if we are setting a public attribute on the base metadata class,\n                we use the extra field.\n        \"\"\"\n        if self._is_field_typed():\n            return super().__setattr__(name, value)\n\n        # We want to avoid setting any private attributes in the extra\n        # dictionary\n        if name.startswith(\"_\"):\n            return super().__setattr__(name, value)\n\n        # If it is `owner` or `task_results`, we want\n        # to avoid setting the attribute in the `extra` dictionary\n        if name in [\"owner\", \"task_results\"]:\n            return super().__setattr__(name, value)\n\n        self.extra[name] = value\n\n    def __delattr__(self, name: str) -&gt; None:\n        \"\"\"\n        Ensure that we can delete attributes from the metadata class.\n\n        The attributes are deleted through the following heirarchy:\n            - If the attribute is `task_results`, we use the descriptor to delete the task results.\n            - Otherwise, if it is a sub-classed model, it will be deleted as normal.\n            - Finally, if we are deleting a public attribute on the base metadata class,\n                we use the extra field.\n        \"\"\"\n\n        # We want to use the descriptor to delete the task results\n        if name == \"task_results\":\n            self._task_results.__delete__(self)\n            return\n\n        # Otherwise, we use our standard fallback tiers\n        if self._is_field_typed():\n            return super().__delattr__(name)\n\n        del self.extra[name]\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.pipeline.BaseMetadata.owner","title":"<code>owner: TMetadataOwner</code>  <code>property</code> <code>writable</code>","text":"<p>Return the owner of the metadata.</p>"},{"location":"reference/schema/#docprompt.schema.pipeline.BaseMetadata.task_results","title":"<code>task_results: TaskResultsDescriptor</code>  <code>property</code> <code>writable</code>","text":"<p>Return the task results descriptor.</p>"},{"location":"reference/schema/#docprompt.schema.pipeline.BaseMetadata.__delattr__","title":"<code>__delattr__(name)</code>","text":"<p>Ensure that we can delete attributes from the metadata class.</p> The attributes are deleted through the following heirarchy <ul> <li>If the attribute is <code>task_results</code>, we use the descriptor to delete the task results.</li> <li>Otherwise, if it is a sub-classed model, it will be deleted as normal.</li> <li>Finally, if we are deleting a public attribute on the base metadata class,     we use the extra field.</li> </ul> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>def __delattr__(self, name: str) -&gt; None:\n    \"\"\"\n    Ensure that we can delete attributes from the metadata class.\n\n    The attributes are deleted through the following heirarchy:\n        - If the attribute is `task_results`, we use the descriptor to delete the task results.\n        - Otherwise, if it is a sub-classed model, it will be deleted as normal.\n        - Finally, if we are deleting a public attribute on the base metadata class,\n            we use the extra field.\n    \"\"\"\n\n    # We want to use the descriptor to delete the task results\n    if name == \"task_results\":\n        self._task_results.__delete__(self)\n        return\n\n    # Otherwise, we use our standard fallback tiers\n    if self._is_field_typed():\n        return super().__delattr__(name)\n\n    del self.extra[name]\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.pipeline.BaseMetadata.__delitem__","title":"<code>__delitem__(name)</code>","text":"<p>Provide dictionary functionality to the metadata class.</p> <p>This only works for the base metadata model. If sub-classed, this will raise an error, unless overridden, as BaseModel's do not have an delitem method.</p> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>def __delitem__(self, name):\n    \"\"\"\n    Provide dictionary functionality to the metadata class.\n\n    This only works for the base metadata model. If sub-classed, this will raise an error,\n    unless overridden, as BaseModel's do not have an __delitem__ method.\n    \"\"\"\n    if not self._is_field_typed():\n        del self.extra[name]\n    else:\n        raise AttributeError(\n            f\"'{self.__class__.__name__}' object has no attribute '{name}'\"\n        )\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.pipeline.BaseMetadata.__getattr__","title":"<code>__getattr__(name)</code>","text":"<p>Allow for getting of attributes on the metadata class.</p> The attributes are retrieved through the following heirarchy <ul> <li>If the model is sub-classed, it will be retrieved as normal.</li> <li>Otherwise, if the attribute is private, it will be retrieved as normal.</li> <li>Finally, if we are getting a public attribute on the base metadata class,     we use the extra field.</li> <li>If the key is not set in the <code>extra</code> dictionary, we resort back to just trying to get the field.<ul> <li>This is when we grab the <code>owner</code> or <code>task_result</code> attribuite.</li> </ul> </li> </ul> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>def __getattr__(self, name):\n    \"\"\"\n    Allow for getting of attributes on the metadata class.\n\n    The attributes are retrieved through the following heirarchy:\n        - If the model is sub-classed, it will be retrieved as normal.\n        - Otherwise, if the attribute is private, it will be retrieved as normal.\n        - Finally, if we are getting a public attribute on the base metadata class,\n            we use the extra field.\n        - If the key is not set in the `extra` dictionary, we resort back to just\n        trying to get the field.\n            - This is when we grab the `owner` or `task_result` attribuite.\n    \"\"\"\n    if self._is_field_typed():\n        return super().__getattr__(name)\n\n    if name.startswith(\"_\"):\n        return super().__getattr__(name)\n\n    # Attempt to retreieve the attr from the `extra` field\n    try:\n        return self.extra.get(name)\n\n    except KeyError:\n        # This is for grabbing properties on the base metadata class\n        return super().__getattr__(name)\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.pipeline.BaseMetadata.__getitem__","title":"<code>__getitem__(name)</code>","text":"<p>Provide dictionary functionlaity to the metadata class.</p> <p>This only works for the base metadata model. If sub-classed, this will raise an error, unless overridden, as BaseModel's do not have an getitem method.</p> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>def __getitem__(self, name):\n    \"\"\"\n    Provide dictionary functionlaity to the metadata class.\n\n    This only works for the base metadata model. If sub-classed, this will raise an error,\n    unless overridden, as BaseModel's do not have an __getitem__ method.\n    \"\"\"\n    if not self._is_field_typed():\n        return self.extra[name]\n\n    raise AttributeError(\n        f\"'{self.__class__.__name__}' object has no attribute '{name}'\"\n    )\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.pipeline.BaseMetadata.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over the keys in the metadata.</p> <p>This only works for the base metadata model. If sub-classed, this will raise an error, unless overridden, as BaseModel's do not have an iter method.</p> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>def __iter__(self):\n    \"\"\"\n    Iterate over the keys in the metadata.\n\n    This only works for the base metadata model. If sub-classed, this will raise an error,\n    unless overridden, as BaseModel's do not have an __iter__ method.\n    \"\"\"\n    if self._is_field_typed():\n        raise AttributeError(f\"'{self.__class__.__name__}' object is not iterable\")\n\n    return iter(self.extra)\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.pipeline.BaseMetadata.__len__","title":"<code>__len__()</code>","text":"<p>Get the number of keys in the metadata.</p> <p>This only works for the base metadata model. If sub-classed, this will raise an error, unless overridden, as BaseModel's do not have a len method.</p> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>def __len__(self):\n    \"\"\"\n    Get the number of keys in the metadata.\n\n    This only works for the base metadata model. If sub-classed, this will raise an error,\n    unless overridden, as BaseModel's do not have a __len__ method.\n    \"\"\"\n    if self._is_field_typed():\n        raise AttributeError(\n            f\"'{self.__class__.__name__}' object has no attribute '__len__'\"\n        )\n\n    return len(self.extra)\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.pipeline.BaseMetadata.__repr__","title":"<code>__repr__()</code>","text":"<p>Provide a string representation of the metadata.</p> <p>This only works for the base metadata model. If sub-classed, this will raise an error, unless overridden, as BaseModel's do not have a repr method.</p> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>def __repr__(self):\n    \"\"\"\n    Provide a string representation of the metadata.\n\n    This only works for the base metadata model. If sub-classed, this will raise an error,\n    unless overridden, as BaseModel's do not have a __repr__ method.\n    \"\"\"\n    if self._is_field_typed():\n        return super().__repr__()\n\n    # Otherwise, we are deailing with dictornary-like metadata\n    return json.dumps(self.extra)\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.pipeline.BaseMetadata.__setattr__","title":"<code>__setattr__(name, value)</code>","text":"<p>Allow for setting of attributes on the metadata class.</p> The attributes are set through the following heirarchy <ul> <li>If the model is sub-classed, it will be set as normal.</li> <li>Otherwise, if the attribute is private, it will be set as normal.</li> <li>Finally, if we are setting a public attribute on the base metadata class,     we use the extra field.</li> </ul> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>def __setattr__(self, name: str, value: Any) -&gt; None:\n    \"\"\"\n    Allow for setting of attributes on the metadata class.\n\n    The attributes are set through the following heirarchy:\n        - If the model is sub-classed, it will be set as normal.\n        - Otherwise, if the attribute is private, it will be set as normal.\n        - Finally, if we are setting a public attribute on the base metadata class,\n            we use the extra field.\n    \"\"\"\n    if self._is_field_typed():\n        return super().__setattr__(name, value)\n\n    # We want to avoid setting any private attributes in the extra\n    # dictionary\n    if name.startswith(\"_\"):\n        return super().__setattr__(name, value)\n\n    # If it is `owner` or `task_results`, we want\n    # to avoid setting the attribute in the `extra` dictionary\n    if name in [\"owner\", \"task_results\"]:\n        return super().__setattr__(name, value)\n\n    self.extra[name] = value\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.pipeline.BaseMetadata.__setitem__","title":"<code>__setitem__(name, value)</code>","text":"<p>Provide dictionary functionality to the metadata class.</p> <p>This only works for the base metadata model. If sub-classed, this will raise an error, unless overridden, as BaseModel's do not have an setitem method.</p> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>def __setitem__(self, name, value):\n    \"\"\"\n    Provide dictionary functionality to the metadata class.\n\n    This only works for the base metadata model. If sub-classed, this will raise an error,\n    unless overridden, as BaseModel's do not have an __setitem__ method.\n    \"\"\"\n    if not self._is_field_typed():\n        self.extra[name] = value\n    else:\n        raise AttributeError(\n            f\"'{self.__class__.__name__}' object has no attribute '{name}'\"\n        )\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.pipeline.BaseMetadata.from_owner","title":"<code>from_owner(owner, **data)</code>  <code>classmethod</code>","text":"<p>Create a new instance of the metadata class with the owner set.</p> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>@classmethod\ndef from_owner(cls, owner: TMetadataOwner, **data) -&gt; BaseMetadata:\n    \"\"\"Create a new instance of the metadata class with the owner set.\"\"\"\n    metadata = cls(**data)\n    metadata.owner = owner\n    return metadata\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.pipeline.BaseMetadata.validate_data_fields_from_annotations","title":"<code>validate_data_fields_from_annotations(data)</code>  <code>classmethod</code>","text":"<p>Validate the data fields from the annotations.</p> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>@model_validator(mode=\"before\")\n@classmethod\ndef validate_data_fields_from_annotations(cls, data: Any) -&gt; Any:\n    \"\"\"Validate the data fields from the annotations.\"\"\"\n\n    # We want to make sure that we combine the `extra` metdata along with any\n    # other specific fields that are defined in the metadata.\n    extra = data.pop(\"extra\", {})\n    assert isinstance(extra, dict), \"The `extra` field must be a dictionary.\"\n    data = {**data, **extra}\n\n    # If the model has been sub-classed, then all of our fields must be\n    # validated by the pydantic model.\n    if cls._is_field_typed():\n        # We will get the fields out of extra and set them as potential fields to\n        # validate. They will be ignored if they are not defined in the model, but it\n        # allows for a more flexible way to define metadata.\n        # Otherwise, what ever is in the `extra` field will be stroed in the `extra` field.\n        return {**data, \"extra\": extra}\n\n    # Otherwise, we are using our mock-dict implentation, so we store our\n    # metadata in the `extra` field.\n    return {\"extra\": data}\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.pipeline.DocumentCollection","title":"<code>DocumentCollection</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[DocumentCollectionMetadata, DocumentNodeMetadata, PageNodeMetadata]</code></p> <p>Represents a collection of documents with some common metadata</p> Source code in <code>docprompt/schema/pipeline/node/collection.py</code> <pre><code>class DocumentCollection(\n    BaseModel,\n    Generic[DocumentCollectionMetadata, DocumentNodeMetadata, PageNodeMetadata],\n):\n    \"\"\"\n    Represents a collection of documents with some common metadata\n    \"\"\"\n\n    document_nodes: List[\"DocumentNode[DocumentNodeMetadata, PageNodeMetadata]\"]\n    metadata: DocumentCollectionMetadata = Field(..., default_factory=dict)\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.pipeline.DocumentNode","title":"<code>DocumentNode</code>","text":"<p>               Bases: <code>BaseNode</code>, <code>Generic[DocumentNodeMetadata, PageNodeMetadata]</code></p> <p>Represents a single document, with some metadata</p> Source code in <code>docprompt/schema/pipeline/node/document.py</code> <pre><code>class DocumentNode(BaseNode, Generic[DocumentNodeMetadata, PageNodeMetadata]):\n    \"\"\"\n    Represents a single document, with some metadata\n    \"\"\"\n\n    document: PdfDocument\n    page_nodes: List[PageNode[PageNodeMetadata]] = Field(\n        description=\"The pages in the document\", default_factory=list, repr=False\n    )\n    metadata: DocumentNodeMetadata = Field(\n        description=\"Application-specific metadata for the document\",\n        default_factory=BaseMetadata,\n    )\n\n    _locator: Optional[\"DocumentProvenanceLocator\"] = PrivateAttr(default=None)\n\n    _persistance_path: Optional[str] = PrivateAttr(default=None)\n\n    def __getstate__(self):\n        state = super().__getstate__()\n\n        state[\"__pydantic_private__\"][\"_locator\"] = None\n\n        return state\n\n    def __len__(self):\n        return len(self.page_nodes)\n\n    def __getitem__(self, index):\n        return self.page_nodes[index]\n\n    def __iter__(self):\n        return iter(self.page_nodes)\n\n    @property\n    def rasterizer(self):\n        return DocumentRasterizer(self)\n\n    @property\n    def locator(self):\n        if self._locator is None:\n            self.refresh_locator()\n\n        return self._locator\n\n    def refresh_locator(self):\n        \"\"\"\n        Refreshes the locator for this document node\n        \"\"\"\n        from docprompt.provenance.search import DocumentProvenanceLocator\n\n        if any(not page.ocr_results.result for page in self.page_nodes):\n            raise ValueError(\n                \"Cannot create a locator for a document node with missing OCR results\"\n            )\n\n        self._locator = DocumentProvenanceLocator.from_document_node(self)\n\n        return self.locator\n\n    @classmethod\n    def from_document(\n        cls,\n        document: PdfDocument,\n        document_metadata: Optional[DocumentNodeMetadata] = None,\n        page_metadata: Optional[List[PageNodeMetadata]] = None,\n    ):\n        document_node: \"DocumentNode[DocumentNodeMetadata, PageNodeMetadata]\" = cls(\n            document=document,\n        )\n        document_node.metadata = document_metadata or cls.metadata_class().from_owner(\n            document_node, **{}\n        )\n\n        if page_metadata is not None and len(page_metadata) != len(document):\n            raise ValueError(\n                \"The number of page metadata items must match the number of pages in the document.\"\n            )\n\n        for page_number in range(1, len(document) + 1):\n            if page_metadata is not None:\n                page_node = PageNode(\n                    document=document_node,\n                    page_number=page_number,\n                    metadata=page_metadata[page_number - 1],\n                )\n            else:\n                page_node = PageNode(document=document_node, page_number=page_number)\n\n            document_node.page_nodes.append(page_node)\n\n        return document_node\n\n    @property\n    def file_hash(self):\n        return self.document.document_hash\n\n    @property\n    def document_name(self):\n        return self.document.name\n\n    @classmethod\n    def metadata_class(cls) -&gt; Type[BaseMetadata]:\n        \"\"\"Get the metadata class for instantiating metadata from the model.\"\"\"\n\n        fields = cls.model_fields\n\n        # NOTE: The indexing is important here, and relies on the generic type being\n        # the SECOND of the two arguments in the `Union` annotation\n        metadata_field_annotation = fields[\"metadata\"].annotation\n\n        # If no override has been provided to the metadata model, we want to retrieve\n        # it as a TypedDict\n        if metadata_field_annotation == DocumentNodeMetadata:\n            return BaseMetadata\n\n        if isinstance(metadata_field_annotation, ForwardRef):\n            raise ValueError(\n                \"You cannot define DocumentNode with a ForwardRef for Generic metadata model types.\"\n            )\n\n        # Get the overriden Generic type of th DocumentNodeMetadata\n        return metadata_field_annotation\n\n    @classmethod\n    def page_metadata_class(cls) -&gt; Type[Union[dict, BaseModel]]:\n        \"\"\"Get the metadata class for the page nodes in the document.\"\"\"\n        fields = cls.model_fields\n\n        # NOTE: The indexing is important here, and it allows us to get the type of each\n        # page node in the `List` annotation\n        page_nodes_field_class = fields[\"page_nodes\"].annotation.__args__[0]\n\n        # NOTE: The indexing is important here, and relies on the generic type being\n        # the SECOND of the two arguments in the `Union` annotation\n        page_node_metadata_field_annotation = page_nodes_field_class.model_fields[\n            \"metadata\"\n        ].annotation\n\n        if page_node_metadata_field_annotation == PageNodeMetadata:\n            return BaseMetadata\n\n        if isinstance(page_node_metadata_field_annotation, ForwardRef):\n            raise ValueError(\n                \"You cannot define PageNode with a ForwardRef for Generic metadata model types.\"\n            )\n\n        return page_node_metadata_field_annotation\n\n    @property\n    def persistance_path(self):\n        \"\"\"The base path to storage location.\"\"\"\n        return self._persistance_path\n\n    @persistance_path.setter\n    def persistance_path(self, path: str):\n        \"\"\"Set the base path to storage location.\"\"\"\n        self._persistance_path = path\n\n    @classmethod\n    def from_storage(cls, path: str, file_hash: str, **kwargs) -&gt; Self:\n        \"\"\"Load the document node from storage.\n\n        Args:\n            path (str): The base path to storage location.\n                - Example (S3): \"s3://bucket-name/key/to/folder\"\n                - Example (Local FS): \"/tmp/docprompt/storage\"\n            file_hash (str): The hash of the document.\n            **kwargs: Additional keyword arguments for fsspec FileSystem\n\n        Returns:\n            DocumentNode: The loaded document node.\n        \"\"\"\n\n        fs_manager = FileSystemManager(path, **kwargs)\n\n        pdf_bytes, metadata_bytes, page_metadata_bytes = fs_manager.read(\n            file_hash, **kwargs\n        )\n\n        doc = PdfDocument.from_bytes(pdf_bytes, name=fs_manager.get_pdf_name(file_hash))\n        node = cls.from_document(doc)\n\n        if metadata_bytes:\n            metadata_json = json.loads(metadata_bytes.decode(\"utf-8\"))\n            metadata = cls.metadata_class().from_owner(node, **metadata_json)\n        else:\n            metadata = cls.metadata_class().from_owner(node, **{})\n\n        if page_metadata_bytes:\n            page_metadata_json = [\n                json.loads(page_str)\n                for page_str in json.loads(page_metadata_bytes.decode(\"utf-8\"))\n            ]\n            page_metadata = [\n                cls.page_metadata_class()(**page) for page in page_metadata_json\n            ]\n        else:\n            page_metadata = [cls.page_metadata_class()(**{}) for _ in range(len(doc))]\n\n        # Store the metadata on the node and page nodes\n        node.metadata = metadata\n        for page, meta in zip(node.page_nodes, page_metadata):\n            meta.set_owner(page)\n            page.metadata = meta\n\n        # Make sure to set the persistance path on the node\n        node.persistance_path = path\n\n        return node\n\n    def persist(self, path: Optional[str] = None, **kwargs) -&gt; FileSidecarsPathManager:\n        \"\"\"Persist a document node to storage.\n\n        Args:\n            path (Optional[str]): Overwrites the current `persistance_path` property\n                - If `persistance_path` is not currently set, path must be provided.\n            **kwargs: Additional keyword arguments for fsspec FileSystem\n\n        Returns:\n            FileSidecarsPathManager: The file path manager for the persisted document node.\n        \"\"\"\n\n        path = path or self.persistance_path\n\n        if path is None:\n            raise ValueError(\"The path must be provided to persist the document node.\")\n\n        # Make sure to update the persistance path\n        self.persistance_path = path\n\n        fs_manager = FileSystemManager(path, **kwargs)\n\n        pdf_bytes = self.document.get_bytes()\n        metadata_bytes = bytes(self.metadata.model_dump_json(), encoding=\"utf-8\")\n        page_metadata_bytes = bytes(\n            json.dumps([page.metadata.model_dump_json() for page in self.page_nodes]),\n            encoding=\"utf-8\",\n        )\n\n        return fs_manager.write(\n            pdf_bytes, metadata_bytes, page_metadata_bytes, **kwargs\n        )\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.pipeline.DocumentNode.persistance_path","title":"<code>persistance_path</code>  <code>property</code> <code>writable</code>","text":"<p>The base path to storage location.</p>"},{"location":"reference/schema/#docprompt.schema.pipeline.DocumentNode.from_storage","title":"<code>from_storage(path, file_hash, **kwargs)</code>  <code>classmethod</code>","text":"<p>Load the document node from storage.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The base path to storage location. - Example (S3): \"s3://bucket-name/key/to/folder\" - Example (Local FS): \"/tmp/docprompt/storage\"</p> required <code>file_hash</code> <code>str</code> <p>The hash of the document.</p> required <code>**kwargs</code> <p>Additional keyword arguments for fsspec FileSystem</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>DocumentNode</code> <code>Self</code> <p>The loaded document node.</p> Source code in <code>docprompt/schema/pipeline/node/document.py</code> <pre><code>@classmethod\ndef from_storage(cls, path: str, file_hash: str, **kwargs) -&gt; Self:\n    \"\"\"Load the document node from storage.\n\n    Args:\n        path (str): The base path to storage location.\n            - Example (S3): \"s3://bucket-name/key/to/folder\"\n            - Example (Local FS): \"/tmp/docprompt/storage\"\n        file_hash (str): The hash of the document.\n        **kwargs: Additional keyword arguments for fsspec FileSystem\n\n    Returns:\n        DocumentNode: The loaded document node.\n    \"\"\"\n\n    fs_manager = FileSystemManager(path, **kwargs)\n\n    pdf_bytes, metadata_bytes, page_metadata_bytes = fs_manager.read(\n        file_hash, **kwargs\n    )\n\n    doc = PdfDocument.from_bytes(pdf_bytes, name=fs_manager.get_pdf_name(file_hash))\n    node = cls.from_document(doc)\n\n    if metadata_bytes:\n        metadata_json = json.loads(metadata_bytes.decode(\"utf-8\"))\n        metadata = cls.metadata_class().from_owner(node, **metadata_json)\n    else:\n        metadata = cls.metadata_class().from_owner(node, **{})\n\n    if page_metadata_bytes:\n        page_metadata_json = [\n            json.loads(page_str)\n            for page_str in json.loads(page_metadata_bytes.decode(\"utf-8\"))\n        ]\n        page_metadata = [\n            cls.page_metadata_class()(**page) for page in page_metadata_json\n        ]\n    else:\n        page_metadata = [cls.page_metadata_class()(**{}) for _ in range(len(doc))]\n\n    # Store the metadata on the node and page nodes\n    node.metadata = metadata\n    for page, meta in zip(node.page_nodes, page_metadata):\n        meta.set_owner(page)\n        page.metadata = meta\n\n    # Make sure to set the persistance path on the node\n    node.persistance_path = path\n\n    return node\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.pipeline.DocumentNode.metadata_class","title":"<code>metadata_class()</code>  <code>classmethod</code>","text":"<p>Get the metadata class for instantiating metadata from the model.</p> Source code in <code>docprompt/schema/pipeline/node/document.py</code> <pre><code>@classmethod\ndef metadata_class(cls) -&gt; Type[BaseMetadata]:\n    \"\"\"Get the metadata class for instantiating metadata from the model.\"\"\"\n\n    fields = cls.model_fields\n\n    # NOTE: The indexing is important here, and relies on the generic type being\n    # the SECOND of the two arguments in the `Union` annotation\n    metadata_field_annotation = fields[\"metadata\"].annotation\n\n    # If no override has been provided to the metadata model, we want to retrieve\n    # it as a TypedDict\n    if metadata_field_annotation == DocumentNodeMetadata:\n        return BaseMetadata\n\n    if isinstance(metadata_field_annotation, ForwardRef):\n        raise ValueError(\n            \"You cannot define DocumentNode with a ForwardRef for Generic metadata model types.\"\n        )\n\n    # Get the overriden Generic type of th DocumentNodeMetadata\n    return metadata_field_annotation\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.pipeline.DocumentNode.page_metadata_class","title":"<code>page_metadata_class()</code>  <code>classmethod</code>","text":"<p>Get the metadata class for the page nodes in the document.</p> Source code in <code>docprompt/schema/pipeline/node/document.py</code> <pre><code>@classmethod\ndef page_metadata_class(cls) -&gt; Type[Union[dict, BaseModel]]:\n    \"\"\"Get the metadata class for the page nodes in the document.\"\"\"\n    fields = cls.model_fields\n\n    # NOTE: The indexing is important here, and it allows us to get the type of each\n    # page node in the `List` annotation\n    page_nodes_field_class = fields[\"page_nodes\"].annotation.__args__[0]\n\n    # NOTE: The indexing is important here, and relies on the generic type being\n    # the SECOND of the two arguments in the `Union` annotation\n    page_node_metadata_field_annotation = page_nodes_field_class.model_fields[\n        \"metadata\"\n    ].annotation\n\n    if page_node_metadata_field_annotation == PageNodeMetadata:\n        return BaseMetadata\n\n    if isinstance(page_node_metadata_field_annotation, ForwardRef):\n        raise ValueError(\n            \"You cannot define PageNode with a ForwardRef for Generic metadata model types.\"\n        )\n\n    return page_node_metadata_field_annotation\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.pipeline.DocumentNode.persist","title":"<code>persist(path=None, **kwargs)</code>","text":"<p>Persist a document node to storage.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Optional[str]</code> <p>Overwrites the current <code>persistance_path</code> property - If <code>persistance_path</code> is not currently set, path must be provided.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments for fsspec FileSystem</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>FileSidecarsPathManager</code> <code>FileSidecarsPathManager</code> <p>The file path manager for the persisted document node.</p> Source code in <code>docprompt/schema/pipeline/node/document.py</code> <pre><code>def persist(self, path: Optional[str] = None, **kwargs) -&gt; FileSidecarsPathManager:\n    \"\"\"Persist a document node to storage.\n\n    Args:\n        path (Optional[str]): Overwrites the current `persistance_path` property\n            - If `persistance_path` is not currently set, path must be provided.\n        **kwargs: Additional keyword arguments for fsspec FileSystem\n\n    Returns:\n        FileSidecarsPathManager: The file path manager for the persisted document node.\n    \"\"\"\n\n    path = path or self.persistance_path\n\n    if path is None:\n        raise ValueError(\"The path must be provided to persist the document node.\")\n\n    # Make sure to update the persistance path\n    self.persistance_path = path\n\n    fs_manager = FileSystemManager(path, **kwargs)\n\n    pdf_bytes = self.document.get_bytes()\n    metadata_bytes = bytes(self.metadata.model_dump_json(), encoding=\"utf-8\")\n    page_metadata_bytes = bytes(\n        json.dumps([page.metadata.model_dump_json() for page in self.page_nodes]),\n        encoding=\"utf-8\",\n    )\n\n    return fs_manager.write(\n        pdf_bytes, metadata_bytes, page_metadata_bytes, **kwargs\n    )\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.pipeline.DocumentNode.refresh_locator","title":"<code>refresh_locator()</code>","text":"<p>Refreshes the locator for this document node</p> Source code in <code>docprompt/schema/pipeline/node/document.py</code> <pre><code>def refresh_locator(self):\n    \"\"\"\n    Refreshes the locator for this document node\n    \"\"\"\n    from docprompt.provenance.search import DocumentProvenanceLocator\n\n    if any(not page.ocr_results.result for page in self.page_nodes):\n        raise ValueError(\n            \"Cannot create a locator for a document node with missing OCR results\"\n        )\n\n    self._locator = DocumentProvenanceLocator.from_document_node(self)\n\n    return self.locator\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.pipeline.PageNode","title":"<code>PageNode</code>","text":"<p>               Bases: <code>BaseNode</code>, <code>Generic[PageNodeMetadata]</code></p> <p>Represents a single page in a document, with some metadata</p> Source code in <code>docprompt/schema/pipeline/node/page.py</code> <pre><code>class PageNode(BaseNode, Generic[PageNodeMetadata]):\n    \"\"\"\n    Represents a single page in a document, with some metadata\n    \"\"\"\n\n    document: \"DocumentNode\" = Field(exclude=True, repr=False)\n    page_number: PositiveInt = Field(description=\"The page number\")\n    metadata: PageNodeMetadata = Field(\n        description=\"Application-specific metadata for the page\",\n        default_factory=BaseMetadata,\n    )\n    extra: Dict[str, Any] = Field(\n        description=\"Extra data that can be stored on the page node\",\n        default_factory=dict,\n    )\n\n    ocr_results: ResultContainer[OcrPageResult] = Field(\n        default_factory=_result_container_factory,\n        description=\"The OCR results for the page\",\n        repr=False,\n    )\n\n    _raster_cache: Dict[str, bytes] = PrivateAttr(default_factory=dict)\n\n    def __getstate__(self):\n        state = super().__getstate__()\n\n        state[\"__pydantic_private__\"][\"_raster_cache\"] = {}\n\n        return state\n\n    @property\n    def rasterizer(self):\n        return PageRasterizer(self._raster_cache, self)\n\n    def search(\n        self, query: str, refine_to_words: bool = True, require_exact_match: bool = True\n    ):\n        return self.document.locator.search(\n            query,\n            page_number=self.page_number,\n            refine_to_word=refine_to_words,\n            require_exact_match=require_exact_match,\n        )\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.pipeline.metadata","title":"<code>metadata</code>","text":"<p>The metadata class is utilized for defining a basic, yet flexible interface for metadata attached to various fields.</p> <p>In essence, this allows for developers to choose to either create their metadtata in an unstructured manner (i.e. a dictionary), or to sub class the base metadata class in order to create a more strictly typed metadata model for their page and document nodes.</p>"},{"location":"reference/schema/#docprompt.schema.pipeline.metadata.BaseMetadata","title":"<code>BaseMetadata</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>MutableMapping</code>, <code>Generic[TMetadataOwner]</code></p> <p>The base metadata class is utilized for defining a basic yet flexible interface for metadata attached to various fields.</p> The metadata class can be used in two ways <ol> <li>As a dictionary-like object, where metadata is stored in the <code>extra</code> field.</li> <li>As a sub-classed model, where metadata is stored in the fields of the model.</li> </ol> <p>When used out of the box, the metadata class will adobpt dictionary-like behavior. You may easily access different fields of the metadata as if it were a dictionary: <pre><code># Instantiate it with any kwargs you like\nmetadata = BaseMetadata(foo-'bar', cow='moo')\n\nmetadata[\"foo\"]  # \"bar\"\nmetadata[\"cow\"]  # \"moo\"\n\n# Update the value of the key\nmetadata[\"foo\"] = \"fighters\"\n\n# Set new key-value pairs\nmetadata['sheep'] = 'baa'\n</code></pre></p> <p>Otherwise, you may sub-class the metadata class in order to create a more strictly typed metadata model. This is useful when you want to enforce a specific structure for your metadata.</p> <pre><code>class CustomMetadata(BaseMetadata):\n    foo: str\n    cow: str\n\n# Instantiate it with the required fields\nmetadata = CustomMetadata(foo='bar', cow='moo')\n\nmetadata.foo  # \"bar\"\nmetadata.cow  # \"moo\"\n\n# Update the value of the key\nmetadata.foo = \"fighters\"\n\n# Use the extra field to store dynamic metadata\nmetadata.extra['sheep'] = 'baa'\n</code></pre> <p>Additionally, the task results descriptor allows for controlled and easy access to the task results of various tasks that are run on the parent node.</p> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>class BaseMetadata(BaseModel, MutableMapping, Generic[TMetadataOwner]):\n    \"\"\"\n    The base metadata class is utilized for defining a basic yet flexible interface\n    for metadata attached to various fields.\n\n    The metadata class can be used in two ways:\n        1. As a dictionary-like object, where metadata is stored in the `extra` field.\n        2. As a sub-classed model, where metadata is stored in the fields of the model.\n\n    When used out of the box, the metadata class will adobpt dictionary-like behavior. You\n    may easily access different fields of the metadata as if it were a dictionary:\n    ```python\n    # Instantiate it with any kwargs you like\n    metadata = BaseMetadata(foo-'bar', cow='moo')\n\n    metadata[\"foo\"]  # \"bar\"\n    metadata[\"cow\"]  # \"moo\"\n\n    # Update the value of the key\n    metadata[\"foo\"] = \"fighters\"\n\n    # Set new key-value pairs\n    metadata['sheep'] = 'baa'\n    ```\n\n    Otherwise, you may sub-class the metadata class in order to create a more strictly typed\n    metadata model. This is useful when you want to enforce a specific structure for your metadata.\n\n    ```python\n    class CustomMetadata(BaseMetadata):\n        foo: str\n        cow: str\n\n    # Instantiate it with the required fields\n    metadata = CustomMetadata(foo='bar', cow='moo')\n\n    metadata.foo  # \"bar\"\n    metadata.cow  # \"moo\"\n\n    # Update the value of the key\n    metadata.foo = \"fighters\"\n\n    # Use the extra field to store dynamic metadata\n    metadata.extra['sheep'] = 'baa'\n    ```\n\n    Additionally, the task results descriptor allows for controlled and easy access to the task results\n    of various tasks that are run on the parent node.\n    \"\"\"\n\n    extra: Dict[str, Any] = Field(..., default_factory=dict, repr=False)\n\n    _task_results: TaskResultsDescriptor = PrivateAttr(\n        default_factory=TaskResultsDescriptor\n    )\n\n    _owner: TMetadataOwner = PrivateAttr()\n\n    @property\n    def task_results(self) -&gt; TaskResultsDescriptor:\n        \"\"\"Return the task results descriptor.\"\"\"\n        return self._task_results.__get__(self)\n\n    @task_results.setter\n    def task_results(self, value: Any) -&gt; None:\n        \"\"\"This will raise an error, as we do not want to set the task results directly.\n\n        NOTE: This implementation is here purely to avoid the task_results property from being\n        overwritten by accident.\n        \"\"\"\n        self._task_results.__set__(self, value)\n\n    @property\n    def owner(self) -&gt; TMetadataOwner:\n        \"\"\"Return the owner of the metadata.\"\"\"\n        return self._owner\n\n    @owner.setter\n    def owner(self, owner: TMetadataOwner) -&gt; None:\n        \"\"\"Return the owner of the metadata.\"\"\"\n        self._owner = owner\n\n    @classmethod\n    def from_owner(cls, owner: TMetadataOwner, **data) -&gt; BaseMetadata:\n        \"\"\"Create a new instance of the metadata class with the owner set.\"\"\"\n        metadata = cls(**data)\n        metadata.owner = owner\n        return metadata\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def validate_data_fields_from_annotations(cls, data: Any) -&gt; Any:\n        \"\"\"Validate the data fields from the annotations.\"\"\"\n\n        # We want to make sure that we combine the `extra` metdata along with any\n        # other specific fields that are defined in the metadata.\n        extra = data.pop(\"extra\", {})\n        assert isinstance(extra, dict), \"The `extra` field must be a dictionary.\"\n        data = {**data, **extra}\n\n        # If the model has been sub-classed, then all of our fields must be\n        # validated by the pydantic model.\n        if cls._is_field_typed():\n            # We will get the fields out of extra and set them as potential fields to\n            # validate. They will be ignored if they are not defined in the model, but it\n            # allows for a more flexible way to define metadata.\n            # Otherwise, what ever is in the `extra` field will be stroed in the `extra` field.\n            return {**data, \"extra\": extra}\n\n        # Otherwise, we are using our mock-dict implentation, so we store our\n        # metadata in the `extra` field.\n        return {\"extra\": data}\n\n    @classmethod\n    def _is_field_typed(cls):\n        \"\"\"\n        Check if the metadata model is field typed.\n\n        This is used to determine if the metadata model is a dictionary-like model,\n        or a more strictly typed model.\n        \"\"\"\n        if set([\"extra\"]) != set(cls.model_fields.keys()):\n            return True\n\n        return False\n\n    def __repr__(self):\n        \"\"\"\n        Provide a string representation of the metadata.\n\n        This only works for the base metadata model. If sub-classed, this will raise an error,\n        unless overridden, as BaseModel's do not have a __repr__ method.\n        \"\"\"\n        if self._is_field_typed():\n            return super().__repr__()\n\n        # Otherwise, we are deailing with dictornary-like metadata\n        return json.dumps(self.extra)\n\n    def __getitem__(self, name):\n        \"\"\"\n        Provide dictionary functionlaity to the metadata class.\n\n        This only works for the base metadata model. If sub-classed, this will raise an error,\n        unless overridden, as BaseModel's do not have an __getitem__ method.\n        \"\"\"\n        if not self._is_field_typed():\n            return self.extra[name]\n\n        raise AttributeError(\n            f\"'{self.__class__.__name__}' object has no attribute '{name}'\"\n        )\n\n    def __setitem__(self, name, value):\n        \"\"\"\n        Provide dictionary functionality to the metadata class.\n\n        This only works for the base metadata model. If sub-classed, this will raise an error,\n        unless overridden, as BaseModel's do not have an __setitem__ method.\n        \"\"\"\n        if not self._is_field_typed():\n            self.extra[name] = value\n        else:\n            raise AttributeError(\n                f\"'{self.__class__.__name__}' object has no attribute '{name}'\"\n            )\n\n    def __delitem__(self, name):\n        \"\"\"\n        Provide dictionary functionality to the metadata class.\n\n        This only works for the base metadata model. If sub-classed, this will raise an error,\n        unless overridden, as BaseModel's do not have an __delitem__ method.\n        \"\"\"\n        if not self._is_field_typed():\n            del self.extra[name]\n        else:\n            raise AttributeError(\n                f\"'{self.__class__.__name__}' object has no attribute '{name}'\"\n            )\n\n    def __iter__(self):\n        \"\"\"\n        Iterate over the keys in the metadata.\n\n        This only works for the base metadata model. If sub-classed, this will raise an error,\n        unless overridden, as BaseModel's do not have an __iter__ method.\n        \"\"\"\n        if self._is_field_typed():\n            raise AttributeError(f\"'{self.__class__.__name__}' object is not iterable\")\n\n        return iter(self.extra)\n\n    def __len__(self):\n        \"\"\"\n        Get the number of keys in the metadata.\n\n        This only works for the base metadata model. If sub-classed, this will raise an error,\n        unless overridden, as BaseModel's do not have a __len__ method.\n        \"\"\"\n        if self._is_field_typed():\n            raise AttributeError(\n                f\"'{self.__class__.__name__}' object has no attribute '__len__'\"\n            )\n\n        return len(self.extra)\n\n    def __getattr__(self, name):\n        \"\"\"\n        Allow for getting of attributes on the metadata class.\n\n        The attributes are retrieved through the following heirarchy:\n            - If the model is sub-classed, it will be retrieved as normal.\n            - Otherwise, if the attribute is private, it will be retrieved as normal.\n            - Finally, if we are getting a public attribute on the base metadata class,\n                we use the extra field.\n            - If the key is not set in the `extra` dictionary, we resort back to just\n            trying to get the field.\n                - This is when we grab the `owner` or `task_result` attribuite.\n        \"\"\"\n        if self._is_field_typed():\n            return super().__getattr__(name)\n\n        if name.startswith(\"_\"):\n            return super().__getattr__(name)\n\n        # Attempt to retreieve the attr from the `extra` field\n        try:\n            return self.extra.get(name)\n\n        except KeyError:\n            # This is for grabbing properties on the base metadata class\n            return super().__getattr__(name)\n\n    def __setattr__(self, name: str, value: Any) -&gt; None:\n        \"\"\"\n        Allow for setting of attributes on the metadata class.\n\n        The attributes are set through the following heirarchy:\n            - If the model is sub-classed, it will be set as normal.\n            - Otherwise, if the attribute is private, it will be set as normal.\n            - Finally, if we are setting a public attribute on the base metadata class,\n                we use the extra field.\n        \"\"\"\n        if self._is_field_typed():\n            return super().__setattr__(name, value)\n\n        # We want to avoid setting any private attributes in the extra\n        # dictionary\n        if name.startswith(\"_\"):\n            return super().__setattr__(name, value)\n\n        # If it is `owner` or `task_results`, we want\n        # to avoid setting the attribute in the `extra` dictionary\n        if name in [\"owner\", \"task_results\"]:\n            return super().__setattr__(name, value)\n\n        self.extra[name] = value\n\n    def __delattr__(self, name: str) -&gt; None:\n        \"\"\"\n        Ensure that we can delete attributes from the metadata class.\n\n        The attributes are deleted through the following heirarchy:\n            - If the attribute is `task_results`, we use the descriptor to delete the task results.\n            - Otherwise, if it is a sub-classed model, it will be deleted as normal.\n            - Finally, if we are deleting a public attribute on the base metadata class,\n                we use the extra field.\n        \"\"\"\n\n        # We want to use the descriptor to delete the task results\n        if name == \"task_results\":\n            self._task_results.__delete__(self)\n            return\n\n        # Otherwise, we use our standard fallback tiers\n        if self._is_field_typed():\n            return super().__delattr__(name)\n\n        del self.extra[name]\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.pipeline.metadata.BaseMetadata.owner","title":"<code>owner: TMetadataOwner</code>  <code>property</code> <code>writable</code>","text":"<p>Return the owner of the metadata.</p>"},{"location":"reference/schema/#docprompt.schema.pipeline.metadata.BaseMetadata.task_results","title":"<code>task_results: TaskResultsDescriptor</code>  <code>property</code> <code>writable</code>","text":"<p>Return the task results descriptor.</p>"},{"location":"reference/schema/#docprompt.schema.pipeline.metadata.BaseMetadata.__delattr__","title":"<code>__delattr__(name)</code>","text":"<p>Ensure that we can delete attributes from the metadata class.</p> The attributes are deleted through the following heirarchy <ul> <li>If the attribute is <code>task_results</code>, we use the descriptor to delete the task results.</li> <li>Otherwise, if it is a sub-classed model, it will be deleted as normal.</li> <li>Finally, if we are deleting a public attribute on the base metadata class,     we use the extra field.</li> </ul> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>def __delattr__(self, name: str) -&gt; None:\n    \"\"\"\n    Ensure that we can delete attributes from the metadata class.\n\n    The attributes are deleted through the following heirarchy:\n        - If the attribute is `task_results`, we use the descriptor to delete the task results.\n        - Otherwise, if it is a sub-classed model, it will be deleted as normal.\n        - Finally, if we are deleting a public attribute on the base metadata class,\n            we use the extra field.\n    \"\"\"\n\n    # We want to use the descriptor to delete the task results\n    if name == \"task_results\":\n        self._task_results.__delete__(self)\n        return\n\n    # Otherwise, we use our standard fallback tiers\n    if self._is_field_typed():\n        return super().__delattr__(name)\n\n    del self.extra[name]\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.pipeline.metadata.BaseMetadata.__delitem__","title":"<code>__delitem__(name)</code>","text":"<p>Provide dictionary functionality to the metadata class.</p> <p>This only works for the base metadata model. If sub-classed, this will raise an error, unless overridden, as BaseModel's do not have an delitem method.</p> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>def __delitem__(self, name):\n    \"\"\"\n    Provide dictionary functionality to the metadata class.\n\n    This only works for the base metadata model. If sub-classed, this will raise an error,\n    unless overridden, as BaseModel's do not have an __delitem__ method.\n    \"\"\"\n    if not self._is_field_typed():\n        del self.extra[name]\n    else:\n        raise AttributeError(\n            f\"'{self.__class__.__name__}' object has no attribute '{name}'\"\n        )\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.pipeline.metadata.BaseMetadata.__getattr__","title":"<code>__getattr__(name)</code>","text":"<p>Allow for getting of attributes on the metadata class.</p> The attributes are retrieved through the following heirarchy <ul> <li>If the model is sub-classed, it will be retrieved as normal.</li> <li>Otherwise, if the attribute is private, it will be retrieved as normal.</li> <li>Finally, if we are getting a public attribute on the base metadata class,     we use the extra field.</li> <li>If the key is not set in the <code>extra</code> dictionary, we resort back to just trying to get the field.<ul> <li>This is when we grab the <code>owner</code> or <code>task_result</code> attribuite.</li> </ul> </li> </ul> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>def __getattr__(self, name):\n    \"\"\"\n    Allow for getting of attributes on the metadata class.\n\n    The attributes are retrieved through the following heirarchy:\n        - If the model is sub-classed, it will be retrieved as normal.\n        - Otherwise, if the attribute is private, it will be retrieved as normal.\n        - Finally, if we are getting a public attribute on the base metadata class,\n            we use the extra field.\n        - If the key is not set in the `extra` dictionary, we resort back to just\n        trying to get the field.\n            - This is when we grab the `owner` or `task_result` attribuite.\n    \"\"\"\n    if self._is_field_typed():\n        return super().__getattr__(name)\n\n    if name.startswith(\"_\"):\n        return super().__getattr__(name)\n\n    # Attempt to retreieve the attr from the `extra` field\n    try:\n        return self.extra.get(name)\n\n    except KeyError:\n        # This is for grabbing properties on the base metadata class\n        return super().__getattr__(name)\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.pipeline.metadata.BaseMetadata.__getitem__","title":"<code>__getitem__(name)</code>","text":"<p>Provide dictionary functionlaity to the metadata class.</p> <p>This only works for the base metadata model. If sub-classed, this will raise an error, unless overridden, as BaseModel's do not have an getitem method.</p> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>def __getitem__(self, name):\n    \"\"\"\n    Provide dictionary functionlaity to the metadata class.\n\n    This only works for the base metadata model. If sub-classed, this will raise an error,\n    unless overridden, as BaseModel's do not have an __getitem__ method.\n    \"\"\"\n    if not self._is_field_typed():\n        return self.extra[name]\n\n    raise AttributeError(\n        f\"'{self.__class__.__name__}' object has no attribute '{name}'\"\n    )\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.pipeline.metadata.BaseMetadata.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over the keys in the metadata.</p> <p>This only works for the base metadata model. If sub-classed, this will raise an error, unless overridden, as BaseModel's do not have an iter method.</p> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>def __iter__(self):\n    \"\"\"\n    Iterate over the keys in the metadata.\n\n    This only works for the base metadata model. If sub-classed, this will raise an error,\n    unless overridden, as BaseModel's do not have an __iter__ method.\n    \"\"\"\n    if self._is_field_typed():\n        raise AttributeError(f\"'{self.__class__.__name__}' object is not iterable\")\n\n    return iter(self.extra)\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.pipeline.metadata.BaseMetadata.__len__","title":"<code>__len__()</code>","text":"<p>Get the number of keys in the metadata.</p> <p>This only works for the base metadata model. If sub-classed, this will raise an error, unless overridden, as BaseModel's do not have a len method.</p> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>def __len__(self):\n    \"\"\"\n    Get the number of keys in the metadata.\n\n    This only works for the base metadata model. If sub-classed, this will raise an error,\n    unless overridden, as BaseModel's do not have a __len__ method.\n    \"\"\"\n    if self._is_field_typed():\n        raise AttributeError(\n            f\"'{self.__class__.__name__}' object has no attribute '__len__'\"\n        )\n\n    return len(self.extra)\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.pipeline.metadata.BaseMetadata.__repr__","title":"<code>__repr__()</code>","text":"<p>Provide a string representation of the metadata.</p> <p>This only works for the base metadata model. If sub-classed, this will raise an error, unless overridden, as BaseModel's do not have a repr method.</p> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>def __repr__(self):\n    \"\"\"\n    Provide a string representation of the metadata.\n\n    This only works for the base metadata model. If sub-classed, this will raise an error,\n    unless overridden, as BaseModel's do not have a __repr__ method.\n    \"\"\"\n    if self._is_field_typed():\n        return super().__repr__()\n\n    # Otherwise, we are deailing with dictornary-like metadata\n    return json.dumps(self.extra)\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.pipeline.metadata.BaseMetadata.__setattr__","title":"<code>__setattr__(name, value)</code>","text":"<p>Allow for setting of attributes on the metadata class.</p> The attributes are set through the following heirarchy <ul> <li>If the model is sub-classed, it will be set as normal.</li> <li>Otherwise, if the attribute is private, it will be set as normal.</li> <li>Finally, if we are setting a public attribute on the base metadata class,     we use the extra field.</li> </ul> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>def __setattr__(self, name: str, value: Any) -&gt; None:\n    \"\"\"\n    Allow for setting of attributes on the metadata class.\n\n    The attributes are set through the following heirarchy:\n        - If the model is sub-classed, it will be set as normal.\n        - Otherwise, if the attribute is private, it will be set as normal.\n        - Finally, if we are setting a public attribute on the base metadata class,\n            we use the extra field.\n    \"\"\"\n    if self._is_field_typed():\n        return super().__setattr__(name, value)\n\n    # We want to avoid setting any private attributes in the extra\n    # dictionary\n    if name.startswith(\"_\"):\n        return super().__setattr__(name, value)\n\n    # If it is `owner` or `task_results`, we want\n    # to avoid setting the attribute in the `extra` dictionary\n    if name in [\"owner\", \"task_results\"]:\n        return super().__setattr__(name, value)\n\n    self.extra[name] = value\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.pipeline.metadata.BaseMetadata.__setitem__","title":"<code>__setitem__(name, value)</code>","text":"<p>Provide dictionary functionality to the metadata class.</p> <p>This only works for the base metadata model. If sub-classed, this will raise an error, unless overridden, as BaseModel's do not have an setitem method.</p> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>def __setitem__(self, name, value):\n    \"\"\"\n    Provide dictionary functionality to the metadata class.\n\n    This only works for the base metadata model. If sub-classed, this will raise an error,\n    unless overridden, as BaseModel's do not have an __setitem__ method.\n    \"\"\"\n    if not self._is_field_typed():\n        self.extra[name] = value\n    else:\n        raise AttributeError(\n            f\"'{self.__class__.__name__}' object has no attribute '{name}'\"\n        )\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.pipeline.metadata.BaseMetadata.from_owner","title":"<code>from_owner(owner, **data)</code>  <code>classmethod</code>","text":"<p>Create a new instance of the metadata class with the owner set.</p> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>@classmethod\ndef from_owner(cls, owner: TMetadataOwner, **data) -&gt; BaseMetadata:\n    \"\"\"Create a new instance of the metadata class with the owner set.\"\"\"\n    metadata = cls(**data)\n    metadata.owner = owner\n    return metadata\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.pipeline.metadata.BaseMetadata.validate_data_fields_from_annotations","title":"<code>validate_data_fields_from_annotations(data)</code>  <code>classmethod</code>","text":"<p>Validate the data fields from the annotations.</p> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>@model_validator(mode=\"before\")\n@classmethod\ndef validate_data_fields_from_annotations(cls, data: Any) -&gt; Any:\n    \"\"\"Validate the data fields from the annotations.\"\"\"\n\n    # We want to make sure that we combine the `extra` metdata along with any\n    # other specific fields that are defined in the metadata.\n    extra = data.pop(\"extra\", {})\n    assert isinstance(extra, dict), \"The `extra` field must be a dictionary.\"\n    data = {**data, **extra}\n\n    # If the model has been sub-classed, then all of our fields must be\n    # validated by the pydantic model.\n    if cls._is_field_typed():\n        # We will get the fields out of extra and set them as potential fields to\n        # validate. They will be ignored if they are not defined in the model, but it\n        # allows for a more flexible way to define metadata.\n        # Otherwise, what ever is in the `extra` field will be stroed in the `extra` field.\n        return {**data, \"extra\": extra}\n\n    # Otherwise, we are using our mock-dict implentation, so we store our\n    # metadata in the `extra` field.\n    return {\"extra\": data}\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.pipeline.node","title":"<code>node</code>","text":""},{"location":"reference/schema/#docprompt.schema.pipeline.node.DocumentCollection","title":"<code>DocumentCollection</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[DocumentCollectionMetadata, DocumentNodeMetadata, PageNodeMetadata]</code></p> <p>Represents a collection of documents with some common metadata</p> Source code in <code>docprompt/schema/pipeline/node/collection.py</code> <pre><code>class DocumentCollection(\n    BaseModel,\n    Generic[DocumentCollectionMetadata, DocumentNodeMetadata, PageNodeMetadata],\n):\n    \"\"\"\n    Represents a collection of documents with some common metadata\n    \"\"\"\n\n    document_nodes: List[\"DocumentNode[DocumentNodeMetadata, PageNodeMetadata]\"]\n    metadata: DocumentCollectionMetadata = Field(..., default_factory=dict)\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.pipeline.node.DocumentNode","title":"<code>DocumentNode</code>","text":"<p>               Bases: <code>BaseNode</code>, <code>Generic[DocumentNodeMetadata, PageNodeMetadata]</code></p> <p>Represents a single document, with some metadata</p> Source code in <code>docprompt/schema/pipeline/node/document.py</code> <pre><code>class DocumentNode(BaseNode, Generic[DocumentNodeMetadata, PageNodeMetadata]):\n    \"\"\"\n    Represents a single document, with some metadata\n    \"\"\"\n\n    document: PdfDocument\n    page_nodes: List[PageNode[PageNodeMetadata]] = Field(\n        description=\"The pages in the document\", default_factory=list, repr=False\n    )\n    metadata: DocumentNodeMetadata = Field(\n        description=\"Application-specific metadata for the document\",\n        default_factory=BaseMetadata,\n    )\n\n    _locator: Optional[\"DocumentProvenanceLocator\"] = PrivateAttr(default=None)\n\n    _persistance_path: Optional[str] = PrivateAttr(default=None)\n\n    def __getstate__(self):\n        state = super().__getstate__()\n\n        state[\"__pydantic_private__\"][\"_locator\"] = None\n\n        return state\n\n    def __len__(self):\n        return len(self.page_nodes)\n\n    def __getitem__(self, index):\n        return self.page_nodes[index]\n\n    def __iter__(self):\n        return iter(self.page_nodes)\n\n    @property\n    def rasterizer(self):\n        return DocumentRasterizer(self)\n\n    @property\n    def locator(self):\n        if self._locator is None:\n            self.refresh_locator()\n\n        return self._locator\n\n    def refresh_locator(self):\n        \"\"\"\n        Refreshes the locator for this document node\n        \"\"\"\n        from docprompt.provenance.search import DocumentProvenanceLocator\n\n        if any(not page.ocr_results.result for page in self.page_nodes):\n            raise ValueError(\n                \"Cannot create a locator for a document node with missing OCR results\"\n            )\n\n        self._locator = DocumentProvenanceLocator.from_document_node(self)\n\n        return self.locator\n\n    @classmethod\n    def from_document(\n        cls,\n        document: PdfDocument,\n        document_metadata: Optional[DocumentNodeMetadata] = None,\n        page_metadata: Optional[List[PageNodeMetadata]] = None,\n    ):\n        document_node: \"DocumentNode[DocumentNodeMetadata, PageNodeMetadata]\" = cls(\n            document=document,\n        )\n        document_node.metadata = document_metadata or cls.metadata_class().from_owner(\n            document_node, **{}\n        )\n\n        if page_metadata is not None and len(page_metadata) != len(document):\n            raise ValueError(\n                \"The number of page metadata items must match the number of pages in the document.\"\n            )\n\n        for page_number in range(1, len(document) + 1):\n            if page_metadata is not None:\n                page_node = PageNode(\n                    document=document_node,\n                    page_number=page_number,\n                    metadata=page_metadata[page_number - 1],\n                )\n            else:\n                page_node = PageNode(document=document_node, page_number=page_number)\n\n            document_node.page_nodes.append(page_node)\n\n        return document_node\n\n    @property\n    def file_hash(self):\n        return self.document.document_hash\n\n    @property\n    def document_name(self):\n        return self.document.name\n\n    @classmethod\n    def metadata_class(cls) -&gt; Type[BaseMetadata]:\n        \"\"\"Get the metadata class for instantiating metadata from the model.\"\"\"\n\n        fields = cls.model_fields\n\n        # NOTE: The indexing is important here, and relies on the generic type being\n        # the SECOND of the two arguments in the `Union` annotation\n        metadata_field_annotation = fields[\"metadata\"].annotation\n\n        # If no override has been provided to the metadata model, we want to retrieve\n        # it as a TypedDict\n        if metadata_field_annotation == DocumentNodeMetadata:\n            return BaseMetadata\n\n        if isinstance(metadata_field_annotation, ForwardRef):\n            raise ValueError(\n                \"You cannot define DocumentNode with a ForwardRef for Generic metadata model types.\"\n            )\n\n        # Get the overriden Generic type of th DocumentNodeMetadata\n        return metadata_field_annotation\n\n    @classmethod\n    def page_metadata_class(cls) -&gt; Type[Union[dict, BaseModel]]:\n        \"\"\"Get the metadata class for the page nodes in the document.\"\"\"\n        fields = cls.model_fields\n\n        # NOTE: The indexing is important here, and it allows us to get the type of each\n        # page node in the `List` annotation\n        page_nodes_field_class = fields[\"page_nodes\"].annotation.__args__[0]\n\n        # NOTE: The indexing is important here, and relies on the generic type being\n        # the SECOND of the two arguments in the `Union` annotation\n        page_node_metadata_field_annotation = page_nodes_field_class.model_fields[\n            \"metadata\"\n        ].annotation\n\n        if page_node_metadata_field_annotation == PageNodeMetadata:\n            return BaseMetadata\n\n        if isinstance(page_node_metadata_field_annotation, ForwardRef):\n            raise ValueError(\n                \"You cannot define PageNode with a ForwardRef for Generic metadata model types.\"\n            )\n\n        return page_node_metadata_field_annotation\n\n    @property\n    def persistance_path(self):\n        \"\"\"The base path to storage location.\"\"\"\n        return self._persistance_path\n\n    @persistance_path.setter\n    def persistance_path(self, path: str):\n        \"\"\"Set the base path to storage location.\"\"\"\n        self._persistance_path = path\n\n    @classmethod\n    def from_storage(cls, path: str, file_hash: str, **kwargs) -&gt; Self:\n        \"\"\"Load the document node from storage.\n\n        Args:\n            path (str): The base path to storage location.\n                - Example (S3): \"s3://bucket-name/key/to/folder\"\n                - Example (Local FS): \"/tmp/docprompt/storage\"\n            file_hash (str): The hash of the document.\n            **kwargs: Additional keyword arguments for fsspec FileSystem\n\n        Returns:\n            DocumentNode: The loaded document node.\n        \"\"\"\n\n        fs_manager = FileSystemManager(path, **kwargs)\n\n        pdf_bytes, metadata_bytes, page_metadata_bytes = fs_manager.read(\n            file_hash, **kwargs\n        )\n\n        doc = PdfDocument.from_bytes(pdf_bytes, name=fs_manager.get_pdf_name(file_hash))\n        node = cls.from_document(doc)\n\n        if metadata_bytes:\n            metadata_json = json.loads(metadata_bytes.decode(\"utf-8\"))\n            metadata = cls.metadata_class().from_owner(node, **metadata_json)\n        else:\n            metadata = cls.metadata_class().from_owner(node, **{})\n\n        if page_metadata_bytes:\n            page_metadata_json = [\n                json.loads(page_str)\n                for page_str in json.loads(page_metadata_bytes.decode(\"utf-8\"))\n            ]\n            page_metadata = [\n                cls.page_metadata_class()(**page) for page in page_metadata_json\n            ]\n        else:\n            page_metadata = [cls.page_metadata_class()(**{}) for _ in range(len(doc))]\n\n        # Store the metadata on the node and page nodes\n        node.metadata = metadata\n        for page, meta in zip(node.page_nodes, page_metadata):\n            meta.set_owner(page)\n            page.metadata = meta\n\n        # Make sure to set the persistance path on the node\n        node.persistance_path = path\n\n        return node\n\n    def persist(self, path: Optional[str] = None, **kwargs) -&gt; FileSidecarsPathManager:\n        \"\"\"Persist a document node to storage.\n\n        Args:\n            path (Optional[str]): Overwrites the current `persistance_path` property\n                - If `persistance_path` is not currently set, path must be provided.\n            **kwargs: Additional keyword arguments for fsspec FileSystem\n\n        Returns:\n            FileSidecarsPathManager: The file path manager for the persisted document node.\n        \"\"\"\n\n        path = path or self.persistance_path\n\n        if path is None:\n            raise ValueError(\"The path must be provided to persist the document node.\")\n\n        # Make sure to update the persistance path\n        self.persistance_path = path\n\n        fs_manager = FileSystemManager(path, **kwargs)\n\n        pdf_bytes = self.document.get_bytes()\n        metadata_bytes = bytes(self.metadata.model_dump_json(), encoding=\"utf-8\")\n        page_metadata_bytes = bytes(\n            json.dumps([page.metadata.model_dump_json() for page in self.page_nodes]),\n            encoding=\"utf-8\",\n        )\n\n        return fs_manager.write(\n            pdf_bytes, metadata_bytes, page_metadata_bytes, **kwargs\n        )\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.pipeline.node.DocumentNode.persistance_path","title":"<code>persistance_path</code>  <code>property</code> <code>writable</code>","text":"<p>The base path to storage location.</p>"},{"location":"reference/schema/#docprompt.schema.pipeline.node.DocumentNode.from_storage","title":"<code>from_storage(path, file_hash, **kwargs)</code>  <code>classmethod</code>","text":"<p>Load the document node from storage.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The base path to storage location. - Example (S3): \"s3://bucket-name/key/to/folder\" - Example (Local FS): \"/tmp/docprompt/storage\"</p> required <code>file_hash</code> <code>str</code> <p>The hash of the document.</p> required <code>**kwargs</code> <p>Additional keyword arguments for fsspec FileSystem</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>DocumentNode</code> <code>Self</code> <p>The loaded document node.</p> Source code in <code>docprompt/schema/pipeline/node/document.py</code> <pre><code>@classmethod\ndef from_storage(cls, path: str, file_hash: str, **kwargs) -&gt; Self:\n    \"\"\"Load the document node from storage.\n\n    Args:\n        path (str): The base path to storage location.\n            - Example (S3): \"s3://bucket-name/key/to/folder\"\n            - Example (Local FS): \"/tmp/docprompt/storage\"\n        file_hash (str): The hash of the document.\n        **kwargs: Additional keyword arguments for fsspec FileSystem\n\n    Returns:\n        DocumentNode: The loaded document node.\n    \"\"\"\n\n    fs_manager = FileSystemManager(path, **kwargs)\n\n    pdf_bytes, metadata_bytes, page_metadata_bytes = fs_manager.read(\n        file_hash, **kwargs\n    )\n\n    doc = PdfDocument.from_bytes(pdf_bytes, name=fs_manager.get_pdf_name(file_hash))\n    node = cls.from_document(doc)\n\n    if metadata_bytes:\n        metadata_json = json.loads(metadata_bytes.decode(\"utf-8\"))\n        metadata = cls.metadata_class().from_owner(node, **metadata_json)\n    else:\n        metadata = cls.metadata_class().from_owner(node, **{})\n\n    if page_metadata_bytes:\n        page_metadata_json = [\n            json.loads(page_str)\n            for page_str in json.loads(page_metadata_bytes.decode(\"utf-8\"))\n        ]\n        page_metadata = [\n            cls.page_metadata_class()(**page) for page in page_metadata_json\n        ]\n    else:\n        page_metadata = [cls.page_metadata_class()(**{}) for _ in range(len(doc))]\n\n    # Store the metadata on the node and page nodes\n    node.metadata = metadata\n    for page, meta in zip(node.page_nodes, page_metadata):\n        meta.set_owner(page)\n        page.metadata = meta\n\n    # Make sure to set the persistance path on the node\n    node.persistance_path = path\n\n    return node\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.pipeline.node.DocumentNode.metadata_class","title":"<code>metadata_class()</code>  <code>classmethod</code>","text":"<p>Get the metadata class for instantiating metadata from the model.</p> Source code in <code>docprompt/schema/pipeline/node/document.py</code> <pre><code>@classmethod\ndef metadata_class(cls) -&gt; Type[BaseMetadata]:\n    \"\"\"Get the metadata class for instantiating metadata from the model.\"\"\"\n\n    fields = cls.model_fields\n\n    # NOTE: The indexing is important here, and relies on the generic type being\n    # the SECOND of the two arguments in the `Union` annotation\n    metadata_field_annotation = fields[\"metadata\"].annotation\n\n    # If no override has been provided to the metadata model, we want to retrieve\n    # it as a TypedDict\n    if metadata_field_annotation == DocumentNodeMetadata:\n        return BaseMetadata\n\n    if isinstance(metadata_field_annotation, ForwardRef):\n        raise ValueError(\n            \"You cannot define DocumentNode with a ForwardRef for Generic metadata model types.\"\n        )\n\n    # Get the overriden Generic type of th DocumentNodeMetadata\n    return metadata_field_annotation\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.pipeline.node.DocumentNode.page_metadata_class","title":"<code>page_metadata_class()</code>  <code>classmethod</code>","text":"<p>Get the metadata class for the page nodes in the document.</p> Source code in <code>docprompt/schema/pipeline/node/document.py</code> <pre><code>@classmethod\ndef page_metadata_class(cls) -&gt; Type[Union[dict, BaseModel]]:\n    \"\"\"Get the metadata class for the page nodes in the document.\"\"\"\n    fields = cls.model_fields\n\n    # NOTE: The indexing is important here, and it allows us to get the type of each\n    # page node in the `List` annotation\n    page_nodes_field_class = fields[\"page_nodes\"].annotation.__args__[0]\n\n    # NOTE: The indexing is important here, and relies on the generic type being\n    # the SECOND of the two arguments in the `Union` annotation\n    page_node_metadata_field_annotation = page_nodes_field_class.model_fields[\n        \"metadata\"\n    ].annotation\n\n    if page_node_metadata_field_annotation == PageNodeMetadata:\n        return BaseMetadata\n\n    if isinstance(page_node_metadata_field_annotation, ForwardRef):\n        raise ValueError(\n            \"You cannot define PageNode with a ForwardRef for Generic metadata model types.\"\n        )\n\n    return page_node_metadata_field_annotation\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.pipeline.node.DocumentNode.persist","title":"<code>persist(path=None, **kwargs)</code>","text":"<p>Persist a document node to storage.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Optional[str]</code> <p>Overwrites the current <code>persistance_path</code> property - If <code>persistance_path</code> is not currently set, path must be provided.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments for fsspec FileSystem</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>FileSidecarsPathManager</code> <code>FileSidecarsPathManager</code> <p>The file path manager for the persisted document node.</p> Source code in <code>docprompt/schema/pipeline/node/document.py</code> <pre><code>def persist(self, path: Optional[str] = None, **kwargs) -&gt; FileSidecarsPathManager:\n    \"\"\"Persist a document node to storage.\n\n    Args:\n        path (Optional[str]): Overwrites the current `persistance_path` property\n            - If `persistance_path` is not currently set, path must be provided.\n        **kwargs: Additional keyword arguments for fsspec FileSystem\n\n    Returns:\n        FileSidecarsPathManager: The file path manager for the persisted document node.\n    \"\"\"\n\n    path = path or self.persistance_path\n\n    if path is None:\n        raise ValueError(\"The path must be provided to persist the document node.\")\n\n    # Make sure to update the persistance path\n    self.persistance_path = path\n\n    fs_manager = FileSystemManager(path, **kwargs)\n\n    pdf_bytes = self.document.get_bytes()\n    metadata_bytes = bytes(self.metadata.model_dump_json(), encoding=\"utf-8\")\n    page_metadata_bytes = bytes(\n        json.dumps([page.metadata.model_dump_json() for page in self.page_nodes]),\n        encoding=\"utf-8\",\n    )\n\n    return fs_manager.write(\n        pdf_bytes, metadata_bytes, page_metadata_bytes, **kwargs\n    )\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.pipeline.node.DocumentNode.refresh_locator","title":"<code>refresh_locator()</code>","text":"<p>Refreshes the locator for this document node</p> Source code in <code>docprompt/schema/pipeline/node/document.py</code> <pre><code>def refresh_locator(self):\n    \"\"\"\n    Refreshes the locator for this document node\n    \"\"\"\n    from docprompt.provenance.search import DocumentProvenanceLocator\n\n    if any(not page.ocr_results.result for page in self.page_nodes):\n        raise ValueError(\n            \"Cannot create a locator for a document node with missing OCR results\"\n        )\n\n    self._locator = DocumentProvenanceLocator.from_document_node(self)\n\n    return self.locator\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.pipeline.node.PageNode","title":"<code>PageNode</code>","text":"<p>               Bases: <code>BaseNode</code>, <code>Generic[PageNodeMetadata]</code></p> <p>Represents a single page in a document, with some metadata</p> Source code in <code>docprompt/schema/pipeline/node/page.py</code> <pre><code>class PageNode(BaseNode, Generic[PageNodeMetadata]):\n    \"\"\"\n    Represents a single page in a document, with some metadata\n    \"\"\"\n\n    document: \"DocumentNode\" = Field(exclude=True, repr=False)\n    page_number: PositiveInt = Field(description=\"The page number\")\n    metadata: PageNodeMetadata = Field(\n        description=\"Application-specific metadata for the page\",\n        default_factory=BaseMetadata,\n    )\n    extra: Dict[str, Any] = Field(\n        description=\"Extra data that can be stored on the page node\",\n        default_factory=dict,\n    )\n\n    ocr_results: ResultContainer[OcrPageResult] = Field(\n        default_factory=_result_container_factory,\n        description=\"The OCR results for the page\",\n        repr=False,\n    )\n\n    _raster_cache: Dict[str, bytes] = PrivateAttr(default_factory=dict)\n\n    def __getstate__(self):\n        state = super().__getstate__()\n\n        state[\"__pydantic_private__\"][\"_raster_cache\"] = {}\n\n        return state\n\n    @property\n    def rasterizer(self):\n        return PageRasterizer(self._raster_cache, self)\n\n    def search(\n        self, query: str, refine_to_words: bool = True, require_exact_match: bool = True\n    ):\n        return self.document.locator.search(\n            query,\n            page_number=self.page_number,\n            refine_to_word=refine_to_words,\n            require_exact_match=require_exact_match,\n        )\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.pipeline.node.base","title":"<code>base</code>","text":""},{"location":"reference/schema/#docprompt.schema.pipeline.node.base.BaseNode","title":"<code>BaseNode</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>The base node class is utilized for defining a basic yet flexible interface</p> Source code in <code>docprompt/schema/pipeline/node/base.py</code> <pre><code>class BaseNode(BaseModel):\n    \"\"\"The base node class is utilized for defining a basic yet flexible interface\"\"\"\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.pipeline.node.collection","title":"<code>collection</code>","text":""},{"location":"reference/schema/#docprompt.schema.pipeline.node.collection.DocumentCollection","title":"<code>DocumentCollection</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[DocumentCollectionMetadata, DocumentNodeMetadata, PageNodeMetadata]</code></p> <p>Represents a collection of documents with some common metadata</p> Source code in <code>docprompt/schema/pipeline/node/collection.py</code> <pre><code>class DocumentCollection(\n    BaseModel,\n    Generic[DocumentCollectionMetadata, DocumentNodeMetadata, PageNodeMetadata],\n):\n    \"\"\"\n    Represents a collection of documents with some common metadata\n    \"\"\"\n\n    document_nodes: List[\"DocumentNode[DocumentNodeMetadata, PageNodeMetadata]\"]\n    metadata: DocumentCollectionMetadata = Field(..., default_factory=dict)\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.pipeline.node.document","title":"<code>document</code>","text":""},{"location":"reference/schema/#docprompt.schema.pipeline.node.document.DocumentNode","title":"<code>DocumentNode</code>","text":"<p>               Bases: <code>BaseNode</code>, <code>Generic[DocumentNodeMetadata, PageNodeMetadata]</code></p> <p>Represents a single document, with some metadata</p> Source code in <code>docprompt/schema/pipeline/node/document.py</code> <pre><code>class DocumentNode(BaseNode, Generic[DocumentNodeMetadata, PageNodeMetadata]):\n    \"\"\"\n    Represents a single document, with some metadata\n    \"\"\"\n\n    document: PdfDocument\n    page_nodes: List[PageNode[PageNodeMetadata]] = Field(\n        description=\"The pages in the document\", default_factory=list, repr=False\n    )\n    metadata: DocumentNodeMetadata = Field(\n        description=\"Application-specific metadata for the document\",\n        default_factory=BaseMetadata,\n    )\n\n    _locator: Optional[\"DocumentProvenanceLocator\"] = PrivateAttr(default=None)\n\n    _persistance_path: Optional[str] = PrivateAttr(default=None)\n\n    def __getstate__(self):\n        state = super().__getstate__()\n\n        state[\"__pydantic_private__\"][\"_locator\"] = None\n\n        return state\n\n    def __len__(self):\n        return len(self.page_nodes)\n\n    def __getitem__(self, index):\n        return self.page_nodes[index]\n\n    def __iter__(self):\n        return iter(self.page_nodes)\n\n    @property\n    def rasterizer(self):\n        return DocumentRasterizer(self)\n\n    @property\n    def locator(self):\n        if self._locator is None:\n            self.refresh_locator()\n\n        return self._locator\n\n    def refresh_locator(self):\n        \"\"\"\n        Refreshes the locator for this document node\n        \"\"\"\n        from docprompt.provenance.search import DocumentProvenanceLocator\n\n        if any(not page.ocr_results.result for page in self.page_nodes):\n            raise ValueError(\n                \"Cannot create a locator for a document node with missing OCR results\"\n            )\n\n        self._locator = DocumentProvenanceLocator.from_document_node(self)\n\n        return self.locator\n\n    @classmethod\n    def from_document(\n        cls,\n        document: PdfDocument,\n        document_metadata: Optional[DocumentNodeMetadata] = None,\n        page_metadata: Optional[List[PageNodeMetadata]] = None,\n    ):\n        document_node: \"DocumentNode[DocumentNodeMetadata, PageNodeMetadata]\" = cls(\n            document=document,\n        )\n        document_node.metadata = document_metadata or cls.metadata_class().from_owner(\n            document_node, **{}\n        )\n\n        if page_metadata is not None and len(page_metadata) != len(document):\n            raise ValueError(\n                \"The number of page metadata items must match the number of pages in the document.\"\n            )\n\n        for page_number in range(1, len(document) + 1):\n            if page_metadata is not None:\n                page_node = PageNode(\n                    document=document_node,\n                    page_number=page_number,\n                    metadata=page_metadata[page_number - 1],\n                )\n            else:\n                page_node = PageNode(document=document_node, page_number=page_number)\n\n            document_node.page_nodes.append(page_node)\n\n        return document_node\n\n    @property\n    def file_hash(self):\n        return self.document.document_hash\n\n    @property\n    def document_name(self):\n        return self.document.name\n\n    @classmethod\n    def metadata_class(cls) -&gt; Type[BaseMetadata]:\n        \"\"\"Get the metadata class for instantiating metadata from the model.\"\"\"\n\n        fields = cls.model_fields\n\n        # NOTE: The indexing is important here, and relies on the generic type being\n        # the SECOND of the two arguments in the `Union` annotation\n        metadata_field_annotation = fields[\"metadata\"].annotation\n\n        # If no override has been provided to the metadata model, we want to retrieve\n        # it as a TypedDict\n        if metadata_field_annotation == DocumentNodeMetadata:\n            return BaseMetadata\n\n        if isinstance(metadata_field_annotation, ForwardRef):\n            raise ValueError(\n                \"You cannot define DocumentNode with a ForwardRef for Generic metadata model types.\"\n            )\n\n        # Get the overriden Generic type of th DocumentNodeMetadata\n        return metadata_field_annotation\n\n    @classmethod\n    def page_metadata_class(cls) -&gt; Type[Union[dict, BaseModel]]:\n        \"\"\"Get the metadata class for the page nodes in the document.\"\"\"\n        fields = cls.model_fields\n\n        # NOTE: The indexing is important here, and it allows us to get the type of each\n        # page node in the `List` annotation\n        page_nodes_field_class = fields[\"page_nodes\"].annotation.__args__[0]\n\n        # NOTE: The indexing is important here, and relies on the generic type being\n        # the SECOND of the two arguments in the `Union` annotation\n        page_node_metadata_field_annotation = page_nodes_field_class.model_fields[\n            \"metadata\"\n        ].annotation\n\n        if page_node_metadata_field_annotation == PageNodeMetadata:\n            return BaseMetadata\n\n        if isinstance(page_node_metadata_field_annotation, ForwardRef):\n            raise ValueError(\n                \"You cannot define PageNode with a ForwardRef for Generic metadata model types.\"\n            )\n\n        return page_node_metadata_field_annotation\n\n    @property\n    def persistance_path(self):\n        \"\"\"The base path to storage location.\"\"\"\n        return self._persistance_path\n\n    @persistance_path.setter\n    def persistance_path(self, path: str):\n        \"\"\"Set the base path to storage location.\"\"\"\n        self._persistance_path = path\n\n    @classmethod\n    def from_storage(cls, path: str, file_hash: str, **kwargs) -&gt; Self:\n        \"\"\"Load the document node from storage.\n\n        Args:\n            path (str): The base path to storage location.\n                - Example (S3): \"s3://bucket-name/key/to/folder\"\n                - Example (Local FS): \"/tmp/docprompt/storage\"\n            file_hash (str): The hash of the document.\n            **kwargs: Additional keyword arguments for fsspec FileSystem\n\n        Returns:\n            DocumentNode: The loaded document node.\n        \"\"\"\n\n        fs_manager = FileSystemManager(path, **kwargs)\n\n        pdf_bytes, metadata_bytes, page_metadata_bytes = fs_manager.read(\n            file_hash, **kwargs\n        )\n\n        doc = PdfDocument.from_bytes(pdf_bytes, name=fs_manager.get_pdf_name(file_hash))\n        node = cls.from_document(doc)\n\n        if metadata_bytes:\n            metadata_json = json.loads(metadata_bytes.decode(\"utf-8\"))\n            metadata = cls.metadata_class().from_owner(node, **metadata_json)\n        else:\n            metadata = cls.metadata_class().from_owner(node, **{})\n\n        if page_metadata_bytes:\n            page_metadata_json = [\n                json.loads(page_str)\n                for page_str in json.loads(page_metadata_bytes.decode(\"utf-8\"))\n            ]\n            page_metadata = [\n                cls.page_metadata_class()(**page) for page in page_metadata_json\n            ]\n        else:\n            page_metadata = [cls.page_metadata_class()(**{}) for _ in range(len(doc))]\n\n        # Store the metadata on the node and page nodes\n        node.metadata = metadata\n        for page, meta in zip(node.page_nodes, page_metadata):\n            meta.set_owner(page)\n            page.metadata = meta\n\n        # Make sure to set the persistance path on the node\n        node.persistance_path = path\n\n        return node\n\n    def persist(self, path: Optional[str] = None, **kwargs) -&gt; FileSidecarsPathManager:\n        \"\"\"Persist a document node to storage.\n\n        Args:\n            path (Optional[str]): Overwrites the current `persistance_path` property\n                - If `persistance_path` is not currently set, path must be provided.\n            **kwargs: Additional keyword arguments for fsspec FileSystem\n\n        Returns:\n            FileSidecarsPathManager: The file path manager for the persisted document node.\n        \"\"\"\n\n        path = path or self.persistance_path\n\n        if path is None:\n            raise ValueError(\"The path must be provided to persist the document node.\")\n\n        # Make sure to update the persistance path\n        self.persistance_path = path\n\n        fs_manager = FileSystemManager(path, **kwargs)\n\n        pdf_bytes = self.document.get_bytes()\n        metadata_bytes = bytes(self.metadata.model_dump_json(), encoding=\"utf-8\")\n        page_metadata_bytes = bytes(\n            json.dumps([page.metadata.model_dump_json() for page in self.page_nodes]),\n            encoding=\"utf-8\",\n        )\n\n        return fs_manager.write(\n            pdf_bytes, metadata_bytes, page_metadata_bytes, **kwargs\n        )\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.pipeline.node.document.DocumentNode.persistance_path","title":"<code>persistance_path</code>  <code>property</code> <code>writable</code>","text":"<p>The base path to storage location.</p>"},{"location":"reference/schema/#docprompt.schema.pipeline.node.document.DocumentNode.from_storage","title":"<code>from_storage(path, file_hash, **kwargs)</code>  <code>classmethod</code>","text":"<p>Load the document node from storage.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The base path to storage location. - Example (S3): \"s3://bucket-name/key/to/folder\" - Example (Local FS): \"/tmp/docprompt/storage\"</p> required <code>file_hash</code> <code>str</code> <p>The hash of the document.</p> required <code>**kwargs</code> <p>Additional keyword arguments for fsspec FileSystem</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>DocumentNode</code> <code>Self</code> <p>The loaded document node.</p> Source code in <code>docprompt/schema/pipeline/node/document.py</code> <pre><code>@classmethod\ndef from_storage(cls, path: str, file_hash: str, **kwargs) -&gt; Self:\n    \"\"\"Load the document node from storage.\n\n    Args:\n        path (str): The base path to storage location.\n            - Example (S3): \"s3://bucket-name/key/to/folder\"\n            - Example (Local FS): \"/tmp/docprompt/storage\"\n        file_hash (str): The hash of the document.\n        **kwargs: Additional keyword arguments for fsspec FileSystem\n\n    Returns:\n        DocumentNode: The loaded document node.\n    \"\"\"\n\n    fs_manager = FileSystemManager(path, **kwargs)\n\n    pdf_bytes, metadata_bytes, page_metadata_bytes = fs_manager.read(\n        file_hash, **kwargs\n    )\n\n    doc = PdfDocument.from_bytes(pdf_bytes, name=fs_manager.get_pdf_name(file_hash))\n    node = cls.from_document(doc)\n\n    if metadata_bytes:\n        metadata_json = json.loads(metadata_bytes.decode(\"utf-8\"))\n        metadata = cls.metadata_class().from_owner(node, **metadata_json)\n    else:\n        metadata = cls.metadata_class().from_owner(node, **{})\n\n    if page_metadata_bytes:\n        page_metadata_json = [\n            json.loads(page_str)\n            for page_str in json.loads(page_metadata_bytes.decode(\"utf-8\"))\n        ]\n        page_metadata = [\n            cls.page_metadata_class()(**page) for page in page_metadata_json\n        ]\n    else:\n        page_metadata = [cls.page_metadata_class()(**{}) for _ in range(len(doc))]\n\n    # Store the metadata on the node and page nodes\n    node.metadata = metadata\n    for page, meta in zip(node.page_nodes, page_metadata):\n        meta.set_owner(page)\n        page.metadata = meta\n\n    # Make sure to set the persistance path on the node\n    node.persistance_path = path\n\n    return node\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.pipeline.node.document.DocumentNode.metadata_class","title":"<code>metadata_class()</code>  <code>classmethod</code>","text":"<p>Get the metadata class for instantiating metadata from the model.</p> Source code in <code>docprompt/schema/pipeline/node/document.py</code> <pre><code>@classmethod\ndef metadata_class(cls) -&gt; Type[BaseMetadata]:\n    \"\"\"Get the metadata class for instantiating metadata from the model.\"\"\"\n\n    fields = cls.model_fields\n\n    # NOTE: The indexing is important here, and relies on the generic type being\n    # the SECOND of the two arguments in the `Union` annotation\n    metadata_field_annotation = fields[\"metadata\"].annotation\n\n    # If no override has been provided to the metadata model, we want to retrieve\n    # it as a TypedDict\n    if metadata_field_annotation == DocumentNodeMetadata:\n        return BaseMetadata\n\n    if isinstance(metadata_field_annotation, ForwardRef):\n        raise ValueError(\n            \"You cannot define DocumentNode with a ForwardRef for Generic metadata model types.\"\n        )\n\n    # Get the overriden Generic type of th DocumentNodeMetadata\n    return metadata_field_annotation\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.pipeline.node.document.DocumentNode.page_metadata_class","title":"<code>page_metadata_class()</code>  <code>classmethod</code>","text":"<p>Get the metadata class for the page nodes in the document.</p> Source code in <code>docprompt/schema/pipeline/node/document.py</code> <pre><code>@classmethod\ndef page_metadata_class(cls) -&gt; Type[Union[dict, BaseModel]]:\n    \"\"\"Get the metadata class for the page nodes in the document.\"\"\"\n    fields = cls.model_fields\n\n    # NOTE: The indexing is important here, and it allows us to get the type of each\n    # page node in the `List` annotation\n    page_nodes_field_class = fields[\"page_nodes\"].annotation.__args__[0]\n\n    # NOTE: The indexing is important here, and relies on the generic type being\n    # the SECOND of the two arguments in the `Union` annotation\n    page_node_metadata_field_annotation = page_nodes_field_class.model_fields[\n        \"metadata\"\n    ].annotation\n\n    if page_node_metadata_field_annotation == PageNodeMetadata:\n        return BaseMetadata\n\n    if isinstance(page_node_metadata_field_annotation, ForwardRef):\n        raise ValueError(\n            \"You cannot define PageNode with a ForwardRef for Generic metadata model types.\"\n        )\n\n    return page_node_metadata_field_annotation\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.pipeline.node.document.DocumentNode.persist","title":"<code>persist(path=None, **kwargs)</code>","text":"<p>Persist a document node to storage.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Optional[str]</code> <p>Overwrites the current <code>persistance_path</code> property - If <code>persistance_path</code> is not currently set, path must be provided.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments for fsspec FileSystem</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>FileSidecarsPathManager</code> <code>FileSidecarsPathManager</code> <p>The file path manager for the persisted document node.</p> Source code in <code>docprompt/schema/pipeline/node/document.py</code> <pre><code>def persist(self, path: Optional[str] = None, **kwargs) -&gt; FileSidecarsPathManager:\n    \"\"\"Persist a document node to storage.\n\n    Args:\n        path (Optional[str]): Overwrites the current `persistance_path` property\n            - If `persistance_path` is not currently set, path must be provided.\n        **kwargs: Additional keyword arguments for fsspec FileSystem\n\n    Returns:\n        FileSidecarsPathManager: The file path manager for the persisted document node.\n    \"\"\"\n\n    path = path or self.persistance_path\n\n    if path is None:\n        raise ValueError(\"The path must be provided to persist the document node.\")\n\n    # Make sure to update the persistance path\n    self.persistance_path = path\n\n    fs_manager = FileSystemManager(path, **kwargs)\n\n    pdf_bytes = self.document.get_bytes()\n    metadata_bytes = bytes(self.metadata.model_dump_json(), encoding=\"utf-8\")\n    page_metadata_bytes = bytes(\n        json.dumps([page.metadata.model_dump_json() for page in self.page_nodes]),\n        encoding=\"utf-8\",\n    )\n\n    return fs_manager.write(\n        pdf_bytes, metadata_bytes, page_metadata_bytes, **kwargs\n    )\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.pipeline.node.document.DocumentNode.refresh_locator","title":"<code>refresh_locator()</code>","text":"<p>Refreshes the locator for this document node</p> Source code in <code>docprompt/schema/pipeline/node/document.py</code> <pre><code>def refresh_locator(self):\n    \"\"\"\n    Refreshes the locator for this document node\n    \"\"\"\n    from docprompt.provenance.search import DocumentProvenanceLocator\n\n    if any(not page.ocr_results.result for page in self.page_nodes):\n        raise ValueError(\n            \"Cannot create a locator for a document node with missing OCR results\"\n        )\n\n    self._locator = DocumentProvenanceLocator.from_document_node(self)\n\n    return self.locator\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.pipeline.node.page","title":"<code>page</code>","text":""},{"location":"reference/schema/#docprompt.schema.pipeline.node.page.PageNode","title":"<code>PageNode</code>","text":"<p>               Bases: <code>BaseNode</code>, <code>Generic[PageNodeMetadata]</code></p> <p>Represents a single page in a document, with some metadata</p> Source code in <code>docprompt/schema/pipeline/node/page.py</code> <pre><code>class PageNode(BaseNode, Generic[PageNodeMetadata]):\n    \"\"\"\n    Represents a single page in a document, with some metadata\n    \"\"\"\n\n    document: \"DocumentNode\" = Field(exclude=True, repr=False)\n    page_number: PositiveInt = Field(description=\"The page number\")\n    metadata: PageNodeMetadata = Field(\n        description=\"Application-specific metadata for the page\",\n        default_factory=BaseMetadata,\n    )\n    extra: Dict[str, Any] = Field(\n        description=\"Extra data that can be stored on the page node\",\n        default_factory=dict,\n    )\n\n    ocr_results: ResultContainer[OcrPageResult] = Field(\n        default_factory=_result_container_factory,\n        description=\"The OCR results for the page\",\n        repr=False,\n    )\n\n    _raster_cache: Dict[str, bytes] = PrivateAttr(default_factory=dict)\n\n    def __getstate__(self):\n        state = super().__getstate__()\n\n        state[\"__pydantic_private__\"][\"_raster_cache\"] = {}\n\n        return state\n\n    @property\n    def rasterizer(self):\n        return PageRasterizer(self._raster_cache, self)\n\n    def search(\n        self, query: str, refine_to_words: bool = True, require_exact_match: bool = True\n    ):\n        return self.document.locator.search(\n            query,\n            page_number=self.page_number,\n            refine_to_word=refine_to_words,\n            require_exact_match=require_exact_match,\n        )\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.pipeline.node.typing","title":"<code>typing</code>","text":""},{"location":"reference/schema/#docprompt.schema.pipeline.rasterizer","title":"<code>rasterizer</code>","text":""},{"location":"reference/schema/#docprompt.schema.pipeline.rasterizer.DocumentRasterizer","title":"<code>DocumentRasterizer</code>","text":"Source code in <code>docprompt/schema/pipeline/rasterizer.py</code> <pre><code>class DocumentRasterizer:\n    def __init__(self, owner: \"DocumentNode\"):\n        self.owner = owner\n\n    def rasterize(\n        self,\n        name: str,\n        *,\n        return_mode: Literal[\"bytes\", \"pil\"] = \"bytes\",\n        dpi: int = 100,\n        downscale_size: Optional[Tuple[int, int]] = None,\n        resize_mode: ResizeModes = \"thumbnail\",\n        resize_aspect_ratios: Optional[Iterable[AspectRatioRule]] = None,\n        do_convert: bool = False,\n        image_convert_mode: str = \"L\",\n        do_quantize: bool = False,\n        quantize_color_count: int = 8,\n        max_file_size_bytes: Optional[int] = None,\n        render_grayscale: bool = False,\n    ) -&gt; List[Union[bytes, Image.Image]]:\n        images = self.owner.document.rasterize_pdf(\n            dpi=dpi,\n            downscale_size=downscale_size,\n            resize_mode=resize_mode,\n            resize_aspect_ratios=resize_aspect_ratios,\n            do_convert=do_convert,\n            image_convert_mode=image_convert_mode,\n            do_quantize=do_quantize,\n            quantize_color_count=quantize_color_count,\n            max_file_size_bytes=max_file_size_bytes,\n            render_grayscale=render_grayscale,\n            return_mode=return_mode,\n        )\n\n        for page_number, image in images.items():\n            page_node = self.owner.page_nodes[page_number - 1]\n\n            page_node._raster_cache[name] = image\n\n        return list(images.values())\n\n    def propagate_cache(self, name: str, rasters: Dict[int, Union[bytes, Image.Image]]):\n        \"\"\"\n        Should be one-indexed\n        \"\"\"\n        for page_number, raster in rasters.items():\n            page_node = self.owner.page_nodes[page_number - 1]\n\n            page_node._raster_cache[name] = raster\n</code></pre>"},{"location":"reference/schema/#docprompt.schema.pipeline.rasterizer.DocumentRasterizer.propagate_cache","title":"<code>propagate_cache(name, rasters)</code>","text":"<p>Should be one-indexed</p> Source code in <code>docprompt/schema/pipeline/rasterizer.py</code> <pre><code>def propagate_cache(self, name: str, rasters: Dict[int, Union[bytes, Image.Image]]):\n    \"\"\"\n    Should be one-indexed\n    \"\"\"\n    for page_number, raster in rasters.items():\n        page_node = self.owner.page_nodes[page_number - 1]\n\n        page_node._raster_cache[name] = raster\n</code></pre>"},{"location":"reference/schema/document/","title":"document","text":""},{"location":"reference/schema/document/#docprompt.schema.document.PdfDocument","title":"<code>PdfDocument</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a PDF document</p> Source code in <code>docprompt/schema/document.py</code> <pre><code>class PdfDocument(BaseModel):\n    \"\"\"\n    Represents a PDF document\n    \"\"\"\n\n    name: str = Field(description=\"The name of the document\")\n    file_bytes: bytes = Field(description=\"The bytes of the document\", repr=False)\n    file_path: Optional[str] = None\n\n    password: Optional[SecretStr] = None\n\n    def __len__(self):\n        return self.num_pages\n\n    def __hash__(self):\n        return hash(self.document_hash)\n\n    @computed_field\n    @cached_property\n    def page_count(self) -&gt; PositiveInt:\n        from docprompt.utils.util import get_page_count\n\n        return get_page_count(self.file_bytes)\n\n    @property\n    def num_pages(self):\n        return self.page_count\n\n    @property\n    def bytes_per_page(self):\n        return len(self.file_bytes) / self.num_pages\n\n    @computed_field\n    @cached_property\n    def document_hash(self) -&gt; str:\n        from docprompt.utils.util import hash_from_bytes\n\n        return hash_from_bytes(self.file_bytes)\n\n    @field_serializer(\"file_bytes\")\n    def serialize_file_bytes(self, v: bytes, _info):\n        compressed = gzip.compress(v)\n\n        return base64.b64encode(compressed).decode(\"utf-8\")\n\n    @field_validator(\"file_bytes\")\n    def validate_file_bytes(cls, v: bytes):\n        if not isinstance(v, bytes):\n            raise ValueError(\"File bytes must be bytes\")\n\n        if len(v) == 0:\n            raise ValueError(\"File bytes must not be empty\")\n\n        if filetype.guess_mime(v) == \"text/plain\":\n            v = base64.b64decode(v, validate=True)\n\n        if filetype.guess_mime(v) == \"application/gzip\":\n            v = gzip.decompress(v)\n\n        if filetype.guess_mime(v) != \"application/pdf\":\n            raise ValueError(\"File bytes must be a PDF\")\n\n        return v\n\n    @classmethod\n    def from_path(cls, file_path: Union[PathLike, str]):\n        file_path = Path(file_path)\n\n        if not file_path.is_file():\n            raise ValueError(f\"File path {file_path} is not a file\")\n\n        file_bytes = file_path.read_bytes()\n\n        return cls(name=file_path.name, file_path=str(file_path), file_bytes=file_bytes)\n\n    @classmethod\n    def from_bytes(cls, file_bytes: bytes, name: Optional[str] = None):\n        if name is None:\n            name = f\"PDF-{datetime.now().isoformat()}.pdf\"\n\n        return cls(name=name, file_bytes=file_bytes)\n\n    def get_bytes(self) -&gt; bytes:\n        return self.file_bytes  # Deprecated\n\n    @property\n    def path(self):\n        return self.file_path\n\n    def get_page_render_size(\n        self, page_number: int, dpi: int = DEFAULT_DPI\n    ) -&gt; Tuple[int, int]:\n        \"\"\"\n        Returns the render size of a page in pixels\n        \"\"\"\n        return get_page_render_size_from_bytes(self.get_bytes(), page_number, dpi=dpi)\n\n    def to_compressed_bytes(self, compression_kwargs: dict = {}) -&gt; bytes:\n        \"\"\"\n        Compresses the document using Ghostscript\n        \"\"\"\n        with self.as_tempfile() as temp_path:\n            return compress_pdf_to_bytes(temp_path, **compression_kwargs)\n\n    def rasterize_page(\n        self,\n        page_number: int,\n        *,\n        dpi: int = DEFAULT_DPI,\n        downscale_size: Optional[Tuple[int, int]] = None,\n        resize_mode: ResizeModes = \"thumbnail\",\n        max_file_size_bytes: Optional[int] = None,\n        resize_aspect_ratios: Optional[Iterable[AspectRatioRule]] = None,\n        do_convert: bool = False,\n        image_convert_mode: str = \"L\",\n        do_quantize: bool = False,\n        quantize_color_count: int = 8,\n        return_mode: Literal[\"pil\", \"bytes\"] = \"bytes\",\n    ):\n        \"\"\"\n        Rasterizes a page of the document using Pdfium\n        \"\"\"\n        if page_number &lt;= 0 or page_number &gt; self.num_pages:\n            raise ValueError(f\"Page number must be between 0 and {self.num_pages}\")\n\n        post_process_fn = None\n\n        if any(\n            (\n                downscale_size,\n                max_file_size_bytes,\n                resize_aspect_ratios,\n                do_convert,\n                do_quantize,\n            )\n        ):\n            post_process_fn = partial(\n                process_raster_image,\n                resize_width=downscale_size[0] if downscale_size else None,\n                resize_height=downscale_size[1] if downscale_size else None,\n                resize_mode=resize_mode,\n                resize_aspect_ratios=resize_aspect_ratios,\n                do_convert=do_convert,\n                image_convert_mode=image_convert_mode,\n                do_quantize=do_quantize,\n                quantize_color_count=quantize_color_count,\n                max_file_size_bytes=max_file_size_bytes,\n            )\n\n        rastered = rasterize_page_with_pdfium(\n            self.file_bytes,\n            page_number,\n            return_mode=return_mode,\n            post_process_fn=post_process_fn,\n            scale=(1 / 72) * dpi,\n        )\n\n        return rastered\n\n    def rasterize_page_to_data_uri(\n        self,\n        page_number: int,\n        *,\n        dpi: int = DEFAULT_DPI,\n        downscale_size: Optional[Tuple[int, int]] = None,\n        resize_mode: ResizeModes = \"thumbnail\",\n        max_file_size_bytes: Optional[int] = None,\n        resize_aspect_ratios: Optional[Iterable[AspectRatioRule]] = None,\n        do_convert: bool = False,\n        image_convert_mode: str = \"L\",\n        do_quantize: bool = False,\n        quantize_color_count: int = 8,\n        render_grayscale: bool = False,\n    ) -&gt; str:\n        \"\"\"\n        Rasterizes a page of the document using Pdfium and returns a data URI, which can\n        be embedded into HTML or passed to large language models\n        \"\"\"\n        image_bytes = self.rasterize_page(\n            page_number,\n            dpi=dpi,\n            downscale_size=downscale_size,\n            do_convert=do_convert,\n            image_convert_mode=image_convert_mode,\n            do_quantize=do_quantize,\n            quantize_color_count=quantize_color_count,\n            resize_mode=resize_mode,\n            max_file_size_bytes=max_file_size_bytes,\n            resize_aspect_ratios=resize_aspect_ratios,\n            return_mode=\"bytes\",\n        )\n        return f\"data:image/png;base64,{base64.b64encode(image_bytes).decode('utf-8')}\"\n\n    def rasterize_pdf(\n        self,\n        dpi: int = DEFAULT_DPI,\n        downscale_size: Optional[Tuple[int, int]] = None,\n        resize_mode: ResizeModes = \"thumbnail\",\n        max_file_size_bytes: Optional[int] = None,\n        resize_aspect_ratios: Optional[Iterable[AspectRatioRule]] = None,\n        do_convert: bool = False,\n        image_convert_mode: str = \"L\",\n        do_quantize: bool = False,\n        quantize_color_count: int = 8,\n        return_mode: Literal[\"pil\", \"bytes\"] = \"bytes\",\n        render_grayscale: bool = False,\n    ) -&gt; Dict[int, bytes]:\n        \"\"\"\n        Rasterizes the entire document using Pdfium\n        \"\"\"\n        result = {}\n\n        post_process_fn = None\n\n        if any(\n            (\n                downscale_size,\n                max_file_size_bytes,\n                resize_aspect_ratios,\n                do_convert,\n                do_quantize,\n            )\n        ):\n            post_process_fn = partial(\n                process_raster_image,\n                resize_width=downscale_size[0] if downscale_size else None,\n                resize_height=downscale_size[1] if downscale_size else None,\n                resize_mode=resize_mode,\n                resize_aspect_ratios=resize_aspect_ratios,\n                do_convert=do_convert,\n                image_convert_mode=image_convert_mode,\n                do_quantize=do_quantize,\n                quantize_color_count=quantize_color_count,\n                max_file_size_bytes=max_file_size_bytes,\n            )\n\n        for idx, rastered in enumerate(\n            rasterize_pdf_with_pdfium(\n                self.file_bytes,\n                scale=(1 / 72) * dpi,\n                grayscale=render_grayscale,\n                return_mode=return_mode,\n                post_process_fn=post_process_fn,\n            )\n        ):\n            result[idx + 1] = rastered\n\n        return result\n\n    def split(self, start: Optional[int] = None, stop: Optional[int] = None):\n        \"\"\"\n        Splits a document into multiple documents\n        \"\"\"\n        if start is None and stop is None:\n            raise ValueError(\"Must specify either start or stop\")\n\n        start = start or 0\n\n        from docprompt.utils.splitter import split_pdf_to_bytes\n\n        split_bytes = split_pdf_to_bytes(\n            self.file_bytes, start_page=start, stop_page=stop\n        )\n\n        return Document.from_bytes(split_bytes, name=self.name)\n\n    def as_tempfile(self, **kwargs):\n        \"\"\"\n        Returns a tempfile of the document\n        \"\"\"\n\n        @contextmanager\n        def tempfile_context() -&gt; Generator[str, None, None]:\n            tempfile_kwargs = {\"mode\": \"wb\", \"delete\": True, \"suffix\": \".pdf\", **kwargs}\n\n            with tempfile.NamedTemporaryFile(**tempfile_kwargs) as f:\n                f.write(self.file_bytes)\n                f.flush()\n                yield f.name\n\n        return tempfile_context()\n\n    def write_to_path(self, path: Union[PathLike, str], **kwargs):\n        \"\"\"\n        Writes the document to a path\n        \"\"\"\n        path = Path(path)\n\n        if path.is_dir():\n            path = path / self.name\n\n        with path.open(\"wb\") as f:\n            f.write(self.file_bytes)\n</code></pre>"},{"location":"reference/schema/document/#docprompt.schema.document.PdfDocument.as_tempfile","title":"<code>as_tempfile(**kwargs)</code>","text":"<p>Returns a tempfile of the document</p> Source code in <code>docprompt/schema/document.py</code> <pre><code>def as_tempfile(self, **kwargs):\n    \"\"\"\n    Returns a tempfile of the document\n    \"\"\"\n\n    @contextmanager\n    def tempfile_context() -&gt; Generator[str, None, None]:\n        tempfile_kwargs = {\"mode\": \"wb\", \"delete\": True, \"suffix\": \".pdf\", **kwargs}\n\n        with tempfile.NamedTemporaryFile(**tempfile_kwargs) as f:\n            f.write(self.file_bytes)\n            f.flush()\n            yield f.name\n\n    return tempfile_context()\n</code></pre>"},{"location":"reference/schema/document/#docprompt.schema.document.PdfDocument.get_page_render_size","title":"<code>get_page_render_size(page_number, dpi=DEFAULT_DPI)</code>","text":"<p>Returns the render size of a page in pixels</p> Source code in <code>docprompt/schema/document.py</code> <pre><code>def get_page_render_size(\n    self, page_number: int, dpi: int = DEFAULT_DPI\n) -&gt; Tuple[int, int]:\n    \"\"\"\n    Returns the render size of a page in pixels\n    \"\"\"\n    return get_page_render_size_from_bytes(self.get_bytes(), page_number, dpi=dpi)\n</code></pre>"},{"location":"reference/schema/document/#docprompt.schema.document.PdfDocument.rasterize_page","title":"<code>rasterize_page(page_number, *, dpi=DEFAULT_DPI, downscale_size=None, resize_mode='thumbnail', max_file_size_bytes=None, resize_aspect_ratios=None, do_convert=False, image_convert_mode='L', do_quantize=False, quantize_color_count=8, return_mode='bytes')</code>","text":"<p>Rasterizes a page of the document using Pdfium</p> Source code in <code>docprompt/schema/document.py</code> <pre><code>def rasterize_page(\n    self,\n    page_number: int,\n    *,\n    dpi: int = DEFAULT_DPI,\n    downscale_size: Optional[Tuple[int, int]] = None,\n    resize_mode: ResizeModes = \"thumbnail\",\n    max_file_size_bytes: Optional[int] = None,\n    resize_aspect_ratios: Optional[Iterable[AspectRatioRule]] = None,\n    do_convert: bool = False,\n    image_convert_mode: str = \"L\",\n    do_quantize: bool = False,\n    quantize_color_count: int = 8,\n    return_mode: Literal[\"pil\", \"bytes\"] = \"bytes\",\n):\n    \"\"\"\n    Rasterizes a page of the document using Pdfium\n    \"\"\"\n    if page_number &lt;= 0 or page_number &gt; self.num_pages:\n        raise ValueError(f\"Page number must be between 0 and {self.num_pages}\")\n\n    post_process_fn = None\n\n    if any(\n        (\n            downscale_size,\n            max_file_size_bytes,\n            resize_aspect_ratios,\n            do_convert,\n            do_quantize,\n        )\n    ):\n        post_process_fn = partial(\n            process_raster_image,\n            resize_width=downscale_size[0] if downscale_size else None,\n            resize_height=downscale_size[1] if downscale_size else None,\n            resize_mode=resize_mode,\n            resize_aspect_ratios=resize_aspect_ratios,\n            do_convert=do_convert,\n            image_convert_mode=image_convert_mode,\n            do_quantize=do_quantize,\n            quantize_color_count=quantize_color_count,\n            max_file_size_bytes=max_file_size_bytes,\n        )\n\n    rastered = rasterize_page_with_pdfium(\n        self.file_bytes,\n        page_number,\n        return_mode=return_mode,\n        post_process_fn=post_process_fn,\n        scale=(1 / 72) * dpi,\n    )\n\n    return rastered\n</code></pre>"},{"location":"reference/schema/document/#docprompt.schema.document.PdfDocument.rasterize_page_to_data_uri","title":"<code>rasterize_page_to_data_uri(page_number, *, dpi=DEFAULT_DPI, downscale_size=None, resize_mode='thumbnail', max_file_size_bytes=None, resize_aspect_ratios=None, do_convert=False, image_convert_mode='L', do_quantize=False, quantize_color_count=8, render_grayscale=False)</code>","text":"<p>Rasterizes a page of the document using Pdfium and returns a data URI, which can be embedded into HTML or passed to large language models</p> Source code in <code>docprompt/schema/document.py</code> <pre><code>def rasterize_page_to_data_uri(\n    self,\n    page_number: int,\n    *,\n    dpi: int = DEFAULT_DPI,\n    downscale_size: Optional[Tuple[int, int]] = None,\n    resize_mode: ResizeModes = \"thumbnail\",\n    max_file_size_bytes: Optional[int] = None,\n    resize_aspect_ratios: Optional[Iterable[AspectRatioRule]] = None,\n    do_convert: bool = False,\n    image_convert_mode: str = \"L\",\n    do_quantize: bool = False,\n    quantize_color_count: int = 8,\n    render_grayscale: bool = False,\n) -&gt; str:\n    \"\"\"\n    Rasterizes a page of the document using Pdfium and returns a data URI, which can\n    be embedded into HTML or passed to large language models\n    \"\"\"\n    image_bytes = self.rasterize_page(\n        page_number,\n        dpi=dpi,\n        downscale_size=downscale_size,\n        do_convert=do_convert,\n        image_convert_mode=image_convert_mode,\n        do_quantize=do_quantize,\n        quantize_color_count=quantize_color_count,\n        resize_mode=resize_mode,\n        max_file_size_bytes=max_file_size_bytes,\n        resize_aspect_ratios=resize_aspect_ratios,\n        return_mode=\"bytes\",\n    )\n    return f\"data:image/png;base64,{base64.b64encode(image_bytes).decode('utf-8')}\"\n</code></pre>"},{"location":"reference/schema/document/#docprompt.schema.document.PdfDocument.rasterize_pdf","title":"<code>rasterize_pdf(dpi=DEFAULT_DPI, downscale_size=None, resize_mode='thumbnail', max_file_size_bytes=None, resize_aspect_ratios=None, do_convert=False, image_convert_mode='L', do_quantize=False, quantize_color_count=8, return_mode='bytes', render_grayscale=False)</code>","text":"<p>Rasterizes the entire document using Pdfium</p> Source code in <code>docprompt/schema/document.py</code> <pre><code>def rasterize_pdf(\n    self,\n    dpi: int = DEFAULT_DPI,\n    downscale_size: Optional[Tuple[int, int]] = None,\n    resize_mode: ResizeModes = \"thumbnail\",\n    max_file_size_bytes: Optional[int] = None,\n    resize_aspect_ratios: Optional[Iterable[AspectRatioRule]] = None,\n    do_convert: bool = False,\n    image_convert_mode: str = \"L\",\n    do_quantize: bool = False,\n    quantize_color_count: int = 8,\n    return_mode: Literal[\"pil\", \"bytes\"] = \"bytes\",\n    render_grayscale: bool = False,\n) -&gt; Dict[int, bytes]:\n    \"\"\"\n    Rasterizes the entire document using Pdfium\n    \"\"\"\n    result = {}\n\n    post_process_fn = None\n\n    if any(\n        (\n            downscale_size,\n            max_file_size_bytes,\n            resize_aspect_ratios,\n            do_convert,\n            do_quantize,\n        )\n    ):\n        post_process_fn = partial(\n            process_raster_image,\n            resize_width=downscale_size[0] if downscale_size else None,\n            resize_height=downscale_size[1] if downscale_size else None,\n            resize_mode=resize_mode,\n            resize_aspect_ratios=resize_aspect_ratios,\n            do_convert=do_convert,\n            image_convert_mode=image_convert_mode,\n            do_quantize=do_quantize,\n            quantize_color_count=quantize_color_count,\n            max_file_size_bytes=max_file_size_bytes,\n        )\n\n    for idx, rastered in enumerate(\n        rasterize_pdf_with_pdfium(\n            self.file_bytes,\n            scale=(1 / 72) * dpi,\n            grayscale=render_grayscale,\n            return_mode=return_mode,\n            post_process_fn=post_process_fn,\n        )\n    ):\n        result[idx + 1] = rastered\n\n    return result\n</code></pre>"},{"location":"reference/schema/document/#docprompt.schema.document.PdfDocument.split","title":"<code>split(start=None, stop=None)</code>","text":"<p>Splits a document into multiple documents</p> Source code in <code>docprompt/schema/document.py</code> <pre><code>def split(self, start: Optional[int] = None, stop: Optional[int] = None):\n    \"\"\"\n    Splits a document into multiple documents\n    \"\"\"\n    if start is None and stop is None:\n        raise ValueError(\"Must specify either start or stop\")\n\n    start = start or 0\n\n    from docprompt.utils.splitter import split_pdf_to_bytes\n\n    split_bytes = split_pdf_to_bytes(\n        self.file_bytes, start_page=start, stop_page=stop\n    )\n\n    return Document.from_bytes(split_bytes, name=self.name)\n</code></pre>"},{"location":"reference/schema/document/#docprompt.schema.document.PdfDocument.to_compressed_bytes","title":"<code>to_compressed_bytes(compression_kwargs={})</code>","text":"<p>Compresses the document using Ghostscript</p> Source code in <code>docprompt/schema/document.py</code> <pre><code>def to_compressed_bytes(self, compression_kwargs: dict = {}) -&gt; bytes:\n    \"\"\"\n    Compresses the document using Ghostscript\n    \"\"\"\n    with self.as_tempfile() as temp_path:\n        return compress_pdf_to_bytes(temp_path, **compression_kwargs)\n</code></pre>"},{"location":"reference/schema/document/#docprompt.schema.document.PdfDocument.write_to_path","title":"<code>write_to_path(path, **kwargs)</code>","text":"<p>Writes the document to a path</p> Source code in <code>docprompt/schema/document.py</code> <pre><code>def write_to_path(self, path: Union[PathLike, str], **kwargs):\n    \"\"\"\n    Writes the document to a path\n    \"\"\"\n    path = Path(path)\n\n    if path.is_dir():\n        path = path / self.name\n\n    with path.open(\"wb\") as f:\n        f.write(self.file_bytes)\n</code></pre>"},{"location":"reference/schema/document/#docprompt.schema.document.get_page_render_size_from_bytes","title":"<code>get_page_render_size_from_bytes(file_bytes, page_number, dpi=DEFAULT_DPI)</code>","text":"<p>Returns the render size of a page in pixels</p> Source code in <code>docprompt/schema/document.py</code> <pre><code>def get_page_render_size_from_bytes(\n    file_bytes: bytes, page_number: int, dpi: int = DEFAULT_DPI\n):\n    \"\"\"\n    Returns the render size of a page in pixels\n    \"\"\"\n\n    with get_pdfium_document(file_bytes) as pdf:\n        page = pdf.get_page(page_number)\n\n        mediabox = page.get_mediabox()\n\n        base_width = int(mediabox[2] - mediabox[0])\n        base_height = int(mediabox[3] - mediabox[1])\n\n        width = int(base_width * dpi / 72)\n        height = int(base_height * dpi / 72)\n\n        return width, height\n</code></pre>"},{"location":"reference/schema/layout/","title":"layout","text":""},{"location":"reference/schema/layout/#docprompt.schema.layout.BoundingPoly","title":"<code>BoundingPoly</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a normalized bounding poly with each value in the range [0, 1]</p> <p>Used for higher order shapes like polygons on a page</p> Source code in <code>docprompt/schema/layout.py</code> <pre><code>class BoundingPoly(BaseModel):\n    \"\"\"\n    Represents a normalized bounding poly with each value in the range [0, 1]\n\n    Used for higher order shapes like polygons on a page\n    \"\"\"\n\n    normalized_vertices: List[Point]\n\n    def __getitem__(self, index):\n        return self.normalized_vertices[index]\n</code></pre>"},{"location":"reference/schema/layout/#docprompt.schema.layout.NormBBox","title":"<code>NormBBox</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a normalized bounding box with each value in the range [0, 1]</p> <p>Where x1 &gt; x0 and bottom &gt; top</p> Source code in <code>docprompt/schema/layout.py</code> <pre><code>class NormBBox(BaseModel):\n    \"\"\"\n    Represents a normalized bounding box with each value in the range [0, 1]\n\n    Where x1 &gt; x0 and bottom &gt; top\n    \"\"\"\n\n    x0: BoundedFloat\n    top: BoundedFloat\n    x1: BoundedFloat\n    bottom: BoundedFloat\n\n    model_config: ConfigDict = {\"json_encoders\": {float: lambda v: round(v, 5)}}\n\n    def as_tuple(self):\n        return (self.x0, self.top, self.x1, self.bottom)\n\n    def __getitem__(self, index):\n        # Lots of if statements to prevent new allocations\n        if index &gt; 3:\n            raise IndexError(\"Index out of range\")\n\n        if index == 0:\n            return self.x0\n        elif index == 1:\n            return self.top\n        elif index == 2:\n            return self.x1\n        elif index == 3:\n            return self.bottom\n\n    def __eq__(self, other):\n        if not isinstance(other, NormBBox):\n            return False\n\n        return self.as_tuple() == other.as_tuple()\n\n    def __hash__(self):\n        return hash(self.as_tuple())\n\n    def __and__(self, other):\n        if not isinstance(other, NormBBox):\n            raise TypeError(\"Can only compute intersection with NormBBox\")\n        # Compute the intersection of two bounding boxes\n        new_x0 = max(self.x0, other.x0)\n        new_top = max(self.top, other.top)\n        new_x1 = min(self.x1, other.x1)\n        new_bottom = min(self.bottom, other.bottom)\n\n        # Check if there is an actual intersection and if the resulting bounding box is valid\n        if new_x0 &lt;= new_x1 and new_top &lt;= new_bottom:\n            return NormBBox(x0=new_x0, top=new_top, x1=new_x1, bottom=new_bottom)\n        else:\n            # Return an empty or non-existent bounding box representation\n            return None\n\n    def __add__(self, other):\n        if not isinstance(other, NormBBox):\n            raise TypeError(\"Can only add NormBBox to NormBBox\")\n\n        return NormBBox(\n            x0=min(self.x0, other.x0),\n            top=min(self.top, other.top),\n            x1=max(self.x1, other.x1),\n            bottom=max(self.bottom, other.bottom),\n        )\n\n    def __contains__(self, other):\n        return (\n            self.x0 &lt;= other.x0\n            and self.top &lt;= other.top\n            and self.x1 &gt;= other.x1\n            and self.bottom &gt;= other.bottom\n        )\n\n    def intersection_over_union(self, other):\n        if not isinstance(other, NormBBox):\n            raise TypeError(\"Can only compute IOU with NormBBox\")\n\n        # Compute the intersection\n        intersection_bbox = self &amp; other\n\n        if intersection_bbox:\n            intersection_area = intersection_bbox.area\n            union_area = self.area + other.area - intersection_area\n            return intersection_area / union_area\n\n        return 0  # No intersection\n\n    def x_overlap(self, other):\n        \"\"\"\n        Get the overlap, between 0 and 1, of the x-axis of two bounding boxes\n        \"\"\"\n        return max(0, min(self.x1, other.x1) - max(self.x0, other.x0))\n\n    def y_overlap(self, other):\n        \"\"\"\n        Get the overlap, between 0 and 1, of the y-axis of two bounding boxes\n        \"\"\"\n        return max(0, min(self.bottom, other.bottom) - max(self.top, other.top))\n\n    @classmethod\n    def combine(cls, *bboxes: \"NormBBox\"):\n        \"\"\"\n        Combines multiple bounding boxes into a single bounding box\n        \"\"\"\n        if len(bboxes) == 0:\n            raise ValueError(\"Must provide at least one bounding box\")\n\n        if len(bboxes) == 1:\n            return bboxes[0]\n\n        working_bbox = bboxes[0]\n        for bbox in bboxes[1:]:\n            working_bbox = working_bbox + bbox\n\n        return working_bbox\n\n    @classmethod\n    def from_bounding_poly(cls, bounding_poly: \"BoundingPoly\"):\n        \"\"\"\n        Returns a NormBBox from a BoundingPoly\n        \"\"\"\n        if len(bounding_poly.normalized_vertices) != 4:\n            raise ValueError(\n                \"BoundingPoly must have 4 vertices for NormBBox conversion\"\n            )\n\n        (\n            top_left,\n            top_right,\n            bottom_right,\n            bottom_left,\n        ) = bounding_poly.normalized_vertices\n\n        return cls(\n            x0=top_left.x,\n            top=top_left.y,\n            x1=bottom_right.x,\n            bottom=bottom_right.y,\n        )\n\n    @property\n    def width(self):\n        return self.x1 - self.x0\n\n    @property\n    def height(self):\n        return self.bottom - self.top\n\n    @property\n    def area(self):\n        return self.width * self.height\n\n    @property\n    def centroid(self):\n        return (self.x0 + self.x1) / 2, (self.top + self.bottom) / 2\n\n    @property\n    def y_center(self):\n        return (self.top + self.bottom) / 2\n\n    @property\n    def x_center(self):\n        return (self.x0 + self.x1) / 2\n</code></pre>"},{"location":"reference/schema/layout/#docprompt.schema.layout.NormBBox.combine","title":"<code>combine(*bboxes)</code>  <code>classmethod</code>","text":"<p>Combines multiple bounding boxes into a single bounding box</p> Source code in <code>docprompt/schema/layout.py</code> <pre><code>@classmethod\ndef combine(cls, *bboxes: \"NormBBox\"):\n    \"\"\"\n    Combines multiple bounding boxes into a single bounding box\n    \"\"\"\n    if len(bboxes) == 0:\n        raise ValueError(\"Must provide at least one bounding box\")\n\n    if len(bboxes) == 1:\n        return bboxes[0]\n\n    working_bbox = bboxes[0]\n    for bbox in bboxes[1:]:\n        working_bbox = working_bbox + bbox\n\n    return working_bbox\n</code></pre>"},{"location":"reference/schema/layout/#docprompt.schema.layout.NormBBox.from_bounding_poly","title":"<code>from_bounding_poly(bounding_poly)</code>  <code>classmethod</code>","text":"<p>Returns a NormBBox from a BoundingPoly</p> Source code in <code>docprompt/schema/layout.py</code> <pre><code>@classmethod\ndef from_bounding_poly(cls, bounding_poly: \"BoundingPoly\"):\n    \"\"\"\n    Returns a NormBBox from a BoundingPoly\n    \"\"\"\n    if len(bounding_poly.normalized_vertices) != 4:\n        raise ValueError(\n            \"BoundingPoly must have 4 vertices for NormBBox conversion\"\n        )\n\n    (\n        top_left,\n        top_right,\n        bottom_right,\n        bottom_left,\n    ) = bounding_poly.normalized_vertices\n\n    return cls(\n        x0=top_left.x,\n        top=top_left.y,\n        x1=bottom_right.x,\n        bottom=bottom_right.y,\n    )\n</code></pre>"},{"location":"reference/schema/layout/#docprompt.schema.layout.NormBBox.x_overlap","title":"<code>x_overlap(other)</code>","text":"<p>Get the overlap, between 0 and 1, of the x-axis of two bounding boxes</p> Source code in <code>docprompt/schema/layout.py</code> <pre><code>def x_overlap(self, other):\n    \"\"\"\n    Get the overlap, between 0 and 1, of the x-axis of two bounding boxes\n    \"\"\"\n    return max(0, min(self.x1, other.x1) - max(self.x0, other.x0))\n</code></pre>"},{"location":"reference/schema/layout/#docprompt.schema.layout.NormBBox.y_overlap","title":"<code>y_overlap(other)</code>","text":"<p>Get the overlap, between 0 and 1, of the y-axis of two bounding boxes</p> Source code in <code>docprompt/schema/layout.py</code> <pre><code>def y_overlap(self, other):\n    \"\"\"\n    Get the overlap, between 0 and 1, of the y-axis of two bounding boxes\n    \"\"\"\n    return max(0, min(self.bottom, other.bottom) - max(self.top, other.top))\n</code></pre>"},{"location":"reference/schema/layout/#docprompt.schema.layout.Point","title":"<code>Point</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a normalized bounding box with each value in the range [0, 1]</p> Source code in <code>docprompt/schema/layout.py</code> <pre><code>class Point(BaseModel):\n    \"\"\"\n    Represents a normalized bounding box with each value in the range [0, 1]\n    \"\"\"\n\n    model_config: ConfigDict = {\"json_encoders\": {float: lambda v: round(v, 5)}}\n\n    x: BoundedFloat\n    y: BoundedFloat\n</code></pre>"},{"location":"reference/schema/layout/#docprompt.schema.layout.TextBlock","title":"<code>TextBlock</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a single block of text, with its bounding box. The bounding box is a tuple of (x0, top, x1, bottom) and is normalized to the page size.</p> Source code in <code>docprompt/schema/layout.py</code> <pre><code>class TextBlock(BaseModel):\n    \"\"\"\n    Represents a single block of text, with its bounding box.\n    The bounding box is a tuple of (x0, top, x1, bottom) and\n    is normalized to the page size.\n    \"\"\"\n\n    model_config: ConfigDict = {\"json_encoders\": {float: lambda v: round(v, 5)}}\n\n    text: str\n    type: SegmentLevels\n    source: TextblockSource = Field(\n        default=\"derived\", description=\"The source of the text block\"\n    )\n\n    # Layout information\n    bounding_box: NormBBox = Field(default=None, repr=False)\n    bounding_poly: Optional[BoundingPoly] = Field(default=None, repr=False)\n    text_spans: Optional[List[TextSpan]] = Field(default=None, repr=False)\n\n    metadata: Optional[TextBlockMetadata] = Field(default_factory=TextBlockMetadata)\n\n    def __getitem__(self, index):\n        return getattr(self, index)\n\n    def __hash__(self):\n        return hash((self.text, self.bounding_box.as_tuple()))\n\n    @property\n    def confidence(self):\n        return self.metadata.confidence\n\n    @property\n    def direction(self):\n        return self.metadata.direction\n</code></pre>"},{"location":"reference/schema/pipeline/","title":"Index","text":""},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.BaseMetadata","title":"<code>BaseMetadata</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>MutableMapping</code>, <code>Generic[TMetadataOwner]</code></p> <p>The base metadata class is utilized for defining a basic yet flexible interface for metadata attached to various fields.</p> The metadata class can be used in two ways <ol> <li>As a dictionary-like object, where metadata is stored in the <code>extra</code> field.</li> <li>As a sub-classed model, where metadata is stored in the fields of the model.</li> </ol> <p>When used out of the box, the metadata class will adobpt dictionary-like behavior. You may easily access different fields of the metadata as if it were a dictionary: <pre><code># Instantiate it with any kwargs you like\nmetadata = BaseMetadata(foo-'bar', cow='moo')\n\nmetadata[\"foo\"]  # \"bar\"\nmetadata[\"cow\"]  # \"moo\"\n\n# Update the value of the key\nmetadata[\"foo\"] = \"fighters\"\n\n# Set new key-value pairs\nmetadata['sheep'] = 'baa'\n</code></pre></p> <p>Otherwise, you may sub-class the metadata class in order to create a more strictly typed metadata model. This is useful when you want to enforce a specific structure for your metadata.</p> <pre><code>class CustomMetadata(BaseMetadata):\n    foo: str\n    cow: str\n\n# Instantiate it with the required fields\nmetadata = CustomMetadata(foo='bar', cow='moo')\n\nmetadata.foo  # \"bar\"\nmetadata.cow  # \"moo\"\n\n# Update the value of the key\nmetadata.foo = \"fighters\"\n\n# Use the extra field to store dynamic metadata\nmetadata.extra['sheep'] = 'baa'\n</code></pre> <p>Additionally, the task results descriptor allows for controlled and easy access to the task results of various tasks that are run on the parent node.</p> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>class BaseMetadata(BaseModel, MutableMapping, Generic[TMetadataOwner]):\n    \"\"\"\n    The base metadata class is utilized for defining a basic yet flexible interface\n    for metadata attached to various fields.\n\n    The metadata class can be used in two ways:\n        1. As a dictionary-like object, where metadata is stored in the `extra` field.\n        2. As a sub-classed model, where metadata is stored in the fields of the model.\n\n    When used out of the box, the metadata class will adobpt dictionary-like behavior. You\n    may easily access different fields of the metadata as if it were a dictionary:\n    ```python\n    # Instantiate it with any kwargs you like\n    metadata = BaseMetadata(foo-'bar', cow='moo')\n\n    metadata[\"foo\"]  # \"bar\"\n    metadata[\"cow\"]  # \"moo\"\n\n    # Update the value of the key\n    metadata[\"foo\"] = \"fighters\"\n\n    # Set new key-value pairs\n    metadata['sheep'] = 'baa'\n    ```\n\n    Otherwise, you may sub-class the metadata class in order to create a more strictly typed\n    metadata model. This is useful when you want to enforce a specific structure for your metadata.\n\n    ```python\n    class CustomMetadata(BaseMetadata):\n        foo: str\n        cow: str\n\n    # Instantiate it with the required fields\n    metadata = CustomMetadata(foo='bar', cow='moo')\n\n    metadata.foo  # \"bar\"\n    metadata.cow  # \"moo\"\n\n    # Update the value of the key\n    metadata.foo = \"fighters\"\n\n    # Use the extra field to store dynamic metadata\n    metadata.extra['sheep'] = 'baa'\n    ```\n\n    Additionally, the task results descriptor allows for controlled and easy access to the task results\n    of various tasks that are run on the parent node.\n    \"\"\"\n\n    extra: Dict[str, Any] = Field(..., default_factory=dict, repr=False)\n\n    _task_results: TaskResultsDescriptor = PrivateAttr(\n        default_factory=TaskResultsDescriptor\n    )\n\n    _owner: TMetadataOwner = PrivateAttr()\n\n    @property\n    def task_results(self) -&gt; TaskResultsDescriptor:\n        \"\"\"Return the task results descriptor.\"\"\"\n        return self._task_results.__get__(self)\n\n    @task_results.setter\n    def task_results(self, value: Any) -&gt; None:\n        \"\"\"This will raise an error, as we do not want to set the task results directly.\n\n        NOTE: This implementation is here purely to avoid the task_results property from being\n        overwritten by accident.\n        \"\"\"\n        self._task_results.__set__(self, value)\n\n    @property\n    def owner(self) -&gt; TMetadataOwner:\n        \"\"\"Return the owner of the metadata.\"\"\"\n        return self._owner\n\n    @owner.setter\n    def owner(self, owner: TMetadataOwner) -&gt; None:\n        \"\"\"Return the owner of the metadata.\"\"\"\n        self._owner = owner\n\n    @classmethod\n    def from_owner(cls, owner: TMetadataOwner, **data) -&gt; BaseMetadata:\n        \"\"\"Create a new instance of the metadata class with the owner set.\"\"\"\n        metadata = cls(**data)\n        metadata.owner = owner\n        return metadata\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def validate_data_fields_from_annotations(cls, data: Any) -&gt; Any:\n        \"\"\"Validate the data fields from the annotations.\"\"\"\n\n        # We want to make sure that we combine the `extra` metdata along with any\n        # other specific fields that are defined in the metadata.\n        extra = data.pop(\"extra\", {})\n        assert isinstance(extra, dict), \"The `extra` field must be a dictionary.\"\n        data = {**data, **extra}\n\n        # If the model has been sub-classed, then all of our fields must be\n        # validated by the pydantic model.\n        if cls._is_field_typed():\n            # We will get the fields out of extra and set them as potential fields to\n            # validate. They will be ignored if they are not defined in the model, but it\n            # allows for a more flexible way to define metadata.\n            # Otherwise, what ever is in the `extra` field will be stroed in the `extra` field.\n            return {**data, \"extra\": extra}\n\n        # Otherwise, we are using our mock-dict implentation, so we store our\n        # metadata in the `extra` field.\n        return {\"extra\": data}\n\n    @classmethod\n    def _is_field_typed(cls):\n        \"\"\"\n        Check if the metadata model is field typed.\n\n        This is used to determine if the metadata model is a dictionary-like model,\n        or a more strictly typed model.\n        \"\"\"\n        if set([\"extra\"]) != set(cls.model_fields.keys()):\n            return True\n\n        return False\n\n    def __repr__(self):\n        \"\"\"\n        Provide a string representation of the metadata.\n\n        This only works for the base metadata model. If sub-classed, this will raise an error,\n        unless overridden, as BaseModel's do not have a __repr__ method.\n        \"\"\"\n        if self._is_field_typed():\n            return super().__repr__()\n\n        # Otherwise, we are deailing with dictornary-like metadata\n        return json.dumps(self.extra)\n\n    def __getitem__(self, name):\n        \"\"\"\n        Provide dictionary functionlaity to the metadata class.\n\n        This only works for the base metadata model. If sub-classed, this will raise an error,\n        unless overridden, as BaseModel's do not have an __getitem__ method.\n        \"\"\"\n        if not self._is_field_typed():\n            return self.extra[name]\n\n        raise AttributeError(\n            f\"'{self.__class__.__name__}' object has no attribute '{name}'\"\n        )\n\n    def __setitem__(self, name, value):\n        \"\"\"\n        Provide dictionary functionality to the metadata class.\n\n        This only works for the base metadata model. If sub-classed, this will raise an error,\n        unless overridden, as BaseModel's do not have an __setitem__ method.\n        \"\"\"\n        if not self._is_field_typed():\n            self.extra[name] = value\n        else:\n            raise AttributeError(\n                f\"'{self.__class__.__name__}' object has no attribute '{name}'\"\n            )\n\n    def __delitem__(self, name):\n        \"\"\"\n        Provide dictionary functionality to the metadata class.\n\n        This only works for the base metadata model. If sub-classed, this will raise an error,\n        unless overridden, as BaseModel's do not have an __delitem__ method.\n        \"\"\"\n        if not self._is_field_typed():\n            del self.extra[name]\n        else:\n            raise AttributeError(\n                f\"'{self.__class__.__name__}' object has no attribute '{name}'\"\n            )\n\n    def __iter__(self):\n        \"\"\"\n        Iterate over the keys in the metadata.\n\n        This only works for the base metadata model. If sub-classed, this will raise an error,\n        unless overridden, as BaseModel's do not have an __iter__ method.\n        \"\"\"\n        if self._is_field_typed():\n            raise AttributeError(f\"'{self.__class__.__name__}' object is not iterable\")\n\n        return iter(self.extra)\n\n    def __len__(self):\n        \"\"\"\n        Get the number of keys in the metadata.\n\n        This only works for the base metadata model. If sub-classed, this will raise an error,\n        unless overridden, as BaseModel's do not have a __len__ method.\n        \"\"\"\n        if self._is_field_typed():\n            raise AttributeError(\n                f\"'{self.__class__.__name__}' object has no attribute '__len__'\"\n            )\n\n        return len(self.extra)\n\n    def __getattr__(self, name):\n        \"\"\"\n        Allow for getting of attributes on the metadata class.\n\n        The attributes are retrieved through the following heirarchy:\n            - If the model is sub-classed, it will be retrieved as normal.\n            - Otherwise, if the attribute is private, it will be retrieved as normal.\n            - Finally, if we are getting a public attribute on the base metadata class,\n                we use the extra field.\n            - If the key is not set in the `extra` dictionary, we resort back to just\n            trying to get the field.\n                - This is when we grab the `owner` or `task_result` attribuite.\n        \"\"\"\n        if self._is_field_typed():\n            return super().__getattr__(name)\n\n        if name.startswith(\"_\"):\n            return super().__getattr__(name)\n\n        # Attempt to retreieve the attr from the `extra` field\n        try:\n            return self.extra.get(name)\n\n        except KeyError:\n            # This is for grabbing properties on the base metadata class\n            return super().__getattr__(name)\n\n    def __setattr__(self, name: str, value: Any) -&gt; None:\n        \"\"\"\n        Allow for setting of attributes on the metadata class.\n\n        The attributes are set through the following heirarchy:\n            - If the model is sub-classed, it will be set as normal.\n            - Otherwise, if the attribute is private, it will be set as normal.\n            - Finally, if we are setting a public attribute on the base metadata class,\n                we use the extra field.\n        \"\"\"\n        if self._is_field_typed():\n            return super().__setattr__(name, value)\n\n        # We want to avoid setting any private attributes in the extra\n        # dictionary\n        if name.startswith(\"_\"):\n            return super().__setattr__(name, value)\n\n        # If it is `owner` or `task_results`, we want\n        # to avoid setting the attribute in the `extra` dictionary\n        if name in [\"owner\", \"task_results\"]:\n            return super().__setattr__(name, value)\n\n        self.extra[name] = value\n\n    def __delattr__(self, name: str) -&gt; None:\n        \"\"\"\n        Ensure that we can delete attributes from the metadata class.\n\n        The attributes are deleted through the following heirarchy:\n            - If the attribute is `task_results`, we use the descriptor to delete the task results.\n            - Otherwise, if it is a sub-classed model, it will be deleted as normal.\n            - Finally, if we are deleting a public attribute on the base metadata class,\n                we use the extra field.\n        \"\"\"\n\n        # We want to use the descriptor to delete the task results\n        if name == \"task_results\":\n            self._task_results.__delete__(self)\n            return\n\n        # Otherwise, we use our standard fallback tiers\n        if self._is_field_typed():\n            return super().__delattr__(name)\n\n        del self.extra[name]\n</code></pre>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.BaseMetadata.owner","title":"<code>owner: TMetadataOwner</code>  <code>property</code> <code>writable</code>","text":"<p>Return the owner of the metadata.</p>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.BaseMetadata.task_results","title":"<code>task_results: TaskResultsDescriptor</code>  <code>property</code> <code>writable</code>","text":"<p>Return the task results descriptor.</p>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.BaseMetadata.__delattr__","title":"<code>__delattr__(name)</code>","text":"<p>Ensure that we can delete attributes from the metadata class.</p> The attributes are deleted through the following heirarchy <ul> <li>If the attribute is <code>task_results</code>, we use the descriptor to delete the task results.</li> <li>Otherwise, if it is a sub-classed model, it will be deleted as normal.</li> <li>Finally, if we are deleting a public attribute on the base metadata class,     we use the extra field.</li> </ul> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>def __delattr__(self, name: str) -&gt; None:\n    \"\"\"\n    Ensure that we can delete attributes from the metadata class.\n\n    The attributes are deleted through the following heirarchy:\n        - If the attribute is `task_results`, we use the descriptor to delete the task results.\n        - Otherwise, if it is a sub-classed model, it will be deleted as normal.\n        - Finally, if we are deleting a public attribute on the base metadata class,\n            we use the extra field.\n    \"\"\"\n\n    # We want to use the descriptor to delete the task results\n    if name == \"task_results\":\n        self._task_results.__delete__(self)\n        return\n\n    # Otherwise, we use our standard fallback tiers\n    if self._is_field_typed():\n        return super().__delattr__(name)\n\n    del self.extra[name]\n</code></pre>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.BaseMetadata.__delitem__","title":"<code>__delitem__(name)</code>","text":"<p>Provide dictionary functionality to the metadata class.</p> <p>This only works for the base metadata model. If sub-classed, this will raise an error, unless overridden, as BaseModel's do not have an delitem method.</p> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>def __delitem__(self, name):\n    \"\"\"\n    Provide dictionary functionality to the metadata class.\n\n    This only works for the base metadata model. If sub-classed, this will raise an error,\n    unless overridden, as BaseModel's do not have an __delitem__ method.\n    \"\"\"\n    if not self._is_field_typed():\n        del self.extra[name]\n    else:\n        raise AttributeError(\n            f\"'{self.__class__.__name__}' object has no attribute '{name}'\"\n        )\n</code></pre>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.BaseMetadata.__getattr__","title":"<code>__getattr__(name)</code>","text":"<p>Allow for getting of attributes on the metadata class.</p> The attributes are retrieved through the following heirarchy <ul> <li>If the model is sub-classed, it will be retrieved as normal.</li> <li>Otherwise, if the attribute is private, it will be retrieved as normal.</li> <li>Finally, if we are getting a public attribute on the base metadata class,     we use the extra field.</li> <li>If the key is not set in the <code>extra</code> dictionary, we resort back to just trying to get the field.<ul> <li>This is when we grab the <code>owner</code> or <code>task_result</code> attribuite.</li> </ul> </li> </ul> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>def __getattr__(self, name):\n    \"\"\"\n    Allow for getting of attributes on the metadata class.\n\n    The attributes are retrieved through the following heirarchy:\n        - If the model is sub-classed, it will be retrieved as normal.\n        - Otherwise, if the attribute is private, it will be retrieved as normal.\n        - Finally, if we are getting a public attribute on the base metadata class,\n            we use the extra field.\n        - If the key is not set in the `extra` dictionary, we resort back to just\n        trying to get the field.\n            - This is when we grab the `owner` or `task_result` attribuite.\n    \"\"\"\n    if self._is_field_typed():\n        return super().__getattr__(name)\n\n    if name.startswith(\"_\"):\n        return super().__getattr__(name)\n\n    # Attempt to retreieve the attr from the `extra` field\n    try:\n        return self.extra.get(name)\n\n    except KeyError:\n        # This is for grabbing properties on the base metadata class\n        return super().__getattr__(name)\n</code></pre>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.BaseMetadata.__getitem__","title":"<code>__getitem__(name)</code>","text":"<p>Provide dictionary functionlaity to the metadata class.</p> <p>This only works for the base metadata model. If sub-classed, this will raise an error, unless overridden, as BaseModel's do not have an getitem method.</p> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>def __getitem__(self, name):\n    \"\"\"\n    Provide dictionary functionlaity to the metadata class.\n\n    This only works for the base metadata model. If sub-classed, this will raise an error,\n    unless overridden, as BaseModel's do not have an __getitem__ method.\n    \"\"\"\n    if not self._is_field_typed():\n        return self.extra[name]\n\n    raise AttributeError(\n        f\"'{self.__class__.__name__}' object has no attribute '{name}'\"\n    )\n</code></pre>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.BaseMetadata.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over the keys in the metadata.</p> <p>This only works for the base metadata model. If sub-classed, this will raise an error, unless overridden, as BaseModel's do not have an iter method.</p> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>def __iter__(self):\n    \"\"\"\n    Iterate over the keys in the metadata.\n\n    This only works for the base metadata model. If sub-classed, this will raise an error,\n    unless overridden, as BaseModel's do not have an __iter__ method.\n    \"\"\"\n    if self._is_field_typed():\n        raise AttributeError(f\"'{self.__class__.__name__}' object is not iterable\")\n\n    return iter(self.extra)\n</code></pre>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.BaseMetadata.__len__","title":"<code>__len__()</code>","text":"<p>Get the number of keys in the metadata.</p> <p>This only works for the base metadata model. If sub-classed, this will raise an error, unless overridden, as BaseModel's do not have a len method.</p> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>def __len__(self):\n    \"\"\"\n    Get the number of keys in the metadata.\n\n    This only works for the base metadata model. If sub-classed, this will raise an error,\n    unless overridden, as BaseModel's do not have a __len__ method.\n    \"\"\"\n    if self._is_field_typed():\n        raise AttributeError(\n            f\"'{self.__class__.__name__}' object has no attribute '__len__'\"\n        )\n\n    return len(self.extra)\n</code></pre>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.BaseMetadata.__repr__","title":"<code>__repr__()</code>","text":"<p>Provide a string representation of the metadata.</p> <p>This only works for the base metadata model. If sub-classed, this will raise an error, unless overridden, as BaseModel's do not have a repr method.</p> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>def __repr__(self):\n    \"\"\"\n    Provide a string representation of the metadata.\n\n    This only works for the base metadata model. If sub-classed, this will raise an error,\n    unless overridden, as BaseModel's do not have a __repr__ method.\n    \"\"\"\n    if self._is_field_typed():\n        return super().__repr__()\n\n    # Otherwise, we are deailing with dictornary-like metadata\n    return json.dumps(self.extra)\n</code></pre>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.BaseMetadata.__setattr__","title":"<code>__setattr__(name, value)</code>","text":"<p>Allow for setting of attributes on the metadata class.</p> The attributes are set through the following heirarchy <ul> <li>If the model is sub-classed, it will be set as normal.</li> <li>Otherwise, if the attribute is private, it will be set as normal.</li> <li>Finally, if we are setting a public attribute on the base metadata class,     we use the extra field.</li> </ul> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>def __setattr__(self, name: str, value: Any) -&gt; None:\n    \"\"\"\n    Allow for setting of attributes on the metadata class.\n\n    The attributes are set through the following heirarchy:\n        - If the model is sub-classed, it will be set as normal.\n        - Otherwise, if the attribute is private, it will be set as normal.\n        - Finally, if we are setting a public attribute on the base metadata class,\n            we use the extra field.\n    \"\"\"\n    if self._is_field_typed():\n        return super().__setattr__(name, value)\n\n    # We want to avoid setting any private attributes in the extra\n    # dictionary\n    if name.startswith(\"_\"):\n        return super().__setattr__(name, value)\n\n    # If it is `owner` or `task_results`, we want\n    # to avoid setting the attribute in the `extra` dictionary\n    if name in [\"owner\", \"task_results\"]:\n        return super().__setattr__(name, value)\n\n    self.extra[name] = value\n</code></pre>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.BaseMetadata.__setitem__","title":"<code>__setitem__(name, value)</code>","text":"<p>Provide dictionary functionality to the metadata class.</p> <p>This only works for the base metadata model. If sub-classed, this will raise an error, unless overridden, as BaseModel's do not have an setitem method.</p> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>def __setitem__(self, name, value):\n    \"\"\"\n    Provide dictionary functionality to the metadata class.\n\n    This only works for the base metadata model. If sub-classed, this will raise an error,\n    unless overridden, as BaseModel's do not have an __setitem__ method.\n    \"\"\"\n    if not self._is_field_typed():\n        self.extra[name] = value\n    else:\n        raise AttributeError(\n            f\"'{self.__class__.__name__}' object has no attribute '{name}'\"\n        )\n</code></pre>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.BaseMetadata.from_owner","title":"<code>from_owner(owner, **data)</code>  <code>classmethod</code>","text":"<p>Create a new instance of the metadata class with the owner set.</p> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>@classmethod\ndef from_owner(cls, owner: TMetadataOwner, **data) -&gt; BaseMetadata:\n    \"\"\"Create a new instance of the metadata class with the owner set.\"\"\"\n    metadata = cls(**data)\n    metadata.owner = owner\n    return metadata\n</code></pre>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.BaseMetadata.validate_data_fields_from_annotations","title":"<code>validate_data_fields_from_annotations(data)</code>  <code>classmethod</code>","text":"<p>Validate the data fields from the annotations.</p> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>@model_validator(mode=\"before\")\n@classmethod\ndef validate_data_fields_from_annotations(cls, data: Any) -&gt; Any:\n    \"\"\"Validate the data fields from the annotations.\"\"\"\n\n    # We want to make sure that we combine the `extra` metdata along with any\n    # other specific fields that are defined in the metadata.\n    extra = data.pop(\"extra\", {})\n    assert isinstance(extra, dict), \"The `extra` field must be a dictionary.\"\n    data = {**data, **extra}\n\n    # If the model has been sub-classed, then all of our fields must be\n    # validated by the pydantic model.\n    if cls._is_field_typed():\n        # We will get the fields out of extra and set them as potential fields to\n        # validate. They will be ignored if they are not defined in the model, but it\n        # allows for a more flexible way to define metadata.\n        # Otherwise, what ever is in the `extra` field will be stroed in the `extra` field.\n        return {**data, \"extra\": extra}\n\n    # Otherwise, we are using our mock-dict implentation, so we store our\n    # metadata in the `extra` field.\n    return {\"extra\": data}\n</code></pre>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.DocumentCollection","title":"<code>DocumentCollection</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[DocumentCollectionMetadata, DocumentNodeMetadata, PageNodeMetadata]</code></p> <p>Represents a collection of documents with some common metadata</p> Source code in <code>docprompt/schema/pipeline/node/collection.py</code> <pre><code>class DocumentCollection(\n    BaseModel,\n    Generic[DocumentCollectionMetadata, DocumentNodeMetadata, PageNodeMetadata],\n):\n    \"\"\"\n    Represents a collection of documents with some common metadata\n    \"\"\"\n\n    document_nodes: List[\"DocumentNode[DocumentNodeMetadata, PageNodeMetadata]\"]\n    metadata: DocumentCollectionMetadata = Field(..., default_factory=dict)\n</code></pre>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.DocumentNode","title":"<code>DocumentNode</code>","text":"<p>               Bases: <code>BaseNode</code>, <code>Generic[DocumentNodeMetadata, PageNodeMetadata]</code></p> <p>Represents a single document, with some metadata</p> Source code in <code>docprompt/schema/pipeline/node/document.py</code> <pre><code>class DocumentNode(BaseNode, Generic[DocumentNodeMetadata, PageNodeMetadata]):\n    \"\"\"\n    Represents a single document, with some metadata\n    \"\"\"\n\n    document: PdfDocument\n    page_nodes: List[PageNode[PageNodeMetadata]] = Field(\n        description=\"The pages in the document\", default_factory=list, repr=False\n    )\n    metadata: DocumentNodeMetadata = Field(\n        description=\"Application-specific metadata for the document\",\n        default_factory=BaseMetadata,\n    )\n\n    _locator: Optional[\"DocumentProvenanceLocator\"] = PrivateAttr(default=None)\n\n    _persistance_path: Optional[str] = PrivateAttr(default=None)\n\n    def __getstate__(self):\n        state = super().__getstate__()\n\n        state[\"__pydantic_private__\"][\"_locator\"] = None\n\n        return state\n\n    def __len__(self):\n        return len(self.page_nodes)\n\n    def __getitem__(self, index):\n        return self.page_nodes[index]\n\n    def __iter__(self):\n        return iter(self.page_nodes)\n\n    @property\n    def rasterizer(self):\n        return DocumentRasterizer(self)\n\n    @property\n    def locator(self):\n        if self._locator is None:\n            self.refresh_locator()\n\n        return self._locator\n\n    def refresh_locator(self):\n        \"\"\"\n        Refreshes the locator for this document node\n        \"\"\"\n        from docprompt.provenance.search import DocumentProvenanceLocator\n\n        if any(not page.ocr_results.result for page in self.page_nodes):\n            raise ValueError(\n                \"Cannot create a locator for a document node with missing OCR results\"\n            )\n\n        self._locator = DocumentProvenanceLocator.from_document_node(self)\n\n        return self.locator\n\n    @classmethod\n    def from_document(\n        cls,\n        document: PdfDocument,\n        document_metadata: Optional[DocumentNodeMetadata] = None,\n        page_metadata: Optional[List[PageNodeMetadata]] = None,\n    ):\n        document_node: \"DocumentNode[DocumentNodeMetadata, PageNodeMetadata]\" = cls(\n            document=document,\n        )\n        document_node.metadata = document_metadata or cls.metadata_class().from_owner(\n            document_node, **{}\n        )\n\n        if page_metadata is not None and len(page_metadata) != len(document):\n            raise ValueError(\n                \"The number of page metadata items must match the number of pages in the document.\"\n            )\n\n        for page_number in range(1, len(document) + 1):\n            if page_metadata is not None:\n                page_node = PageNode(\n                    document=document_node,\n                    page_number=page_number,\n                    metadata=page_metadata[page_number - 1],\n                )\n            else:\n                page_node = PageNode(document=document_node, page_number=page_number)\n\n            document_node.page_nodes.append(page_node)\n\n        return document_node\n\n    @property\n    def file_hash(self):\n        return self.document.document_hash\n\n    @property\n    def document_name(self):\n        return self.document.name\n\n    @classmethod\n    def metadata_class(cls) -&gt; Type[BaseMetadata]:\n        \"\"\"Get the metadata class for instantiating metadata from the model.\"\"\"\n\n        fields = cls.model_fields\n\n        # NOTE: The indexing is important here, and relies on the generic type being\n        # the SECOND of the two arguments in the `Union` annotation\n        metadata_field_annotation = fields[\"metadata\"].annotation\n\n        # If no override has been provided to the metadata model, we want to retrieve\n        # it as a TypedDict\n        if metadata_field_annotation == DocumentNodeMetadata:\n            return BaseMetadata\n\n        if isinstance(metadata_field_annotation, ForwardRef):\n            raise ValueError(\n                \"You cannot define DocumentNode with a ForwardRef for Generic metadata model types.\"\n            )\n\n        # Get the overriden Generic type of th DocumentNodeMetadata\n        return metadata_field_annotation\n\n    @classmethod\n    def page_metadata_class(cls) -&gt; Type[Union[dict, BaseModel]]:\n        \"\"\"Get the metadata class for the page nodes in the document.\"\"\"\n        fields = cls.model_fields\n\n        # NOTE: The indexing is important here, and it allows us to get the type of each\n        # page node in the `List` annotation\n        page_nodes_field_class = fields[\"page_nodes\"].annotation.__args__[0]\n\n        # NOTE: The indexing is important here, and relies on the generic type being\n        # the SECOND of the two arguments in the `Union` annotation\n        page_node_metadata_field_annotation = page_nodes_field_class.model_fields[\n            \"metadata\"\n        ].annotation\n\n        if page_node_metadata_field_annotation == PageNodeMetadata:\n            return BaseMetadata\n\n        if isinstance(page_node_metadata_field_annotation, ForwardRef):\n            raise ValueError(\n                \"You cannot define PageNode with a ForwardRef for Generic metadata model types.\"\n            )\n\n        return page_node_metadata_field_annotation\n\n    @property\n    def persistance_path(self):\n        \"\"\"The base path to storage location.\"\"\"\n        return self._persistance_path\n\n    @persistance_path.setter\n    def persistance_path(self, path: str):\n        \"\"\"Set the base path to storage location.\"\"\"\n        self._persistance_path = path\n\n    @classmethod\n    def from_storage(cls, path: str, file_hash: str, **kwargs) -&gt; Self:\n        \"\"\"Load the document node from storage.\n\n        Args:\n            path (str): The base path to storage location.\n                - Example (S3): \"s3://bucket-name/key/to/folder\"\n                - Example (Local FS): \"/tmp/docprompt/storage\"\n            file_hash (str): The hash of the document.\n            **kwargs: Additional keyword arguments for fsspec FileSystem\n\n        Returns:\n            DocumentNode: The loaded document node.\n        \"\"\"\n\n        fs_manager = FileSystemManager(path, **kwargs)\n\n        pdf_bytes, metadata_bytes, page_metadata_bytes = fs_manager.read(\n            file_hash, **kwargs\n        )\n\n        doc = PdfDocument.from_bytes(pdf_bytes, name=fs_manager.get_pdf_name(file_hash))\n        node = cls.from_document(doc)\n\n        if metadata_bytes:\n            metadata_json = json.loads(metadata_bytes.decode(\"utf-8\"))\n            metadata = cls.metadata_class().from_owner(node, **metadata_json)\n        else:\n            metadata = cls.metadata_class().from_owner(node, **{})\n\n        if page_metadata_bytes:\n            page_metadata_json = [\n                json.loads(page_str)\n                for page_str in json.loads(page_metadata_bytes.decode(\"utf-8\"))\n            ]\n            page_metadata = [\n                cls.page_metadata_class()(**page) for page in page_metadata_json\n            ]\n        else:\n            page_metadata = [cls.page_metadata_class()(**{}) for _ in range(len(doc))]\n\n        # Store the metadata on the node and page nodes\n        node.metadata = metadata\n        for page, meta in zip(node.page_nodes, page_metadata):\n            meta.set_owner(page)\n            page.metadata = meta\n\n        # Make sure to set the persistance path on the node\n        node.persistance_path = path\n\n        return node\n\n    def persist(self, path: Optional[str] = None, **kwargs) -&gt; FileSidecarsPathManager:\n        \"\"\"Persist a document node to storage.\n\n        Args:\n            path (Optional[str]): Overwrites the current `persistance_path` property\n                - If `persistance_path` is not currently set, path must be provided.\n            **kwargs: Additional keyword arguments for fsspec FileSystem\n\n        Returns:\n            FileSidecarsPathManager: The file path manager for the persisted document node.\n        \"\"\"\n\n        path = path or self.persistance_path\n\n        if path is None:\n            raise ValueError(\"The path must be provided to persist the document node.\")\n\n        # Make sure to update the persistance path\n        self.persistance_path = path\n\n        fs_manager = FileSystemManager(path, **kwargs)\n\n        pdf_bytes = self.document.get_bytes()\n        metadata_bytes = bytes(self.metadata.model_dump_json(), encoding=\"utf-8\")\n        page_metadata_bytes = bytes(\n            json.dumps([page.metadata.model_dump_json() for page in self.page_nodes]),\n            encoding=\"utf-8\",\n        )\n\n        return fs_manager.write(\n            pdf_bytes, metadata_bytes, page_metadata_bytes, **kwargs\n        )\n</code></pre>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.DocumentNode.persistance_path","title":"<code>persistance_path</code>  <code>property</code> <code>writable</code>","text":"<p>The base path to storage location.</p>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.DocumentNode.from_storage","title":"<code>from_storage(path, file_hash, **kwargs)</code>  <code>classmethod</code>","text":"<p>Load the document node from storage.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The base path to storage location. - Example (S3): \"s3://bucket-name/key/to/folder\" - Example (Local FS): \"/tmp/docprompt/storage\"</p> required <code>file_hash</code> <code>str</code> <p>The hash of the document.</p> required <code>**kwargs</code> <p>Additional keyword arguments for fsspec FileSystem</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>DocumentNode</code> <code>Self</code> <p>The loaded document node.</p> Source code in <code>docprompt/schema/pipeline/node/document.py</code> <pre><code>@classmethod\ndef from_storage(cls, path: str, file_hash: str, **kwargs) -&gt; Self:\n    \"\"\"Load the document node from storage.\n\n    Args:\n        path (str): The base path to storage location.\n            - Example (S3): \"s3://bucket-name/key/to/folder\"\n            - Example (Local FS): \"/tmp/docprompt/storage\"\n        file_hash (str): The hash of the document.\n        **kwargs: Additional keyword arguments for fsspec FileSystem\n\n    Returns:\n        DocumentNode: The loaded document node.\n    \"\"\"\n\n    fs_manager = FileSystemManager(path, **kwargs)\n\n    pdf_bytes, metadata_bytes, page_metadata_bytes = fs_manager.read(\n        file_hash, **kwargs\n    )\n\n    doc = PdfDocument.from_bytes(pdf_bytes, name=fs_manager.get_pdf_name(file_hash))\n    node = cls.from_document(doc)\n\n    if metadata_bytes:\n        metadata_json = json.loads(metadata_bytes.decode(\"utf-8\"))\n        metadata = cls.metadata_class().from_owner(node, **metadata_json)\n    else:\n        metadata = cls.metadata_class().from_owner(node, **{})\n\n    if page_metadata_bytes:\n        page_metadata_json = [\n            json.loads(page_str)\n            for page_str in json.loads(page_metadata_bytes.decode(\"utf-8\"))\n        ]\n        page_metadata = [\n            cls.page_metadata_class()(**page) for page in page_metadata_json\n        ]\n    else:\n        page_metadata = [cls.page_metadata_class()(**{}) for _ in range(len(doc))]\n\n    # Store the metadata on the node and page nodes\n    node.metadata = metadata\n    for page, meta in zip(node.page_nodes, page_metadata):\n        meta.set_owner(page)\n        page.metadata = meta\n\n    # Make sure to set the persistance path on the node\n    node.persistance_path = path\n\n    return node\n</code></pre>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.DocumentNode.metadata_class","title":"<code>metadata_class()</code>  <code>classmethod</code>","text":"<p>Get the metadata class for instantiating metadata from the model.</p> Source code in <code>docprompt/schema/pipeline/node/document.py</code> <pre><code>@classmethod\ndef metadata_class(cls) -&gt; Type[BaseMetadata]:\n    \"\"\"Get the metadata class for instantiating metadata from the model.\"\"\"\n\n    fields = cls.model_fields\n\n    # NOTE: The indexing is important here, and relies on the generic type being\n    # the SECOND of the two arguments in the `Union` annotation\n    metadata_field_annotation = fields[\"metadata\"].annotation\n\n    # If no override has been provided to the metadata model, we want to retrieve\n    # it as a TypedDict\n    if metadata_field_annotation == DocumentNodeMetadata:\n        return BaseMetadata\n\n    if isinstance(metadata_field_annotation, ForwardRef):\n        raise ValueError(\n            \"You cannot define DocumentNode with a ForwardRef for Generic metadata model types.\"\n        )\n\n    # Get the overriden Generic type of th DocumentNodeMetadata\n    return metadata_field_annotation\n</code></pre>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.DocumentNode.page_metadata_class","title":"<code>page_metadata_class()</code>  <code>classmethod</code>","text":"<p>Get the metadata class for the page nodes in the document.</p> Source code in <code>docprompt/schema/pipeline/node/document.py</code> <pre><code>@classmethod\ndef page_metadata_class(cls) -&gt; Type[Union[dict, BaseModel]]:\n    \"\"\"Get the metadata class for the page nodes in the document.\"\"\"\n    fields = cls.model_fields\n\n    # NOTE: The indexing is important here, and it allows us to get the type of each\n    # page node in the `List` annotation\n    page_nodes_field_class = fields[\"page_nodes\"].annotation.__args__[0]\n\n    # NOTE: The indexing is important here, and relies on the generic type being\n    # the SECOND of the two arguments in the `Union` annotation\n    page_node_metadata_field_annotation = page_nodes_field_class.model_fields[\n        \"metadata\"\n    ].annotation\n\n    if page_node_metadata_field_annotation == PageNodeMetadata:\n        return BaseMetadata\n\n    if isinstance(page_node_metadata_field_annotation, ForwardRef):\n        raise ValueError(\n            \"You cannot define PageNode with a ForwardRef for Generic metadata model types.\"\n        )\n\n    return page_node_metadata_field_annotation\n</code></pre>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.DocumentNode.persist","title":"<code>persist(path=None, **kwargs)</code>","text":"<p>Persist a document node to storage.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Optional[str]</code> <p>Overwrites the current <code>persistance_path</code> property - If <code>persistance_path</code> is not currently set, path must be provided.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments for fsspec FileSystem</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>FileSidecarsPathManager</code> <code>FileSidecarsPathManager</code> <p>The file path manager for the persisted document node.</p> Source code in <code>docprompt/schema/pipeline/node/document.py</code> <pre><code>def persist(self, path: Optional[str] = None, **kwargs) -&gt; FileSidecarsPathManager:\n    \"\"\"Persist a document node to storage.\n\n    Args:\n        path (Optional[str]): Overwrites the current `persistance_path` property\n            - If `persistance_path` is not currently set, path must be provided.\n        **kwargs: Additional keyword arguments for fsspec FileSystem\n\n    Returns:\n        FileSidecarsPathManager: The file path manager for the persisted document node.\n    \"\"\"\n\n    path = path or self.persistance_path\n\n    if path is None:\n        raise ValueError(\"The path must be provided to persist the document node.\")\n\n    # Make sure to update the persistance path\n    self.persistance_path = path\n\n    fs_manager = FileSystemManager(path, **kwargs)\n\n    pdf_bytes = self.document.get_bytes()\n    metadata_bytes = bytes(self.metadata.model_dump_json(), encoding=\"utf-8\")\n    page_metadata_bytes = bytes(\n        json.dumps([page.metadata.model_dump_json() for page in self.page_nodes]),\n        encoding=\"utf-8\",\n    )\n\n    return fs_manager.write(\n        pdf_bytes, metadata_bytes, page_metadata_bytes, **kwargs\n    )\n</code></pre>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.DocumentNode.refresh_locator","title":"<code>refresh_locator()</code>","text":"<p>Refreshes the locator for this document node</p> Source code in <code>docprompt/schema/pipeline/node/document.py</code> <pre><code>def refresh_locator(self):\n    \"\"\"\n    Refreshes the locator for this document node\n    \"\"\"\n    from docprompt.provenance.search import DocumentProvenanceLocator\n\n    if any(not page.ocr_results.result for page in self.page_nodes):\n        raise ValueError(\n            \"Cannot create a locator for a document node with missing OCR results\"\n        )\n\n    self._locator = DocumentProvenanceLocator.from_document_node(self)\n\n    return self.locator\n</code></pre>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.PageNode","title":"<code>PageNode</code>","text":"<p>               Bases: <code>BaseNode</code>, <code>Generic[PageNodeMetadata]</code></p> <p>Represents a single page in a document, with some metadata</p> Source code in <code>docprompt/schema/pipeline/node/page.py</code> <pre><code>class PageNode(BaseNode, Generic[PageNodeMetadata]):\n    \"\"\"\n    Represents a single page in a document, with some metadata\n    \"\"\"\n\n    document: \"DocumentNode\" = Field(exclude=True, repr=False)\n    page_number: PositiveInt = Field(description=\"The page number\")\n    metadata: PageNodeMetadata = Field(\n        description=\"Application-specific metadata for the page\",\n        default_factory=BaseMetadata,\n    )\n    extra: Dict[str, Any] = Field(\n        description=\"Extra data that can be stored on the page node\",\n        default_factory=dict,\n    )\n\n    ocr_results: ResultContainer[OcrPageResult] = Field(\n        default_factory=_result_container_factory,\n        description=\"The OCR results for the page\",\n        repr=False,\n    )\n\n    _raster_cache: Dict[str, bytes] = PrivateAttr(default_factory=dict)\n\n    def __getstate__(self):\n        state = super().__getstate__()\n\n        state[\"__pydantic_private__\"][\"_raster_cache\"] = {}\n\n        return state\n\n    @property\n    def rasterizer(self):\n        return PageRasterizer(self._raster_cache, self)\n\n    def search(\n        self, query: str, refine_to_words: bool = True, require_exact_match: bool = True\n    ):\n        return self.document.locator.search(\n            query,\n            page_number=self.page_number,\n            refine_to_word=refine_to_words,\n            require_exact_match=require_exact_match,\n        )\n</code></pre>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.metadata","title":"<code>metadata</code>","text":"<p>The metadata class is utilized for defining a basic, yet flexible interface for metadata attached to various fields.</p> <p>In essence, this allows for developers to choose to either create their metadtata in an unstructured manner (i.e. a dictionary), or to sub class the base metadata class in order to create a more strictly typed metadata model for their page and document nodes.</p>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.metadata.BaseMetadata","title":"<code>BaseMetadata</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>MutableMapping</code>, <code>Generic[TMetadataOwner]</code></p> <p>The base metadata class is utilized for defining a basic yet flexible interface for metadata attached to various fields.</p> The metadata class can be used in two ways <ol> <li>As a dictionary-like object, where metadata is stored in the <code>extra</code> field.</li> <li>As a sub-classed model, where metadata is stored in the fields of the model.</li> </ol> <p>When used out of the box, the metadata class will adobpt dictionary-like behavior. You may easily access different fields of the metadata as if it were a dictionary: <pre><code># Instantiate it with any kwargs you like\nmetadata = BaseMetadata(foo-'bar', cow='moo')\n\nmetadata[\"foo\"]  # \"bar\"\nmetadata[\"cow\"]  # \"moo\"\n\n# Update the value of the key\nmetadata[\"foo\"] = \"fighters\"\n\n# Set new key-value pairs\nmetadata['sheep'] = 'baa'\n</code></pre></p> <p>Otherwise, you may sub-class the metadata class in order to create a more strictly typed metadata model. This is useful when you want to enforce a specific structure for your metadata.</p> <pre><code>class CustomMetadata(BaseMetadata):\n    foo: str\n    cow: str\n\n# Instantiate it with the required fields\nmetadata = CustomMetadata(foo='bar', cow='moo')\n\nmetadata.foo  # \"bar\"\nmetadata.cow  # \"moo\"\n\n# Update the value of the key\nmetadata.foo = \"fighters\"\n\n# Use the extra field to store dynamic metadata\nmetadata.extra['sheep'] = 'baa'\n</code></pre> <p>Additionally, the task results descriptor allows for controlled and easy access to the task results of various tasks that are run on the parent node.</p> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>class BaseMetadata(BaseModel, MutableMapping, Generic[TMetadataOwner]):\n    \"\"\"\n    The base metadata class is utilized for defining a basic yet flexible interface\n    for metadata attached to various fields.\n\n    The metadata class can be used in two ways:\n        1. As a dictionary-like object, where metadata is stored in the `extra` field.\n        2. As a sub-classed model, where metadata is stored in the fields of the model.\n\n    When used out of the box, the metadata class will adobpt dictionary-like behavior. You\n    may easily access different fields of the metadata as if it were a dictionary:\n    ```python\n    # Instantiate it with any kwargs you like\n    metadata = BaseMetadata(foo-'bar', cow='moo')\n\n    metadata[\"foo\"]  # \"bar\"\n    metadata[\"cow\"]  # \"moo\"\n\n    # Update the value of the key\n    metadata[\"foo\"] = \"fighters\"\n\n    # Set new key-value pairs\n    metadata['sheep'] = 'baa'\n    ```\n\n    Otherwise, you may sub-class the metadata class in order to create a more strictly typed\n    metadata model. This is useful when you want to enforce a specific structure for your metadata.\n\n    ```python\n    class CustomMetadata(BaseMetadata):\n        foo: str\n        cow: str\n\n    # Instantiate it with the required fields\n    metadata = CustomMetadata(foo='bar', cow='moo')\n\n    metadata.foo  # \"bar\"\n    metadata.cow  # \"moo\"\n\n    # Update the value of the key\n    metadata.foo = \"fighters\"\n\n    # Use the extra field to store dynamic metadata\n    metadata.extra['sheep'] = 'baa'\n    ```\n\n    Additionally, the task results descriptor allows for controlled and easy access to the task results\n    of various tasks that are run on the parent node.\n    \"\"\"\n\n    extra: Dict[str, Any] = Field(..., default_factory=dict, repr=False)\n\n    _task_results: TaskResultsDescriptor = PrivateAttr(\n        default_factory=TaskResultsDescriptor\n    )\n\n    _owner: TMetadataOwner = PrivateAttr()\n\n    @property\n    def task_results(self) -&gt; TaskResultsDescriptor:\n        \"\"\"Return the task results descriptor.\"\"\"\n        return self._task_results.__get__(self)\n\n    @task_results.setter\n    def task_results(self, value: Any) -&gt; None:\n        \"\"\"This will raise an error, as we do not want to set the task results directly.\n\n        NOTE: This implementation is here purely to avoid the task_results property from being\n        overwritten by accident.\n        \"\"\"\n        self._task_results.__set__(self, value)\n\n    @property\n    def owner(self) -&gt; TMetadataOwner:\n        \"\"\"Return the owner of the metadata.\"\"\"\n        return self._owner\n\n    @owner.setter\n    def owner(self, owner: TMetadataOwner) -&gt; None:\n        \"\"\"Return the owner of the metadata.\"\"\"\n        self._owner = owner\n\n    @classmethod\n    def from_owner(cls, owner: TMetadataOwner, **data) -&gt; BaseMetadata:\n        \"\"\"Create a new instance of the metadata class with the owner set.\"\"\"\n        metadata = cls(**data)\n        metadata.owner = owner\n        return metadata\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def validate_data_fields_from_annotations(cls, data: Any) -&gt; Any:\n        \"\"\"Validate the data fields from the annotations.\"\"\"\n\n        # We want to make sure that we combine the `extra` metdata along with any\n        # other specific fields that are defined in the metadata.\n        extra = data.pop(\"extra\", {})\n        assert isinstance(extra, dict), \"The `extra` field must be a dictionary.\"\n        data = {**data, **extra}\n\n        # If the model has been sub-classed, then all of our fields must be\n        # validated by the pydantic model.\n        if cls._is_field_typed():\n            # We will get the fields out of extra and set them as potential fields to\n            # validate. They will be ignored if they are not defined in the model, but it\n            # allows for a more flexible way to define metadata.\n            # Otherwise, what ever is in the `extra` field will be stroed in the `extra` field.\n            return {**data, \"extra\": extra}\n\n        # Otherwise, we are using our mock-dict implentation, so we store our\n        # metadata in the `extra` field.\n        return {\"extra\": data}\n\n    @classmethod\n    def _is_field_typed(cls):\n        \"\"\"\n        Check if the metadata model is field typed.\n\n        This is used to determine if the metadata model is a dictionary-like model,\n        or a more strictly typed model.\n        \"\"\"\n        if set([\"extra\"]) != set(cls.model_fields.keys()):\n            return True\n\n        return False\n\n    def __repr__(self):\n        \"\"\"\n        Provide a string representation of the metadata.\n\n        This only works for the base metadata model. If sub-classed, this will raise an error,\n        unless overridden, as BaseModel's do not have a __repr__ method.\n        \"\"\"\n        if self._is_field_typed():\n            return super().__repr__()\n\n        # Otherwise, we are deailing with dictornary-like metadata\n        return json.dumps(self.extra)\n\n    def __getitem__(self, name):\n        \"\"\"\n        Provide dictionary functionlaity to the metadata class.\n\n        This only works for the base metadata model. If sub-classed, this will raise an error,\n        unless overridden, as BaseModel's do not have an __getitem__ method.\n        \"\"\"\n        if not self._is_field_typed():\n            return self.extra[name]\n\n        raise AttributeError(\n            f\"'{self.__class__.__name__}' object has no attribute '{name}'\"\n        )\n\n    def __setitem__(self, name, value):\n        \"\"\"\n        Provide dictionary functionality to the metadata class.\n\n        This only works for the base metadata model. If sub-classed, this will raise an error,\n        unless overridden, as BaseModel's do not have an __setitem__ method.\n        \"\"\"\n        if not self._is_field_typed():\n            self.extra[name] = value\n        else:\n            raise AttributeError(\n                f\"'{self.__class__.__name__}' object has no attribute '{name}'\"\n            )\n\n    def __delitem__(self, name):\n        \"\"\"\n        Provide dictionary functionality to the metadata class.\n\n        This only works for the base metadata model. If sub-classed, this will raise an error,\n        unless overridden, as BaseModel's do not have an __delitem__ method.\n        \"\"\"\n        if not self._is_field_typed():\n            del self.extra[name]\n        else:\n            raise AttributeError(\n                f\"'{self.__class__.__name__}' object has no attribute '{name}'\"\n            )\n\n    def __iter__(self):\n        \"\"\"\n        Iterate over the keys in the metadata.\n\n        This only works for the base metadata model. If sub-classed, this will raise an error,\n        unless overridden, as BaseModel's do not have an __iter__ method.\n        \"\"\"\n        if self._is_field_typed():\n            raise AttributeError(f\"'{self.__class__.__name__}' object is not iterable\")\n\n        return iter(self.extra)\n\n    def __len__(self):\n        \"\"\"\n        Get the number of keys in the metadata.\n\n        This only works for the base metadata model. If sub-classed, this will raise an error,\n        unless overridden, as BaseModel's do not have a __len__ method.\n        \"\"\"\n        if self._is_field_typed():\n            raise AttributeError(\n                f\"'{self.__class__.__name__}' object has no attribute '__len__'\"\n            )\n\n        return len(self.extra)\n\n    def __getattr__(self, name):\n        \"\"\"\n        Allow for getting of attributes on the metadata class.\n\n        The attributes are retrieved through the following heirarchy:\n            - If the model is sub-classed, it will be retrieved as normal.\n            - Otherwise, if the attribute is private, it will be retrieved as normal.\n            - Finally, if we are getting a public attribute on the base metadata class,\n                we use the extra field.\n            - If the key is not set in the `extra` dictionary, we resort back to just\n            trying to get the field.\n                - This is when we grab the `owner` or `task_result` attribuite.\n        \"\"\"\n        if self._is_field_typed():\n            return super().__getattr__(name)\n\n        if name.startswith(\"_\"):\n            return super().__getattr__(name)\n\n        # Attempt to retreieve the attr from the `extra` field\n        try:\n            return self.extra.get(name)\n\n        except KeyError:\n            # This is for grabbing properties on the base metadata class\n            return super().__getattr__(name)\n\n    def __setattr__(self, name: str, value: Any) -&gt; None:\n        \"\"\"\n        Allow for setting of attributes on the metadata class.\n\n        The attributes are set through the following heirarchy:\n            - If the model is sub-classed, it will be set as normal.\n            - Otherwise, if the attribute is private, it will be set as normal.\n            - Finally, if we are setting a public attribute on the base metadata class,\n                we use the extra field.\n        \"\"\"\n        if self._is_field_typed():\n            return super().__setattr__(name, value)\n\n        # We want to avoid setting any private attributes in the extra\n        # dictionary\n        if name.startswith(\"_\"):\n            return super().__setattr__(name, value)\n\n        # If it is `owner` or `task_results`, we want\n        # to avoid setting the attribute in the `extra` dictionary\n        if name in [\"owner\", \"task_results\"]:\n            return super().__setattr__(name, value)\n\n        self.extra[name] = value\n\n    def __delattr__(self, name: str) -&gt; None:\n        \"\"\"\n        Ensure that we can delete attributes from the metadata class.\n\n        The attributes are deleted through the following heirarchy:\n            - If the attribute is `task_results`, we use the descriptor to delete the task results.\n            - Otherwise, if it is a sub-classed model, it will be deleted as normal.\n            - Finally, if we are deleting a public attribute on the base metadata class,\n                we use the extra field.\n        \"\"\"\n\n        # We want to use the descriptor to delete the task results\n        if name == \"task_results\":\n            self._task_results.__delete__(self)\n            return\n\n        # Otherwise, we use our standard fallback tiers\n        if self._is_field_typed():\n            return super().__delattr__(name)\n\n        del self.extra[name]\n</code></pre>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.metadata.BaseMetadata.owner","title":"<code>owner: TMetadataOwner</code>  <code>property</code> <code>writable</code>","text":"<p>Return the owner of the metadata.</p>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.metadata.BaseMetadata.task_results","title":"<code>task_results: TaskResultsDescriptor</code>  <code>property</code> <code>writable</code>","text":"<p>Return the task results descriptor.</p>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.metadata.BaseMetadata.__delattr__","title":"<code>__delattr__(name)</code>","text":"<p>Ensure that we can delete attributes from the metadata class.</p> The attributes are deleted through the following heirarchy <ul> <li>If the attribute is <code>task_results</code>, we use the descriptor to delete the task results.</li> <li>Otherwise, if it is a sub-classed model, it will be deleted as normal.</li> <li>Finally, if we are deleting a public attribute on the base metadata class,     we use the extra field.</li> </ul> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>def __delattr__(self, name: str) -&gt; None:\n    \"\"\"\n    Ensure that we can delete attributes from the metadata class.\n\n    The attributes are deleted through the following heirarchy:\n        - If the attribute is `task_results`, we use the descriptor to delete the task results.\n        - Otherwise, if it is a sub-classed model, it will be deleted as normal.\n        - Finally, if we are deleting a public attribute on the base metadata class,\n            we use the extra field.\n    \"\"\"\n\n    # We want to use the descriptor to delete the task results\n    if name == \"task_results\":\n        self._task_results.__delete__(self)\n        return\n\n    # Otherwise, we use our standard fallback tiers\n    if self._is_field_typed():\n        return super().__delattr__(name)\n\n    del self.extra[name]\n</code></pre>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.metadata.BaseMetadata.__delitem__","title":"<code>__delitem__(name)</code>","text":"<p>Provide dictionary functionality to the metadata class.</p> <p>This only works for the base metadata model. If sub-classed, this will raise an error, unless overridden, as BaseModel's do not have an delitem method.</p> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>def __delitem__(self, name):\n    \"\"\"\n    Provide dictionary functionality to the metadata class.\n\n    This only works for the base metadata model. If sub-classed, this will raise an error,\n    unless overridden, as BaseModel's do not have an __delitem__ method.\n    \"\"\"\n    if not self._is_field_typed():\n        del self.extra[name]\n    else:\n        raise AttributeError(\n            f\"'{self.__class__.__name__}' object has no attribute '{name}'\"\n        )\n</code></pre>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.metadata.BaseMetadata.__getattr__","title":"<code>__getattr__(name)</code>","text":"<p>Allow for getting of attributes on the metadata class.</p> The attributes are retrieved through the following heirarchy <ul> <li>If the model is sub-classed, it will be retrieved as normal.</li> <li>Otherwise, if the attribute is private, it will be retrieved as normal.</li> <li>Finally, if we are getting a public attribute on the base metadata class,     we use the extra field.</li> <li>If the key is not set in the <code>extra</code> dictionary, we resort back to just trying to get the field.<ul> <li>This is when we grab the <code>owner</code> or <code>task_result</code> attribuite.</li> </ul> </li> </ul> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>def __getattr__(self, name):\n    \"\"\"\n    Allow for getting of attributes on the metadata class.\n\n    The attributes are retrieved through the following heirarchy:\n        - If the model is sub-classed, it will be retrieved as normal.\n        - Otherwise, if the attribute is private, it will be retrieved as normal.\n        - Finally, if we are getting a public attribute on the base metadata class,\n            we use the extra field.\n        - If the key is not set in the `extra` dictionary, we resort back to just\n        trying to get the field.\n            - This is when we grab the `owner` or `task_result` attribuite.\n    \"\"\"\n    if self._is_field_typed():\n        return super().__getattr__(name)\n\n    if name.startswith(\"_\"):\n        return super().__getattr__(name)\n\n    # Attempt to retreieve the attr from the `extra` field\n    try:\n        return self.extra.get(name)\n\n    except KeyError:\n        # This is for grabbing properties on the base metadata class\n        return super().__getattr__(name)\n</code></pre>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.metadata.BaseMetadata.__getitem__","title":"<code>__getitem__(name)</code>","text":"<p>Provide dictionary functionlaity to the metadata class.</p> <p>This only works for the base metadata model. If sub-classed, this will raise an error, unless overridden, as BaseModel's do not have an getitem method.</p> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>def __getitem__(self, name):\n    \"\"\"\n    Provide dictionary functionlaity to the metadata class.\n\n    This only works for the base metadata model. If sub-classed, this will raise an error,\n    unless overridden, as BaseModel's do not have an __getitem__ method.\n    \"\"\"\n    if not self._is_field_typed():\n        return self.extra[name]\n\n    raise AttributeError(\n        f\"'{self.__class__.__name__}' object has no attribute '{name}'\"\n    )\n</code></pre>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.metadata.BaseMetadata.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over the keys in the metadata.</p> <p>This only works for the base metadata model. If sub-classed, this will raise an error, unless overridden, as BaseModel's do not have an iter method.</p> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>def __iter__(self):\n    \"\"\"\n    Iterate over the keys in the metadata.\n\n    This only works for the base metadata model. If sub-classed, this will raise an error,\n    unless overridden, as BaseModel's do not have an __iter__ method.\n    \"\"\"\n    if self._is_field_typed():\n        raise AttributeError(f\"'{self.__class__.__name__}' object is not iterable\")\n\n    return iter(self.extra)\n</code></pre>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.metadata.BaseMetadata.__len__","title":"<code>__len__()</code>","text":"<p>Get the number of keys in the metadata.</p> <p>This only works for the base metadata model. If sub-classed, this will raise an error, unless overridden, as BaseModel's do not have a len method.</p> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>def __len__(self):\n    \"\"\"\n    Get the number of keys in the metadata.\n\n    This only works for the base metadata model. If sub-classed, this will raise an error,\n    unless overridden, as BaseModel's do not have a __len__ method.\n    \"\"\"\n    if self._is_field_typed():\n        raise AttributeError(\n            f\"'{self.__class__.__name__}' object has no attribute '__len__'\"\n        )\n\n    return len(self.extra)\n</code></pre>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.metadata.BaseMetadata.__repr__","title":"<code>__repr__()</code>","text":"<p>Provide a string representation of the metadata.</p> <p>This only works for the base metadata model. If sub-classed, this will raise an error, unless overridden, as BaseModel's do not have a repr method.</p> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>def __repr__(self):\n    \"\"\"\n    Provide a string representation of the metadata.\n\n    This only works for the base metadata model. If sub-classed, this will raise an error,\n    unless overridden, as BaseModel's do not have a __repr__ method.\n    \"\"\"\n    if self._is_field_typed():\n        return super().__repr__()\n\n    # Otherwise, we are deailing with dictornary-like metadata\n    return json.dumps(self.extra)\n</code></pre>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.metadata.BaseMetadata.__setattr__","title":"<code>__setattr__(name, value)</code>","text":"<p>Allow for setting of attributes on the metadata class.</p> The attributes are set through the following heirarchy <ul> <li>If the model is sub-classed, it will be set as normal.</li> <li>Otherwise, if the attribute is private, it will be set as normal.</li> <li>Finally, if we are setting a public attribute on the base metadata class,     we use the extra field.</li> </ul> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>def __setattr__(self, name: str, value: Any) -&gt; None:\n    \"\"\"\n    Allow for setting of attributes on the metadata class.\n\n    The attributes are set through the following heirarchy:\n        - If the model is sub-classed, it will be set as normal.\n        - Otherwise, if the attribute is private, it will be set as normal.\n        - Finally, if we are setting a public attribute on the base metadata class,\n            we use the extra field.\n    \"\"\"\n    if self._is_field_typed():\n        return super().__setattr__(name, value)\n\n    # We want to avoid setting any private attributes in the extra\n    # dictionary\n    if name.startswith(\"_\"):\n        return super().__setattr__(name, value)\n\n    # If it is `owner` or `task_results`, we want\n    # to avoid setting the attribute in the `extra` dictionary\n    if name in [\"owner\", \"task_results\"]:\n        return super().__setattr__(name, value)\n\n    self.extra[name] = value\n</code></pre>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.metadata.BaseMetadata.__setitem__","title":"<code>__setitem__(name, value)</code>","text":"<p>Provide dictionary functionality to the metadata class.</p> <p>This only works for the base metadata model. If sub-classed, this will raise an error, unless overridden, as BaseModel's do not have an setitem method.</p> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>def __setitem__(self, name, value):\n    \"\"\"\n    Provide dictionary functionality to the metadata class.\n\n    This only works for the base metadata model. If sub-classed, this will raise an error,\n    unless overridden, as BaseModel's do not have an __setitem__ method.\n    \"\"\"\n    if not self._is_field_typed():\n        self.extra[name] = value\n    else:\n        raise AttributeError(\n            f\"'{self.__class__.__name__}' object has no attribute '{name}'\"\n        )\n</code></pre>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.metadata.BaseMetadata.from_owner","title":"<code>from_owner(owner, **data)</code>  <code>classmethod</code>","text":"<p>Create a new instance of the metadata class with the owner set.</p> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>@classmethod\ndef from_owner(cls, owner: TMetadataOwner, **data) -&gt; BaseMetadata:\n    \"\"\"Create a new instance of the metadata class with the owner set.\"\"\"\n    metadata = cls(**data)\n    metadata.owner = owner\n    return metadata\n</code></pre>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.metadata.BaseMetadata.validate_data_fields_from_annotations","title":"<code>validate_data_fields_from_annotations(data)</code>  <code>classmethod</code>","text":"<p>Validate the data fields from the annotations.</p> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>@model_validator(mode=\"before\")\n@classmethod\ndef validate_data_fields_from_annotations(cls, data: Any) -&gt; Any:\n    \"\"\"Validate the data fields from the annotations.\"\"\"\n\n    # We want to make sure that we combine the `extra` metdata along with any\n    # other specific fields that are defined in the metadata.\n    extra = data.pop(\"extra\", {})\n    assert isinstance(extra, dict), \"The `extra` field must be a dictionary.\"\n    data = {**data, **extra}\n\n    # If the model has been sub-classed, then all of our fields must be\n    # validated by the pydantic model.\n    if cls._is_field_typed():\n        # We will get the fields out of extra and set them as potential fields to\n        # validate. They will be ignored if they are not defined in the model, but it\n        # allows for a more flexible way to define metadata.\n        # Otherwise, what ever is in the `extra` field will be stroed in the `extra` field.\n        return {**data, \"extra\": extra}\n\n    # Otherwise, we are using our mock-dict implentation, so we store our\n    # metadata in the `extra` field.\n    return {\"extra\": data}\n</code></pre>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.node","title":"<code>node</code>","text":""},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.node.DocumentCollection","title":"<code>DocumentCollection</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[DocumentCollectionMetadata, DocumentNodeMetadata, PageNodeMetadata]</code></p> <p>Represents a collection of documents with some common metadata</p> Source code in <code>docprompt/schema/pipeline/node/collection.py</code> <pre><code>class DocumentCollection(\n    BaseModel,\n    Generic[DocumentCollectionMetadata, DocumentNodeMetadata, PageNodeMetadata],\n):\n    \"\"\"\n    Represents a collection of documents with some common metadata\n    \"\"\"\n\n    document_nodes: List[\"DocumentNode[DocumentNodeMetadata, PageNodeMetadata]\"]\n    metadata: DocumentCollectionMetadata = Field(..., default_factory=dict)\n</code></pre>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.node.DocumentNode","title":"<code>DocumentNode</code>","text":"<p>               Bases: <code>BaseNode</code>, <code>Generic[DocumentNodeMetadata, PageNodeMetadata]</code></p> <p>Represents a single document, with some metadata</p> Source code in <code>docprompt/schema/pipeline/node/document.py</code> <pre><code>class DocumentNode(BaseNode, Generic[DocumentNodeMetadata, PageNodeMetadata]):\n    \"\"\"\n    Represents a single document, with some metadata\n    \"\"\"\n\n    document: PdfDocument\n    page_nodes: List[PageNode[PageNodeMetadata]] = Field(\n        description=\"The pages in the document\", default_factory=list, repr=False\n    )\n    metadata: DocumentNodeMetadata = Field(\n        description=\"Application-specific metadata for the document\",\n        default_factory=BaseMetadata,\n    )\n\n    _locator: Optional[\"DocumentProvenanceLocator\"] = PrivateAttr(default=None)\n\n    _persistance_path: Optional[str] = PrivateAttr(default=None)\n\n    def __getstate__(self):\n        state = super().__getstate__()\n\n        state[\"__pydantic_private__\"][\"_locator\"] = None\n\n        return state\n\n    def __len__(self):\n        return len(self.page_nodes)\n\n    def __getitem__(self, index):\n        return self.page_nodes[index]\n\n    def __iter__(self):\n        return iter(self.page_nodes)\n\n    @property\n    def rasterizer(self):\n        return DocumentRasterizer(self)\n\n    @property\n    def locator(self):\n        if self._locator is None:\n            self.refresh_locator()\n\n        return self._locator\n\n    def refresh_locator(self):\n        \"\"\"\n        Refreshes the locator for this document node\n        \"\"\"\n        from docprompt.provenance.search import DocumentProvenanceLocator\n\n        if any(not page.ocr_results.result for page in self.page_nodes):\n            raise ValueError(\n                \"Cannot create a locator for a document node with missing OCR results\"\n            )\n\n        self._locator = DocumentProvenanceLocator.from_document_node(self)\n\n        return self.locator\n\n    @classmethod\n    def from_document(\n        cls,\n        document: PdfDocument,\n        document_metadata: Optional[DocumentNodeMetadata] = None,\n        page_metadata: Optional[List[PageNodeMetadata]] = None,\n    ):\n        document_node: \"DocumentNode[DocumentNodeMetadata, PageNodeMetadata]\" = cls(\n            document=document,\n        )\n        document_node.metadata = document_metadata or cls.metadata_class().from_owner(\n            document_node, **{}\n        )\n\n        if page_metadata is not None and len(page_metadata) != len(document):\n            raise ValueError(\n                \"The number of page metadata items must match the number of pages in the document.\"\n            )\n\n        for page_number in range(1, len(document) + 1):\n            if page_metadata is not None:\n                page_node = PageNode(\n                    document=document_node,\n                    page_number=page_number,\n                    metadata=page_metadata[page_number - 1],\n                )\n            else:\n                page_node = PageNode(document=document_node, page_number=page_number)\n\n            document_node.page_nodes.append(page_node)\n\n        return document_node\n\n    @property\n    def file_hash(self):\n        return self.document.document_hash\n\n    @property\n    def document_name(self):\n        return self.document.name\n\n    @classmethod\n    def metadata_class(cls) -&gt; Type[BaseMetadata]:\n        \"\"\"Get the metadata class for instantiating metadata from the model.\"\"\"\n\n        fields = cls.model_fields\n\n        # NOTE: The indexing is important here, and relies on the generic type being\n        # the SECOND of the two arguments in the `Union` annotation\n        metadata_field_annotation = fields[\"metadata\"].annotation\n\n        # If no override has been provided to the metadata model, we want to retrieve\n        # it as a TypedDict\n        if metadata_field_annotation == DocumentNodeMetadata:\n            return BaseMetadata\n\n        if isinstance(metadata_field_annotation, ForwardRef):\n            raise ValueError(\n                \"You cannot define DocumentNode with a ForwardRef for Generic metadata model types.\"\n            )\n\n        # Get the overriden Generic type of th DocumentNodeMetadata\n        return metadata_field_annotation\n\n    @classmethod\n    def page_metadata_class(cls) -&gt; Type[Union[dict, BaseModel]]:\n        \"\"\"Get the metadata class for the page nodes in the document.\"\"\"\n        fields = cls.model_fields\n\n        # NOTE: The indexing is important here, and it allows us to get the type of each\n        # page node in the `List` annotation\n        page_nodes_field_class = fields[\"page_nodes\"].annotation.__args__[0]\n\n        # NOTE: The indexing is important here, and relies on the generic type being\n        # the SECOND of the two arguments in the `Union` annotation\n        page_node_metadata_field_annotation = page_nodes_field_class.model_fields[\n            \"metadata\"\n        ].annotation\n\n        if page_node_metadata_field_annotation == PageNodeMetadata:\n            return BaseMetadata\n\n        if isinstance(page_node_metadata_field_annotation, ForwardRef):\n            raise ValueError(\n                \"You cannot define PageNode with a ForwardRef for Generic metadata model types.\"\n            )\n\n        return page_node_metadata_field_annotation\n\n    @property\n    def persistance_path(self):\n        \"\"\"The base path to storage location.\"\"\"\n        return self._persistance_path\n\n    @persistance_path.setter\n    def persistance_path(self, path: str):\n        \"\"\"Set the base path to storage location.\"\"\"\n        self._persistance_path = path\n\n    @classmethod\n    def from_storage(cls, path: str, file_hash: str, **kwargs) -&gt; Self:\n        \"\"\"Load the document node from storage.\n\n        Args:\n            path (str): The base path to storage location.\n                - Example (S3): \"s3://bucket-name/key/to/folder\"\n                - Example (Local FS): \"/tmp/docprompt/storage\"\n            file_hash (str): The hash of the document.\n            **kwargs: Additional keyword arguments for fsspec FileSystem\n\n        Returns:\n            DocumentNode: The loaded document node.\n        \"\"\"\n\n        fs_manager = FileSystemManager(path, **kwargs)\n\n        pdf_bytes, metadata_bytes, page_metadata_bytes = fs_manager.read(\n            file_hash, **kwargs\n        )\n\n        doc = PdfDocument.from_bytes(pdf_bytes, name=fs_manager.get_pdf_name(file_hash))\n        node = cls.from_document(doc)\n\n        if metadata_bytes:\n            metadata_json = json.loads(metadata_bytes.decode(\"utf-8\"))\n            metadata = cls.metadata_class().from_owner(node, **metadata_json)\n        else:\n            metadata = cls.metadata_class().from_owner(node, **{})\n\n        if page_metadata_bytes:\n            page_metadata_json = [\n                json.loads(page_str)\n                for page_str in json.loads(page_metadata_bytes.decode(\"utf-8\"))\n            ]\n            page_metadata = [\n                cls.page_metadata_class()(**page) for page in page_metadata_json\n            ]\n        else:\n            page_metadata = [cls.page_metadata_class()(**{}) for _ in range(len(doc))]\n\n        # Store the metadata on the node and page nodes\n        node.metadata = metadata\n        for page, meta in zip(node.page_nodes, page_metadata):\n            meta.set_owner(page)\n            page.metadata = meta\n\n        # Make sure to set the persistance path on the node\n        node.persistance_path = path\n\n        return node\n\n    def persist(self, path: Optional[str] = None, **kwargs) -&gt; FileSidecarsPathManager:\n        \"\"\"Persist a document node to storage.\n\n        Args:\n            path (Optional[str]): Overwrites the current `persistance_path` property\n                - If `persistance_path` is not currently set, path must be provided.\n            **kwargs: Additional keyword arguments for fsspec FileSystem\n\n        Returns:\n            FileSidecarsPathManager: The file path manager for the persisted document node.\n        \"\"\"\n\n        path = path or self.persistance_path\n\n        if path is None:\n            raise ValueError(\"The path must be provided to persist the document node.\")\n\n        # Make sure to update the persistance path\n        self.persistance_path = path\n\n        fs_manager = FileSystemManager(path, **kwargs)\n\n        pdf_bytes = self.document.get_bytes()\n        metadata_bytes = bytes(self.metadata.model_dump_json(), encoding=\"utf-8\")\n        page_metadata_bytes = bytes(\n            json.dumps([page.metadata.model_dump_json() for page in self.page_nodes]),\n            encoding=\"utf-8\",\n        )\n\n        return fs_manager.write(\n            pdf_bytes, metadata_bytes, page_metadata_bytes, **kwargs\n        )\n</code></pre>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.node.DocumentNode.persistance_path","title":"<code>persistance_path</code>  <code>property</code> <code>writable</code>","text":"<p>The base path to storage location.</p>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.node.DocumentNode.from_storage","title":"<code>from_storage(path, file_hash, **kwargs)</code>  <code>classmethod</code>","text":"<p>Load the document node from storage.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The base path to storage location. - Example (S3): \"s3://bucket-name/key/to/folder\" - Example (Local FS): \"/tmp/docprompt/storage\"</p> required <code>file_hash</code> <code>str</code> <p>The hash of the document.</p> required <code>**kwargs</code> <p>Additional keyword arguments for fsspec FileSystem</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>DocumentNode</code> <code>Self</code> <p>The loaded document node.</p> Source code in <code>docprompt/schema/pipeline/node/document.py</code> <pre><code>@classmethod\ndef from_storage(cls, path: str, file_hash: str, **kwargs) -&gt; Self:\n    \"\"\"Load the document node from storage.\n\n    Args:\n        path (str): The base path to storage location.\n            - Example (S3): \"s3://bucket-name/key/to/folder\"\n            - Example (Local FS): \"/tmp/docprompt/storage\"\n        file_hash (str): The hash of the document.\n        **kwargs: Additional keyword arguments for fsspec FileSystem\n\n    Returns:\n        DocumentNode: The loaded document node.\n    \"\"\"\n\n    fs_manager = FileSystemManager(path, **kwargs)\n\n    pdf_bytes, metadata_bytes, page_metadata_bytes = fs_manager.read(\n        file_hash, **kwargs\n    )\n\n    doc = PdfDocument.from_bytes(pdf_bytes, name=fs_manager.get_pdf_name(file_hash))\n    node = cls.from_document(doc)\n\n    if metadata_bytes:\n        metadata_json = json.loads(metadata_bytes.decode(\"utf-8\"))\n        metadata = cls.metadata_class().from_owner(node, **metadata_json)\n    else:\n        metadata = cls.metadata_class().from_owner(node, **{})\n\n    if page_metadata_bytes:\n        page_metadata_json = [\n            json.loads(page_str)\n            for page_str in json.loads(page_metadata_bytes.decode(\"utf-8\"))\n        ]\n        page_metadata = [\n            cls.page_metadata_class()(**page) for page in page_metadata_json\n        ]\n    else:\n        page_metadata = [cls.page_metadata_class()(**{}) for _ in range(len(doc))]\n\n    # Store the metadata on the node and page nodes\n    node.metadata = metadata\n    for page, meta in zip(node.page_nodes, page_metadata):\n        meta.set_owner(page)\n        page.metadata = meta\n\n    # Make sure to set the persistance path on the node\n    node.persistance_path = path\n\n    return node\n</code></pre>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.node.DocumentNode.metadata_class","title":"<code>metadata_class()</code>  <code>classmethod</code>","text":"<p>Get the metadata class for instantiating metadata from the model.</p> Source code in <code>docprompt/schema/pipeline/node/document.py</code> <pre><code>@classmethod\ndef metadata_class(cls) -&gt; Type[BaseMetadata]:\n    \"\"\"Get the metadata class for instantiating metadata from the model.\"\"\"\n\n    fields = cls.model_fields\n\n    # NOTE: The indexing is important here, and relies on the generic type being\n    # the SECOND of the two arguments in the `Union` annotation\n    metadata_field_annotation = fields[\"metadata\"].annotation\n\n    # If no override has been provided to the metadata model, we want to retrieve\n    # it as a TypedDict\n    if metadata_field_annotation == DocumentNodeMetadata:\n        return BaseMetadata\n\n    if isinstance(metadata_field_annotation, ForwardRef):\n        raise ValueError(\n            \"You cannot define DocumentNode with a ForwardRef for Generic metadata model types.\"\n        )\n\n    # Get the overriden Generic type of th DocumentNodeMetadata\n    return metadata_field_annotation\n</code></pre>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.node.DocumentNode.page_metadata_class","title":"<code>page_metadata_class()</code>  <code>classmethod</code>","text":"<p>Get the metadata class for the page nodes in the document.</p> Source code in <code>docprompt/schema/pipeline/node/document.py</code> <pre><code>@classmethod\ndef page_metadata_class(cls) -&gt; Type[Union[dict, BaseModel]]:\n    \"\"\"Get the metadata class for the page nodes in the document.\"\"\"\n    fields = cls.model_fields\n\n    # NOTE: The indexing is important here, and it allows us to get the type of each\n    # page node in the `List` annotation\n    page_nodes_field_class = fields[\"page_nodes\"].annotation.__args__[0]\n\n    # NOTE: The indexing is important here, and relies on the generic type being\n    # the SECOND of the two arguments in the `Union` annotation\n    page_node_metadata_field_annotation = page_nodes_field_class.model_fields[\n        \"metadata\"\n    ].annotation\n\n    if page_node_metadata_field_annotation == PageNodeMetadata:\n        return BaseMetadata\n\n    if isinstance(page_node_metadata_field_annotation, ForwardRef):\n        raise ValueError(\n            \"You cannot define PageNode with a ForwardRef for Generic metadata model types.\"\n        )\n\n    return page_node_metadata_field_annotation\n</code></pre>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.node.DocumentNode.persist","title":"<code>persist(path=None, **kwargs)</code>","text":"<p>Persist a document node to storage.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Optional[str]</code> <p>Overwrites the current <code>persistance_path</code> property - If <code>persistance_path</code> is not currently set, path must be provided.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments for fsspec FileSystem</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>FileSidecarsPathManager</code> <code>FileSidecarsPathManager</code> <p>The file path manager for the persisted document node.</p> Source code in <code>docprompt/schema/pipeline/node/document.py</code> <pre><code>def persist(self, path: Optional[str] = None, **kwargs) -&gt; FileSidecarsPathManager:\n    \"\"\"Persist a document node to storage.\n\n    Args:\n        path (Optional[str]): Overwrites the current `persistance_path` property\n            - If `persistance_path` is not currently set, path must be provided.\n        **kwargs: Additional keyword arguments for fsspec FileSystem\n\n    Returns:\n        FileSidecarsPathManager: The file path manager for the persisted document node.\n    \"\"\"\n\n    path = path or self.persistance_path\n\n    if path is None:\n        raise ValueError(\"The path must be provided to persist the document node.\")\n\n    # Make sure to update the persistance path\n    self.persistance_path = path\n\n    fs_manager = FileSystemManager(path, **kwargs)\n\n    pdf_bytes = self.document.get_bytes()\n    metadata_bytes = bytes(self.metadata.model_dump_json(), encoding=\"utf-8\")\n    page_metadata_bytes = bytes(\n        json.dumps([page.metadata.model_dump_json() for page in self.page_nodes]),\n        encoding=\"utf-8\",\n    )\n\n    return fs_manager.write(\n        pdf_bytes, metadata_bytes, page_metadata_bytes, **kwargs\n    )\n</code></pre>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.node.DocumentNode.refresh_locator","title":"<code>refresh_locator()</code>","text":"<p>Refreshes the locator for this document node</p> Source code in <code>docprompt/schema/pipeline/node/document.py</code> <pre><code>def refresh_locator(self):\n    \"\"\"\n    Refreshes the locator for this document node\n    \"\"\"\n    from docprompt.provenance.search import DocumentProvenanceLocator\n\n    if any(not page.ocr_results.result for page in self.page_nodes):\n        raise ValueError(\n            \"Cannot create a locator for a document node with missing OCR results\"\n        )\n\n    self._locator = DocumentProvenanceLocator.from_document_node(self)\n\n    return self.locator\n</code></pre>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.node.PageNode","title":"<code>PageNode</code>","text":"<p>               Bases: <code>BaseNode</code>, <code>Generic[PageNodeMetadata]</code></p> <p>Represents a single page in a document, with some metadata</p> Source code in <code>docprompt/schema/pipeline/node/page.py</code> <pre><code>class PageNode(BaseNode, Generic[PageNodeMetadata]):\n    \"\"\"\n    Represents a single page in a document, with some metadata\n    \"\"\"\n\n    document: \"DocumentNode\" = Field(exclude=True, repr=False)\n    page_number: PositiveInt = Field(description=\"The page number\")\n    metadata: PageNodeMetadata = Field(\n        description=\"Application-specific metadata for the page\",\n        default_factory=BaseMetadata,\n    )\n    extra: Dict[str, Any] = Field(\n        description=\"Extra data that can be stored on the page node\",\n        default_factory=dict,\n    )\n\n    ocr_results: ResultContainer[OcrPageResult] = Field(\n        default_factory=_result_container_factory,\n        description=\"The OCR results for the page\",\n        repr=False,\n    )\n\n    _raster_cache: Dict[str, bytes] = PrivateAttr(default_factory=dict)\n\n    def __getstate__(self):\n        state = super().__getstate__()\n\n        state[\"__pydantic_private__\"][\"_raster_cache\"] = {}\n\n        return state\n\n    @property\n    def rasterizer(self):\n        return PageRasterizer(self._raster_cache, self)\n\n    def search(\n        self, query: str, refine_to_words: bool = True, require_exact_match: bool = True\n    ):\n        return self.document.locator.search(\n            query,\n            page_number=self.page_number,\n            refine_to_word=refine_to_words,\n            require_exact_match=require_exact_match,\n        )\n</code></pre>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.node.base","title":"<code>base</code>","text":""},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.node.base.BaseNode","title":"<code>BaseNode</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>The base node class is utilized for defining a basic yet flexible interface</p> Source code in <code>docprompt/schema/pipeline/node/base.py</code> <pre><code>class BaseNode(BaseModel):\n    \"\"\"The base node class is utilized for defining a basic yet flexible interface\"\"\"\n</code></pre>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.node.collection","title":"<code>collection</code>","text":""},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.node.collection.DocumentCollection","title":"<code>DocumentCollection</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[DocumentCollectionMetadata, DocumentNodeMetadata, PageNodeMetadata]</code></p> <p>Represents a collection of documents with some common metadata</p> Source code in <code>docprompt/schema/pipeline/node/collection.py</code> <pre><code>class DocumentCollection(\n    BaseModel,\n    Generic[DocumentCollectionMetadata, DocumentNodeMetadata, PageNodeMetadata],\n):\n    \"\"\"\n    Represents a collection of documents with some common metadata\n    \"\"\"\n\n    document_nodes: List[\"DocumentNode[DocumentNodeMetadata, PageNodeMetadata]\"]\n    metadata: DocumentCollectionMetadata = Field(..., default_factory=dict)\n</code></pre>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.node.document","title":"<code>document</code>","text":""},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.node.document.DocumentNode","title":"<code>DocumentNode</code>","text":"<p>               Bases: <code>BaseNode</code>, <code>Generic[DocumentNodeMetadata, PageNodeMetadata]</code></p> <p>Represents a single document, with some metadata</p> Source code in <code>docprompt/schema/pipeline/node/document.py</code> <pre><code>class DocumentNode(BaseNode, Generic[DocumentNodeMetadata, PageNodeMetadata]):\n    \"\"\"\n    Represents a single document, with some metadata\n    \"\"\"\n\n    document: PdfDocument\n    page_nodes: List[PageNode[PageNodeMetadata]] = Field(\n        description=\"The pages in the document\", default_factory=list, repr=False\n    )\n    metadata: DocumentNodeMetadata = Field(\n        description=\"Application-specific metadata for the document\",\n        default_factory=BaseMetadata,\n    )\n\n    _locator: Optional[\"DocumentProvenanceLocator\"] = PrivateAttr(default=None)\n\n    _persistance_path: Optional[str] = PrivateAttr(default=None)\n\n    def __getstate__(self):\n        state = super().__getstate__()\n\n        state[\"__pydantic_private__\"][\"_locator\"] = None\n\n        return state\n\n    def __len__(self):\n        return len(self.page_nodes)\n\n    def __getitem__(self, index):\n        return self.page_nodes[index]\n\n    def __iter__(self):\n        return iter(self.page_nodes)\n\n    @property\n    def rasterizer(self):\n        return DocumentRasterizer(self)\n\n    @property\n    def locator(self):\n        if self._locator is None:\n            self.refresh_locator()\n\n        return self._locator\n\n    def refresh_locator(self):\n        \"\"\"\n        Refreshes the locator for this document node\n        \"\"\"\n        from docprompt.provenance.search import DocumentProvenanceLocator\n\n        if any(not page.ocr_results.result for page in self.page_nodes):\n            raise ValueError(\n                \"Cannot create a locator for a document node with missing OCR results\"\n            )\n\n        self._locator = DocumentProvenanceLocator.from_document_node(self)\n\n        return self.locator\n\n    @classmethod\n    def from_document(\n        cls,\n        document: PdfDocument,\n        document_metadata: Optional[DocumentNodeMetadata] = None,\n        page_metadata: Optional[List[PageNodeMetadata]] = None,\n    ):\n        document_node: \"DocumentNode[DocumentNodeMetadata, PageNodeMetadata]\" = cls(\n            document=document,\n        )\n        document_node.metadata = document_metadata or cls.metadata_class().from_owner(\n            document_node, **{}\n        )\n\n        if page_metadata is not None and len(page_metadata) != len(document):\n            raise ValueError(\n                \"The number of page metadata items must match the number of pages in the document.\"\n            )\n\n        for page_number in range(1, len(document) + 1):\n            if page_metadata is not None:\n                page_node = PageNode(\n                    document=document_node,\n                    page_number=page_number,\n                    metadata=page_metadata[page_number - 1],\n                )\n            else:\n                page_node = PageNode(document=document_node, page_number=page_number)\n\n            document_node.page_nodes.append(page_node)\n\n        return document_node\n\n    @property\n    def file_hash(self):\n        return self.document.document_hash\n\n    @property\n    def document_name(self):\n        return self.document.name\n\n    @classmethod\n    def metadata_class(cls) -&gt; Type[BaseMetadata]:\n        \"\"\"Get the metadata class for instantiating metadata from the model.\"\"\"\n\n        fields = cls.model_fields\n\n        # NOTE: The indexing is important here, and relies on the generic type being\n        # the SECOND of the two arguments in the `Union` annotation\n        metadata_field_annotation = fields[\"metadata\"].annotation\n\n        # If no override has been provided to the metadata model, we want to retrieve\n        # it as a TypedDict\n        if metadata_field_annotation == DocumentNodeMetadata:\n            return BaseMetadata\n\n        if isinstance(metadata_field_annotation, ForwardRef):\n            raise ValueError(\n                \"You cannot define DocumentNode with a ForwardRef for Generic metadata model types.\"\n            )\n\n        # Get the overriden Generic type of th DocumentNodeMetadata\n        return metadata_field_annotation\n\n    @classmethod\n    def page_metadata_class(cls) -&gt; Type[Union[dict, BaseModel]]:\n        \"\"\"Get the metadata class for the page nodes in the document.\"\"\"\n        fields = cls.model_fields\n\n        # NOTE: The indexing is important here, and it allows us to get the type of each\n        # page node in the `List` annotation\n        page_nodes_field_class = fields[\"page_nodes\"].annotation.__args__[0]\n\n        # NOTE: The indexing is important here, and relies on the generic type being\n        # the SECOND of the two arguments in the `Union` annotation\n        page_node_metadata_field_annotation = page_nodes_field_class.model_fields[\n            \"metadata\"\n        ].annotation\n\n        if page_node_metadata_field_annotation == PageNodeMetadata:\n            return BaseMetadata\n\n        if isinstance(page_node_metadata_field_annotation, ForwardRef):\n            raise ValueError(\n                \"You cannot define PageNode with a ForwardRef for Generic metadata model types.\"\n            )\n\n        return page_node_metadata_field_annotation\n\n    @property\n    def persistance_path(self):\n        \"\"\"The base path to storage location.\"\"\"\n        return self._persistance_path\n\n    @persistance_path.setter\n    def persistance_path(self, path: str):\n        \"\"\"Set the base path to storage location.\"\"\"\n        self._persistance_path = path\n\n    @classmethod\n    def from_storage(cls, path: str, file_hash: str, **kwargs) -&gt; Self:\n        \"\"\"Load the document node from storage.\n\n        Args:\n            path (str): The base path to storage location.\n                - Example (S3): \"s3://bucket-name/key/to/folder\"\n                - Example (Local FS): \"/tmp/docprompt/storage\"\n            file_hash (str): The hash of the document.\n            **kwargs: Additional keyword arguments for fsspec FileSystem\n\n        Returns:\n            DocumentNode: The loaded document node.\n        \"\"\"\n\n        fs_manager = FileSystemManager(path, **kwargs)\n\n        pdf_bytes, metadata_bytes, page_metadata_bytes = fs_manager.read(\n            file_hash, **kwargs\n        )\n\n        doc = PdfDocument.from_bytes(pdf_bytes, name=fs_manager.get_pdf_name(file_hash))\n        node = cls.from_document(doc)\n\n        if metadata_bytes:\n            metadata_json = json.loads(metadata_bytes.decode(\"utf-8\"))\n            metadata = cls.metadata_class().from_owner(node, **metadata_json)\n        else:\n            metadata = cls.metadata_class().from_owner(node, **{})\n\n        if page_metadata_bytes:\n            page_metadata_json = [\n                json.loads(page_str)\n                for page_str in json.loads(page_metadata_bytes.decode(\"utf-8\"))\n            ]\n            page_metadata = [\n                cls.page_metadata_class()(**page) for page in page_metadata_json\n            ]\n        else:\n            page_metadata = [cls.page_metadata_class()(**{}) for _ in range(len(doc))]\n\n        # Store the metadata on the node and page nodes\n        node.metadata = metadata\n        for page, meta in zip(node.page_nodes, page_metadata):\n            meta.set_owner(page)\n            page.metadata = meta\n\n        # Make sure to set the persistance path on the node\n        node.persistance_path = path\n\n        return node\n\n    def persist(self, path: Optional[str] = None, **kwargs) -&gt; FileSidecarsPathManager:\n        \"\"\"Persist a document node to storage.\n\n        Args:\n            path (Optional[str]): Overwrites the current `persistance_path` property\n                - If `persistance_path` is not currently set, path must be provided.\n            **kwargs: Additional keyword arguments for fsspec FileSystem\n\n        Returns:\n            FileSidecarsPathManager: The file path manager for the persisted document node.\n        \"\"\"\n\n        path = path or self.persistance_path\n\n        if path is None:\n            raise ValueError(\"The path must be provided to persist the document node.\")\n\n        # Make sure to update the persistance path\n        self.persistance_path = path\n\n        fs_manager = FileSystemManager(path, **kwargs)\n\n        pdf_bytes = self.document.get_bytes()\n        metadata_bytes = bytes(self.metadata.model_dump_json(), encoding=\"utf-8\")\n        page_metadata_bytes = bytes(\n            json.dumps([page.metadata.model_dump_json() for page in self.page_nodes]),\n            encoding=\"utf-8\",\n        )\n\n        return fs_manager.write(\n            pdf_bytes, metadata_bytes, page_metadata_bytes, **kwargs\n        )\n</code></pre>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.node.document.DocumentNode.persistance_path","title":"<code>persistance_path</code>  <code>property</code> <code>writable</code>","text":"<p>The base path to storage location.</p>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.node.document.DocumentNode.from_storage","title":"<code>from_storage(path, file_hash, **kwargs)</code>  <code>classmethod</code>","text":"<p>Load the document node from storage.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The base path to storage location. - Example (S3): \"s3://bucket-name/key/to/folder\" - Example (Local FS): \"/tmp/docprompt/storage\"</p> required <code>file_hash</code> <code>str</code> <p>The hash of the document.</p> required <code>**kwargs</code> <p>Additional keyword arguments for fsspec FileSystem</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>DocumentNode</code> <code>Self</code> <p>The loaded document node.</p> Source code in <code>docprompt/schema/pipeline/node/document.py</code> <pre><code>@classmethod\ndef from_storage(cls, path: str, file_hash: str, **kwargs) -&gt; Self:\n    \"\"\"Load the document node from storage.\n\n    Args:\n        path (str): The base path to storage location.\n            - Example (S3): \"s3://bucket-name/key/to/folder\"\n            - Example (Local FS): \"/tmp/docprompt/storage\"\n        file_hash (str): The hash of the document.\n        **kwargs: Additional keyword arguments for fsspec FileSystem\n\n    Returns:\n        DocumentNode: The loaded document node.\n    \"\"\"\n\n    fs_manager = FileSystemManager(path, **kwargs)\n\n    pdf_bytes, metadata_bytes, page_metadata_bytes = fs_manager.read(\n        file_hash, **kwargs\n    )\n\n    doc = PdfDocument.from_bytes(pdf_bytes, name=fs_manager.get_pdf_name(file_hash))\n    node = cls.from_document(doc)\n\n    if metadata_bytes:\n        metadata_json = json.loads(metadata_bytes.decode(\"utf-8\"))\n        metadata = cls.metadata_class().from_owner(node, **metadata_json)\n    else:\n        metadata = cls.metadata_class().from_owner(node, **{})\n\n    if page_metadata_bytes:\n        page_metadata_json = [\n            json.loads(page_str)\n            for page_str in json.loads(page_metadata_bytes.decode(\"utf-8\"))\n        ]\n        page_metadata = [\n            cls.page_metadata_class()(**page) for page in page_metadata_json\n        ]\n    else:\n        page_metadata = [cls.page_metadata_class()(**{}) for _ in range(len(doc))]\n\n    # Store the metadata on the node and page nodes\n    node.metadata = metadata\n    for page, meta in zip(node.page_nodes, page_metadata):\n        meta.set_owner(page)\n        page.metadata = meta\n\n    # Make sure to set the persistance path on the node\n    node.persistance_path = path\n\n    return node\n</code></pre>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.node.document.DocumentNode.metadata_class","title":"<code>metadata_class()</code>  <code>classmethod</code>","text":"<p>Get the metadata class for instantiating metadata from the model.</p> Source code in <code>docprompt/schema/pipeline/node/document.py</code> <pre><code>@classmethod\ndef metadata_class(cls) -&gt; Type[BaseMetadata]:\n    \"\"\"Get the metadata class for instantiating metadata from the model.\"\"\"\n\n    fields = cls.model_fields\n\n    # NOTE: The indexing is important here, and relies on the generic type being\n    # the SECOND of the two arguments in the `Union` annotation\n    metadata_field_annotation = fields[\"metadata\"].annotation\n\n    # If no override has been provided to the metadata model, we want to retrieve\n    # it as a TypedDict\n    if metadata_field_annotation == DocumentNodeMetadata:\n        return BaseMetadata\n\n    if isinstance(metadata_field_annotation, ForwardRef):\n        raise ValueError(\n            \"You cannot define DocumentNode with a ForwardRef for Generic metadata model types.\"\n        )\n\n    # Get the overriden Generic type of th DocumentNodeMetadata\n    return metadata_field_annotation\n</code></pre>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.node.document.DocumentNode.page_metadata_class","title":"<code>page_metadata_class()</code>  <code>classmethod</code>","text":"<p>Get the metadata class for the page nodes in the document.</p> Source code in <code>docprompt/schema/pipeline/node/document.py</code> <pre><code>@classmethod\ndef page_metadata_class(cls) -&gt; Type[Union[dict, BaseModel]]:\n    \"\"\"Get the metadata class for the page nodes in the document.\"\"\"\n    fields = cls.model_fields\n\n    # NOTE: The indexing is important here, and it allows us to get the type of each\n    # page node in the `List` annotation\n    page_nodes_field_class = fields[\"page_nodes\"].annotation.__args__[0]\n\n    # NOTE: The indexing is important here, and relies on the generic type being\n    # the SECOND of the two arguments in the `Union` annotation\n    page_node_metadata_field_annotation = page_nodes_field_class.model_fields[\n        \"metadata\"\n    ].annotation\n\n    if page_node_metadata_field_annotation == PageNodeMetadata:\n        return BaseMetadata\n\n    if isinstance(page_node_metadata_field_annotation, ForwardRef):\n        raise ValueError(\n            \"You cannot define PageNode with a ForwardRef for Generic metadata model types.\"\n        )\n\n    return page_node_metadata_field_annotation\n</code></pre>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.node.document.DocumentNode.persist","title":"<code>persist(path=None, **kwargs)</code>","text":"<p>Persist a document node to storage.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Optional[str]</code> <p>Overwrites the current <code>persistance_path</code> property - If <code>persistance_path</code> is not currently set, path must be provided.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments for fsspec FileSystem</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>FileSidecarsPathManager</code> <code>FileSidecarsPathManager</code> <p>The file path manager for the persisted document node.</p> Source code in <code>docprompt/schema/pipeline/node/document.py</code> <pre><code>def persist(self, path: Optional[str] = None, **kwargs) -&gt; FileSidecarsPathManager:\n    \"\"\"Persist a document node to storage.\n\n    Args:\n        path (Optional[str]): Overwrites the current `persistance_path` property\n            - If `persistance_path` is not currently set, path must be provided.\n        **kwargs: Additional keyword arguments for fsspec FileSystem\n\n    Returns:\n        FileSidecarsPathManager: The file path manager for the persisted document node.\n    \"\"\"\n\n    path = path or self.persistance_path\n\n    if path is None:\n        raise ValueError(\"The path must be provided to persist the document node.\")\n\n    # Make sure to update the persistance path\n    self.persistance_path = path\n\n    fs_manager = FileSystemManager(path, **kwargs)\n\n    pdf_bytes = self.document.get_bytes()\n    metadata_bytes = bytes(self.metadata.model_dump_json(), encoding=\"utf-8\")\n    page_metadata_bytes = bytes(\n        json.dumps([page.metadata.model_dump_json() for page in self.page_nodes]),\n        encoding=\"utf-8\",\n    )\n\n    return fs_manager.write(\n        pdf_bytes, metadata_bytes, page_metadata_bytes, **kwargs\n    )\n</code></pre>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.node.document.DocumentNode.refresh_locator","title":"<code>refresh_locator()</code>","text":"<p>Refreshes the locator for this document node</p> Source code in <code>docprompt/schema/pipeline/node/document.py</code> <pre><code>def refresh_locator(self):\n    \"\"\"\n    Refreshes the locator for this document node\n    \"\"\"\n    from docprompt.provenance.search import DocumentProvenanceLocator\n\n    if any(not page.ocr_results.result for page in self.page_nodes):\n        raise ValueError(\n            \"Cannot create a locator for a document node with missing OCR results\"\n        )\n\n    self._locator = DocumentProvenanceLocator.from_document_node(self)\n\n    return self.locator\n</code></pre>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.node.page","title":"<code>page</code>","text":""},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.node.page.PageNode","title":"<code>PageNode</code>","text":"<p>               Bases: <code>BaseNode</code>, <code>Generic[PageNodeMetadata]</code></p> <p>Represents a single page in a document, with some metadata</p> Source code in <code>docprompt/schema/pipeline/node/page.py</code> <pre><code>class PageNode(BaseNode, Generic[PageNodeMetadata]):\n    \"\"\"\n    Represents a single page in a document, with some metadata\n    \"\"\"\n\n    document: \"DocumentNode\" = Field(exclude=True, repr=False)\n    page_number: PositiveInt = Field(description=\"The page number\")\n    metadata: PageNodeMetadata = Field(\n        description=\"Application-specific metadata for the page\",\n        default_factory=BaseMetadata,\n    )\n    extra: Dict[str, Any] = Field(\n        description=\"Extra data that can be stored on the page node\",\n        default_factory=dict,\n    )\n\n    ocr_results: ResultContainer[OcrPageResult] = Field(\n        default_factory=_result_container_factory,\n        description=\"The OCR results for the page\",\n        repr=False,\n    )\n\n    _raster_cache: Dict[str, bytes] = PrivateAttr(default_factory=dict)\n\n    def __getstate__(self):\n        state = super().__getstate__()\n\n        state[\"__pydantic_private__\"][\"_raster_cache\"] = {}\n\n        return state\n\n    @property\n    def rasterizer(self):\n        return PageRasterizer(self._raster_cache, self)\n\n    def search(\n        self, query: str, refine_to_words: bool = True, require_exact_match: bool = True\n    ):\n        return self.document.locator.search(\n            query,\n            page_number=self.page_number,\n            refine_to_word=refine_to_words,\n            require_exact_match=require_exact_match,\n        )\n</code></pre>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.node.typing","title":"<code>typing</code>","text":""},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.rasterizer","title":"<code>rasterizer</code>","text":""},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.rasterizer.DocumentRasterizer","title":"<code>DocumentRasterizer</code>","text":"Source code in <code>docprompt/schema/pipeline/rasterizer.py</code> <pre><code>class DocumentRasterizer:\n    def __init__(self, owner: \"DocumentNode\"):\n        self.owner = owner\n\n    def rasterize(\n        self,\n        name: str,\n        *,\n        return_mode: Literal[\"bytes\", \"pil\"] = \"bytes\",\n        dpi: int = 100,\n        downscale_size: Optional[Tuple[int, int]] = None,\n        resize_mode: ResizeModes = \"thumbnail\",\n        resize_aspect_ratios: Optional[Iterable[AspectRatioRule]] = None,\n        do_convert: bool = False,\n        image_convert_mode: str = \"L\",\n        do_quantize: bool = False,\n        quantize_color_count: int = 8,\n        max_file_size_bytes: Optional[int] = None,\n        render_grayscale: bool = False,\n    ) -&gt; List[Union[bytes, Image.Image]]:\n        images = self.owner.document.rasterize_pdf(\n            dpi=dpi,\n            downscale_size=downscale_size,\n            resize_mode=resize_mode,\n            resize_aspect_ratios=resize_aspect_ratios,\n            do_convert=do_convert,\n            image_convert_mode=image_convert_mode,\n            do_quantize=do_quantize,\n            quantize_color_count=quantize_color_count,\n            max_file_size_bytes=max_file_size_bytes,\n            render_grayscale=render_grayscale,\n            return_mode=return_mode,\n        )\n\n        for page_number, image in images.items():\n            page_node = self.owner.page_nodes[page_number - 1]\n\n            page_node._raster_cache[name] = image\n\n        return list(images.values())\n\n    def propagate_cache(self, name: str, rasters: Dict[int, Union[bytes, Image.Image]]):\n        \"\"\"\n        Should be one-indexed\n        \"\"\"\n        for page_number, raster in rasters.items():\n            page_node = self.owner.page_nodes[page_number - 1]\n\n            page_node._raster_cache[name] = raster\n</code></pre>"},{"location":"reference/schema/pipeline/#docprompt.schema.pipeline.rasterizer.DocumentRasterizer.propagate_cache","title":"<code>propagate_cache(name, rasters)</code>","text":"<p>Should be one-indexed</p> Source code in <code>docprompt/schema/pipeline/rasterizer.py</code> <pre><code>def propagate_cache(self, name: str, rasters: Dict[int, Union[bytes, Image.Image]]):\n    \"\"\"\n    Should be one-indexed\n    \"\"\"\n    for page_number, raster in rasters.items():\n        page_node = self.owner.page_nodes[page_number - 1]\n\n        page_node._raster_cache[name] = raster\n</code></pre>"},{"location":"reference/schema/pipeline/metadata/","title":"metadata","text":"<p>The metadata class is utilized for defining a basic, yet flexible interface for metadata attached to various fields.</p> <p>In essence, this allows for developers to choose to either create their metadtata in an unstructured manner (i.e. a dictionary), or to sub class the base metadata class in order to create a more strictly typed metadata model for their page and document nodes.</p>"},{"location":"reference/schema/pipeline/metadata/#docprompt.schema.pipeline.metadata.BaseMetadata","title":"<code>BaseMetadata</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>MutableMapping</code>, <code>Generic[TMetadataOwner]</code></p> <p>The base metadata class is utilized for defining a basic yet flexible interface for metadata attached to various fields.</p> The metadata class can be used in two ways <ol> <li>As a dictionary-like object, where metadata is stored in the <code>extra</code> field.</li> <li>As a sub-classed model, where metadata is stored in the fields of the model.</li> </ol> <p>When used out of the box, the metadata class will adobpt dictionary-like behavior. You may easily access different fields of the metadata as if it were a dictionary: <pre><code># Instantiate it with any kwargs you like\nmetadata = BaseMetadata(foo-'bar', cow='moo')\n\nmetadata[\"foo\"]  # \"bar\"\nmetadata[\"cow\"]  # \"moo\"\n\n# Update the value of the key\nmetadata[\"foo\"] = \"fighters\"\n\n# Set new key-value pairs\nmetadata['sheep'] = 'baa'\n</code></pre></p> <p>Otherwise, you may sub-class the metadata class in order to create a more strictly typed metadata model. This is useful when you want to enforce a specific structure for your metadata.</p> <pre><code>class CustomMetadata(BaseMetadata):\n    foo: str\n    cow: str\n\n# Instantiate it with the required fields\nmetadata = CustomMetadata(foo='bar', cow='moo')\n\nmetadata.foo  # \"bar\"\nmetadata.cow  # \"moo\"\n\n# Update the value of the key\nmetadata.foo = \"fighters\"\n\n# Use the extra field to store dynamic metadata\nmetadata.extra['sheep'] = 'baa'\n</code></pre> <p>Additionally, the task results descriptor allows for controlled and easy access to the task results of various tasks that are run on the parent node.</p> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>class BaseMetadata(BaseModel, MutableMapping, Generic[TMetadataOwner]):\n    \"\"\"\n    The base metadata class is utilized for defining a basic yet flexible interface\n    for metadata attached to various fields.\n\n    The metadata class can be used in two ways:\n        1. As a dictionary-like object, where metadata is stored in the `extra` field.\n        2. As a sub-classed model, where metadata is stored in the fields of the model.\n\n    When used out of the box, the metadata class will adobpt dictionary-like behavior. You\n    may easily access different fields of the metadata as if it were a dictionary:\n    ```python\n    # Instantiate it with any kwargs you like\n    metadata = BaseMetadata(foo-'bar', cow='moo')\n\n    metadata[\"foo\"]  # \"bar\"\n    metadata[\"cow\"]  # \"moo\"\n\n    # Update the value of the key\n    metadata[\"foo\"] = \"fighters\"\n\n    # Set new key-value pairs\n    metadata['sheep'] = 'baa'\n    ```\n\n    Otherwise, you may sub-class the metadata class in order to create a more strictly typed\n    metadata model. This is useful when you want to enforce a specific structure for your metadata.\n\n    ```python\n    class CustomMetadata(BaseMetadata):\n        foo: str\n        cow: str\n\n    # Instantiate it with the required fields\n    metadata = CustomMetadata(foo='bar', cow='moo')\n\n    metadata.foo  # \"bar\"\n    metadata.cow  # \"moo\"\n\n    # Update the value of the key\n    metadata.foo = \"fighters\"\n\n    # Use the extra field to store dynamic metadata\n    metadata.extra['sheep'] = 'baa'\n    ```\n\n    Additionally, the task results descriptor allows for controlled and easy access to the task results\n    of various tasks that are run on the parent node.\n    \"\"\"\n\n    extra: Dict[str, Any] = Field(..., default_factory=dict, repr=False)\n\n    _task_results: TaskResultsDescriptor = PrivateAttr(\n        default_factory=TaskResultsDescriptor\n    )\n\n    _owner: TMetadataOwner = PrivateAttr()\n\n    @property\n    def task_results(self) -&gt; TaskResultsDescriptor:\n        \"\"\"Return the task results descriptor.\"\"\"\n        return self._task_results.__get__(self)\n\n    @task_results.setter\n    def task_results(self, value: Any) -&gt; None:\n        \"\"\"This will raise an error, as we do not want to set the task results directly.\n\n        NOTE: This implementation is here purely to avoid the task_results property from being\n        overwritten by accident.\n        \"\"\"\n        self._task_results.__set__(self, value)\n\n    @property\n    def owner(self) -&gt; TMetadataOwner:\n        \"\"\"Return the owner of the metadata.\"\"\"\n        return self._owner\n\n    @owner.setter\n    def owner(self, owner: TMetadataOwner) -&gt; None:\n        \"\"\"Return the owner of the metadata.\"\"\"\n        self._owner = owner\n\n    @classmethod\n    def from_owner(cls, owner: TMetadataOwner, **data) -&gt; BaseMetadata:\n        \"\"\"Create a new instance of the metadata class with the owner set.\"\"\"\n        metadata = cls(**data)\n        metadata.owner = owner\n        return metadata\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def validate_data_fields_from_annotations(cls, data: Any) -&gt; Any:\n        \"\"\"Validate the data fields from the annotations.\"\"\"\n\n        # We want to make sure that we combine the `extra` metdata along with any\n        # other specific fields that are defined in the metadata.\n        extra = data.pop(\"extra\", {})\n        assert isinstance(extra, dict), \"The `extra` field must be a dictionary.\"\n        data = {**data, **extra}\n\n        # If the model has been sub-classed, then all of our fields must be\n        # validated by the pydantic model.\n        if cls._is_field_typed():\n            # We will get the fields out of extra and set them as potential fields to\n            # validate. They will be ignored if they are not defined in the model, but it\n            # allows for a more flexible way to define metadata.\n            # Otherwise, what ever is in the `extra` field will be stroed in the `extra` field.\n            return {**data, \"extra\": extra}\n\n        # Otherwise, we are using our mock-dict implentation, so we store our\n        # metadata in the `extra` field.\n        return {\"extra\": data}\n\n    @classmethod\n    def _is_field_typed(cls):\n        \"\"\"\n        Check if the metadata model is field typed.\n\n        This is used to determine if the metadata model is a dictionary-like model,\n        or a more strictly typed model.\n        \"\"\"\n        if set([\"extra\"]) != set(cls.model_fields.keys()):\n            return True\n\n        return False\n\n    def __repr__(self):\n        \"\"\"\n        Provide a string representation of the metadata.\n\n        This only works for the base metadata model. If sub-classed, this will raise an error,\n        unless overridden, as BaseModel's do not have a __repr__ method.\n        \"\"\"\n        if self._is_field_typed():\n            return super().__repr__()\n\n        # Otherwise, we are deailing with dictornary-like metadata\n        return json.dumps(self.extra)\n\n    def __getitem__(self, name):\n        \"\"\"\n        Provide dictionary functionlaity to the metadata class.\n\n        This only works for the base metadata model. If sub-classed, this will raise an error,\n        unless overridden, as BaseModel's do not have an __getitem__ method.\n        \"\"\"\n        if not self._is_field_typed():\n            return self.extra[name]\n\n        raise AttributeError(\n            f\"'{self.__class__.__name__}' object has no attribute '{name}'\"\n        )\n\n    def __setitem__(self, name, value):\n        \"\"\"\n        Provide dictionary functionality to the metadata class.\n\n        This only works for the base metadata model. If sub-classed, this will raise an error,\n        unless overridden, as BaseModel's do not have an __setitem__ method.\n        \"\"\"\n        if not self._is_field_typed():\n            self.extra[name] = value\n        else:\n            raise AttributeError(\n                f\"'{self.__class__.__name__}' object has no attribute '{name}'\"\n            )\n\n    def __delitem__(self, name):\n        \"\"\"\n        Provide dictionary functionality to the metadata class.\n\n        This only works for the base metadata model. If sub-classed, this will raise an error,\n        unless overridden, as BaseModel's do not have an __delitem__ method.\n        \"\"\"\n        if not self._is_field_typed():\n            del self.extra[name]\n        else:\n            raise AttributeError(\n                f\"'{self.__class__.__name__}' object has no attribute '{name}'\"\n            )\n\n    def __iter__(self):\n        \"\"\"\n        Iterate over the keys in the metadata.\n\n        This only works for the base metadata model. If sub-classed, this will raise an error,\n        unless overridden, as BaseModel's do not have an __iter__ method.\n        \"\"\"\n        if self._is_field_typed():\n            raise AttributeError(f\"'{self.__class__.__name__}' object is not iterable\")\n\n        return iter(self.extra)\n\n    def __len__(self):\n        \"\"\"\n        Get the number of keys in the metadata.\n\n        This only works for the base metadata model. If sub-classed, this will raise an error,\n        unless overridden, as BaseModel's do not have a __len__ method.\n        \"\"\"\n        if self._is_field_typed():\n            raise AttributeError(\n                f\"'{self.__class__.__name__}' object has no attribute '__len__'\"\n            )\n\n        return len(self.extra)\n\n    def __getattr__(self, name):\n        \"\"\"\n        Allow for getting of attributes on the metadata class.\n\n        The attributes are retrieved through the following heirarchy:\n            - If the model is sub-classed, it will be retrieved as normal.\n            - Otherwise, if the attribute is private, it will be retrieved as normal.\n            - Finally, if we are getting a public attribute on the base metadata class,\n                we use the extra field.\n            - If the key is not set in the `extra` dictionary, we resort back to just\n            trying to get the field.\n                - This is when we grab the `owner` or `task_result` attribuite.\n        \"\"\"\n        if self._is_field_typed():\n            return super().__getattr__(name)\n\n        if name.startswith(\"_\"):\n            return super().__getattr__(name)\n\n        # Attempt to retreieve the attr from the `extra` field\n        try:\n            return self.extra.get(name)\n\n        except KeyError:\n            # This is for grabbing properties on the base metadata class\n            return super().__getattr__(name)\n\n    def __setattr__(self, name: str, value: Any) -&gt; None:\n        \"\"\"\n        Allow for setting of attributes on the metadata class.\n\n        The attributes are set through the following heirarchy:\n            - If the model is sub-classed, it will be set as normal.\n            - Otherwise, if the attribute is private, it will be set as normal.\n            - Finally, if we are setting a public attribute on the base metadata class,\n                we use the extra field.\n        \"\"\"\n        if self._is_field_typed():\n            return super().__setattr__(name, value)\n\n        # We want to avoid setting any private attributes in the extra\n        # dictionary\n        if name.startswith(\"_\"):\n            return super().__setattr__(name, value)\n\n        # If it is `owner` or `task_results`, we want\n        # to avoid setting the attribute in the `extra` dictionary\n        if name in [\"owner\", \"task_results\"]:\n            return super().__setattr__(name, value)\n\n        self.extra[name] = value\n\n    def __delattr__(self, name: str) -&gt; None:\n        \"\"\"\n        Ensure that we can delete attributes from the metadata class.\n\n        The attributes are deleted through the following heirarchy:\n            - If the attribute is `task_results`, we use the descriptor to delete the task results.\n            - Otherwise, if it is a sub-classed model, it will be deleted as normal.\n            - Finally, if we are deleting a public attribute on the base metadata class,\n                we use the extra field.\n        \"\"\"\n\n        # We want to use the descriptor to delete the task results\n        if name == \"task_results\":\n            self._task_results.__delete__(self)\n            return\n\n        # Otherwise, we use our standard fallback tiers\n        if self._is_field_typed():\n            return super().__delattr__(name)\n\n        del self.extra[name]\n</code></pre>"},{"location":"reference/schema/pipeline/metadata/#docprompt.schema.pipeline.metadata.BaseMetadata.owner","title":"<code>owner: TMetadataOwner</code>  <code>property</code> <code>writable</code>","text":"<p>Return the owner of the metadata.</p>"},{"location":"reference/schema/pipeline/metadata/#docprompt.schema.pipeline.metadata.BaseMetadata.task_results","title":"<code>task_results: TaskResultsDescriptor</code>  <code>property</code> <code>writable</code>","text":"<p>Return the task results descriptor.</p>"},{"location":"reference/schema/pipeline/metadata/#docprompt.schema.pipeline.metadata.BaseMetadata.__delattr__","title":"<code>__delattr__(name)</code>","text":"<p>Ensure that we can delete attributes from the metadata class.</p> The attributes are deleted through the following heirarchy <ul> <li>If the attribute is <code>task_results</code>, we use the descriptor to delete the task results.</li> <li>Otherwise, if it is a sub-classed model, it will be deleted as normal.</li> <li>Finally, if we are deleting a public attribute on the base metadata class,     we use the extra field.</li> </ul> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>def __delattr__(self, name: str) -&gt; None:\n    \"\"\"\n    Ensure that we can delete attributes from the metadata class.\n\n    The attributes are deleted through the following heirarchy:\n        - If the attribute is `task_results`, we use the descriptor to delete the task results.\n        - Otherwise, if it is a sub-classed model, it will be deleted as normal.\n        - Finally, if we are deleting a public attribute on the base metadata class,\n            we use the extra field.\n    \"\"\"\n\n    # We want to use the descriptor to delete the task results\n    if name == \"task_results\":\n        self._task_results.__delete__(self)\n        return\n\n    # Otherwise, we use our standard fallback tiers\n    if self._is_field_typed():\n        return super().__delattr__(name)\n\n    del self.extra[name]\n</code></pre>"},{"location":"reference/schema/pipeline/metadata/#docprompt.schema.pipeline.metadata.BaseMetadata.__delitem__","title":"<code>__delitem__(name)</code>","text":"<p>Provide dictionary functionality to the metadata class.</p> <p>This only works for the base metadata model. If sub-classed, this will raise an error, unless overridden, as BaseModel's do not have an delitem method.</p> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>def __delitem__(self, name):\n    \"\"\"\n    Provide dictionary functionality to the metadata class.\n\n    This only works for the base metadata model. If sub-classed, this will raise an error,\n    unless overridden, as BaseModel's do not have an __delitem__ method.\n    \"\"\"\n    if not self._is_field_typed():\n        del self.extra[name]\n    else:\n        raise AttributeError(\n            f\"'{self.__class__.__name__}' object has no attribute '{name}'\"\n        )\n</code></pre>"},{"location":"reference/schema/pipeline/metadata/#docprompt.schema.pipeline.metadata.BaseMetadata.__getattr__","title":"<code>__getattr__(name)</code>","text":"<p>Allow for getting of attributes on the metadata class.</p> The attributes are retrieved through the following heirarchy <ul> <li>If the model is sub-classed, it will be retrieved as normal.</li> <li>Otherwise, if the attribute is private, it will be retrieved as normal.</li> <li>Finally, if we are getting a public attribute on the base metadata class,     we use the extra field.</li> <li>If the key is not set in the <code>extra</code> dictionary, we resort back to just trying to get the field.<ul> <li>This is when we grab the <code>owner</code> or <code>task_result</code> attribuite.</li> </ul> </li> </ul> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>def __getattr__(self, name):\n    \"\"\"\n    Allow for getting of attributes on the metadata class.\n\n    The attributes are retrieved through the following heirarchy:\n        - If the model is sub-classed, it will be retrieved as normal.\n        - Otherwise, if the attribute is private, it will be retrieved as normal.\n        - Finally, if we are getting a public attribute on the base metadata class,\n            we use the extra field.\n        - If the key is not set in the `extra` dictionary, we resort back to just\n        trying to get the field.\n            - This is when we grab the `owner` or `task_result` attribuite.\n    \"\"\"\n    if self._is_field_typed():\n        return super().__getattr__(name)\n\n    if name.startswith(\"_\"):\n        return super().__getattr__(name)\n\n    # Attempt to retreieve the attr from the `extra` field\n    try:\n        return self.extra.get(name)\n\n    except KeyError:\n        # This is for grabbing properties on the base metadata class\n        return super().__getattr__(name)\n</code></pre>"},{"location":"reference/schema/pipeline/metadata/#docprompt.schema.pipeline.metadata.BaseMetadata.__getitem__","title":"<code>__getitem__(name)</code>","text":"<p>Provide dictionary functionlaity to the metadata class.</p> <p>This only works for the base metadata model. If sub-classed, this will raise an error, unless overridden, as BaseModel's do not have an getitem method.</p> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>def __getitem__(self, name):\n    \"\"\"\n    Provide dictionary functionlaity to the metadata class.\n\n    This only works for the base metadata model. If sub-classed, this will raise an error,\n    unless overridden, as BaseModel's do not have an __getitem__ method.\n    \"\"\"\n    if not self._is_field_typed():\n        return self.extra[name]\n\n    raise AttributeError(\n        f\"'{self.__class__.__name__}' object has no attribute '{name}'\"\n    )\n</code></pre>"},{"location":"reference/schema/pipeline/metadata/#docprompt.schema.pipeline.metadata.BaseMetadata.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over the keys in the metadata.</p> <p>This only works for the base metadata model. If sub-classed, this will raise an error, unless overridden, as BaseModel's do not have an iter method.</p> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>def __iter__(self):\n    \"\"\"\n    Iterate over the keys in the metadata.\n\n    This only works for the base metadata model. If sub-classed, this will raise an error,\n    unless overridden, as BaseModel's do not have an __iter__ method.\n    \"\"\"\n    if self._is_field_typed():\n        raise AttributeError(f\"'{self.__class__.__name__}' object is not iterable\")\n\n    return iter(self.extra)\n</code></pre>"},{"location":"reference/schema/pipeline/metadata/#docprompt.schema.pipeline.metadata.BaseMetadata.__len__","title":"<code>__len__()</code>","text":"<p>Get the number of keys in the metadata.</p> <p>This only works for the base metadata model. If sub-classed, this will raise an error, unless overridden, as BaseModel's do not have a len method.</p> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>def __len__(self):\n    \"\"\"\n    Get the number of keys in the metadata.\n\n    This only works for the base metadata model. If sub-classed, this will raise an error,\n    unless overridden, as BaseModel's do not have a __len__ method.\n    \"\"\"\n    if self._is_field_typed():\n        raise AttributeError(\n            f\"'{self.__class__.__name__}' object has no attribute '__len__'\"\n        )\n\n    return len(self.extra)\n</code></pre>"},{"location":"reference/schema/pipeline/metadata/#docprompt.schema.pipeline.metadata.BaseMetadata.__repr__","title":"<code>__repr__()</code>","text":"<p>Provide a string representation of the metadata.</p> <p>This only works for the base metadata model. If sub-classed, this will raise an error, unless overridden, as BaseModel's do not have a repr method.</p> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>def __repr__(self):\n    \"\"\"\n    Provide a string representation of the metadata.\n\n    This only works for the base metadata model. If sub-classed, this will raise an error,\n    unless overridden, as BaseModel's do not have a __repr__ method.\n    \"\"\"\n    if self._is_field_typed():\n        return super().__repr__()\n\n    # Otherwise, we are deailing with dictornary-like metadata\n    return json.dumps(self.extra)\n</code></pre>"},{"location":"reference/schema/pipeline/metadata/#docprompt.schema.pipeline.metadata.BaseMetadata.__setattr__","title":"<code>__setattr__(name, value)</code>","text":"<p>Allow for setting of attributes on the metadata class.</p> The attributes are set through the following heirarchy <ul> <li>If the model is sub-classed, it will be set as normal.</li> <li>Otherwise, if the attribute is private, it will be set as normal.</li> <li>Finally, if we are setting a public attribute on the base metadata class,     we use the extra field.</li> </ul> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>def __setattr__(self, name: str, value: Any) -&gt; None:\n    \"\"\"\n    Allow for setting of attributes on the metadata class.\n\n    The attributes are set through the following heirarchy:\n        - If the model is sub-classed, it will be set as normal.\n        - Otherwise, if the attribute is private, it will be set as normal.\n        - Finally, if we are setting a public attribute on the base metadata class,\n            we use the extra field.\n    \"\"\"\n    if self._is_field_typed():\n        return super().__setattr__(name, value)\n\n    # We want to avoid setting any private attributes in the extra\n    # dictionary\n    if name.startswith(\"_\"):\n        return super().__setattr__(name, value)\n\n    # If it is `owner` or `task_results`, we want\n    # to avoid setting the attribute in the `extra` dictionary\n    if name in [\"owner\", \"task_results\"]:\n        return super().__setattr__(name, value)\n\n    self.extra[name] = value\n</code></pre>"},{"location":"reference/schema/pipeline/metadata/#docprompt.schema.pipeline.metadata.BaseMetadata.__setitem__","title":"<code>__setitem__(name, value)</code>","text":"<p>Provide dictionary functionality to the metadata class.</p> <p>This only works for the base metadata model. If sub-classed, this will raise an error, unless overridden, as BaseModel's do not have an setitem method.</p> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>def __setitem__(self, name, value):\n    \"\"\"\n    Provide dictionary functionality to the metadata class.\n\n    This only works for the base metadata model. If sub-classed, this will raise an error,\n    unless overridden, as BaseModel's do not have an __setitem__ method.\n    \"\"\"\n    if not self._is_field_typed():\n        self.extra[name] = value\n    else:\n        raise AttributeError(\n            f\"'{self.__class__.__name__}' object has no attribute '{name}'\"\n        )\n</code></pre>"},{"location":"reference/schema/pipeline/metadata/#docprompt.schema.pipeline.metadata.BaseMetadata.from_owner","title":"<code>from_owner(owner, **data)</code>  <code>classmethod</code>","text":"<p>Create a new instance of the metadata class with the owner set.</p> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>@classmethod\ndef from_owner(cls, owner: TMetadataOwner, **data) -&gt; BaseMetadata:\n    \"\"\"Create a new instance of the metadata class with the owner set.\"\"\"\n    metadata = cls(**data)\n    metadata.owner = owner\n    return metadata\n</code></pre>"},{"location":"reference/schema/pipeline/metadata/#docprompt.schema.pipeline.metadata.BaseMetadata.validate_data_fields_from_annotations","title":"<code>validate_data_fields_from_annotations(data)</code>  <code>classmethod</code>","text":"<p>Validate the data fields from the annotations.</p> Source code in <code>docprompt/schema/pipeline/metadata.py</code> <pre><code>@model_validator(mode=\"before\")\n@classmethod\ndef validate_data_fields_from_annotations(cls, data: Any) -&gt; Any:\n    \"\"\"Validate the data fields from the annotations.\"\"\"\n\n    # We want to make sure that we combine the `extra` metdata along with any\n    # other specific fields that are defined in the metadata.\n    extra = data.pop(\"extra\", {})\n    assert isinstance(extra, dict), \"The `extra` field must be a dictionary.\"\n    data = {**data, **extra}\n\n    # If the model has been sub-classed, then all of our fields must be\n    # validated by the pydantic model.\n    if cls._is_field_typed():\n        # We will get the fields out of extra and set them as potential fields to\n        # validate. They will be ignored if they are not defined in the model, but it\n        # allows for a more flexible way to define metadata.\n        # Otherwise, what ever is in the `extra` field will be stroed in the `extra` field.\n        return {**data, \"extra\": extra}\n\n    # Otherwise, we are using our mock-dict implentation, so we store our\n    # metadata in the `extra` field.\n    return {\"extra\": data}\n</code></pre>"},{"location":"reference/schema/pipeline/rasterizer/","title":"rasterizer","text":""},{"location":"reference/schema/pipeline/rasterizer/#docprompt.schema.pipeline.rasterizer.DocumentRasterizer","title":"<code>DocumentRasterizer</code>","text":"Source code in <code>docprompt/schema/pipeline/rasterizer.py</code> <pre><code>class DocumentRasterizer:\n    def __init__(self, owner: \"DocumentNode\"):\n        self.owner = owner\n\n    def rasterize(\n        self,\n        name: str,\n        *,\n        return_mode: Literal[\"bytes\", \"pil\"] = \"bytes\",\n        dpi: int = 100,\n        downscale_size: Optional[Tuple[int, int]] = None,\n        resize_mode: ResizeModes = \"thumbnail\",\n        resize_aspect_ratios: Optional[Iterable[AspectRatioRule]] = None,\n        do_convert: bool = False,\n        image_convert_mode: str = \"L\",\n        do_quantize: bool = False,\n        quantize_color_count: int = 8,\n        max_file_size_bytes: Optional[int] = None,\n        render_grayscale: bool = False,\n    ) -&gt; List[Union[bytes, Image.Image]]:\n        images = self.owner.document.rasterize_pdf(\n            dpi=dpi,\n            downscale_size=downscale_size,\n            resize_mode=resize_mode,\n            resize_aspect_ratios=resize_aspect_ratios,\n            do_convert=do_convert,\n            image_convert_mode=image_convert_mode,\n            do_quantize=do_quantize,\n            quantize_color_count=quantize_color_count,\n            max_file_size_bytes=max_file_size_bytes,\n            render_grayscale=render_grayscale,\n            return_mode=return_mode,\n        )\n\n        for page_number, image in images.items():\n            page_node = self.owner.page_nodes[page_number - 1]\n\n            page_node._raster_cache[name] = image\n\n        return list(images.values())\n\n    def propagate_cache(self, name: str, rasters: Dict[int, Union[bytes, Image.Image]]):\n        \"\"\"\n        Should be one-indexed\n        \"\"\"\n        for page_number, raster in rasters.items():\n            page_node = self.owner.page_nodes[page_number - 1]\n\n            page_node._raster_cache[name] = raster\n</code></pre>"},{"location":"reference/schema/pipeline/rasterizer/#docprompt.schema.pipeline.rasterizer.DocumentRasterizer.propagate_cache","title":"<code>propagate_cache(name, rasters)</code>","text":"<p>Should be one-indexed</p> Source code in <code>docprompt/schema/pipeline/rasterizer.py</code> <pre><code>def propagate_cache(self, name: str, rasters: Dict[int, Union[bytes, Image.Image]]):\n    \"\"\"\n    Should be one-indexed\n    \"\"\"\n    for page_number, raster in rasters.items():\n        page_node = self.owner.page_nodes[page_number - 1]\n\n        page_node._raster_cache[name] = raster\n</code></pre>"},{"location":"reference/schema/pipeline/node/","title":"Index","text":""},{"location":"reference/schema/pipeline/node/#docprompt.schema.pipeline.node.DocumentCollection","title":"<code>DocumentCollection</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[DocumentCollectionMetadata, DocumentNodeMetadata, PageNodeMetadata]</code></p> <p>Represents a collection of documents with some common metadata</p> Source code in <code>docprompt/schema/pipeline/node/collection.py</code> <pre><code>class DocumentCollection(\n    BaseModel,\n    Generic[DocumentCollectionMetadata, DocumentNodeMetadata, PageNodeMetadata],\n):\n    \"\"\"\n    Represents a collection of documents with some common metadata\n    \"\"\"\n\n    document_nodes: List[\"DocumentNode[DocumentNodeMetadata, PageNodeMetadata]\"]\n    metadata: DocumentCollectionMetadata = Field(..., default_factory=dict)\n</code></pre>"},{"location":"reference/schema/pipeline/node/#docprompt.schema.pipeline.node.DocumentNode","title":"<code>DocumentNode</code>","text":"<p>               Bases: <code>BaseNode</code>, <code>Generic[DocumentNodeMetadata, PageNodeMetadata]</code></p> <p>Represents a single document, with some metadata</p> Source code in <code>docprompt/schema/pipeline/node/document.py</code> <pre><code>class DocumentNode(BaseNode, Generic[DocumentNodeMetadata, PageNodeMetadata]):\n    \"\"\"\n    Represents a single document, with some metadata\n    \"\"\"\n\n    document: PdfDocument\n    page_nodes: List[PageNode[PageNodeMetadata]] = Field(\n        description=\"The pages in the document\", default_factory=list, repr=False\n    )\n    metadata: DocumentNodeMetadata = Field(\n        description=\"Application-specific metadata for the document\",\n        default_factory=BaseMetadata,\n    )\n\n    _locator: Optional[\"DocumentProvenanceLocator\"] = PrivateAttr(default=None)\n\n    _persistance_path: Optional[str] = PrivateAttr(default=None)\n\n    def __getstate__(self):\n        state = super().__getstate__()\n\n        state[\"__pydantic_private__\"][\"_locator\"] = None\n\n        return state\n\n    def __len__(self):\n        return len(self.page_nodes)\n\n    def __getitem__(self, index):\n        return self.page_nodes[index]\n\n    def __iter__(self):\n        return iter(self.page_nodes)\n\n    @property\n    def rasterizer(self):\n        return DocumentRasterizer(self)\n\n    @property\n    def locator(self):\n        if self._locator is None:\n            self.refresh_locator()\n\n        return self._locator\n\n    def refresh_locator(self):\n        \"\"\"\n        Refreshes the locator for this document node\n        \"\"\"\n        from docprompt.provenance.search import DocumentProvenanceLocator\n\n        if any(not page.ocr_results.result for page in self.page_nodes):\n            raise ValueError(\n                \"Cannot create a locator for a document node with missing OCR results\"\n            )\n\n        self._locator = DocumentProvenanceLocator.from_document_node(self)\n\n        return self.locator\n\n    @classmethod\n    def from_document(\n        cls,\n        document: PdfDocument,\n        document_metadata: Optional[DocumentNodeMetadata] = None,\n        page_metadata: Optional[List[PageNodeMetadata]] = None,\n    ):\n        document_node: \"DocumentNode[DocumentNodeMetadata, PageNodeMetadata]\" = cls(\n            document=document,\n        )\n        document_node.metadata = document_metadata or cls.metadata_class().from_owner(\n            document_node, **{}\n        )\n\n        if page_metadata is not None and len(page_metadata) != len(document):\n            raise ValueError(\n                \"The number of page metadata items must match the number of pages in the document.\"\n            )\n\n        for page_number in range(1, len(document) + 1):\n            if page_metadata is not None:\n                page_node = PageNode(\n                    document=document_node,\n                    page_number=page_number,\n                    metadata=page_metadata[page_number - 1],\n                )\n            else:\n                page_node = PageNode(document=document_node, page_number=page_number)\n\n            document_node.page_nodes.append(page_node)\n\n        return document_node\n\n    @property\n    def file_hash(self):\n        return self.document.document_hash\n\n    @property\n    def document_name(self):\n        return self.document.name\n\n    @classmethod\n    def metadata_class(cls) -&gt; Type[BaseMetadata]:\n        \"\"\"Get the metadata class for instantiating metadata from the model.\"\"\"\n\n        fields = cls.model_fields\n\n        # NOTE: The indexing is important here, and relies on the generic type being\n        # the SECOND of the two arguments in the `Union` annotation\n        metadata_field_annotation = fields[\"metadata\"].annotation\n\n        # If no override has been provided to the metadata model, we want to retrieve\n        # it as a TypedDict\n        if metadata_field_annotation == DocumentNodeMetadata:\n            return BaseMetadata\n\n        if isinstance(metadata_field_annotation, ForwardRef):\n            raise ValueError(\n                \"You cannot define DocumentNode with a ForwardRef for Generic metadata model types.\"\n            )\n\n        # Get the overriden Generic type of th DocumentNodeMetadata\n        return metadata_field_annotation\n\n    @classmethod\n    def page_metadata_class(cls) -&gt; Type[Union[dict, BaseModel]]:\n        \"\"\"Get the metadata class for the page nodes in the document.\"\"\"\n        fields = cls.model_fields\n\n        # NOTE: The indexing is important here, and it allows us to get the type of each\n        # page node in the `List` annotation\n        page_nodes_field_class = fields[\"page_nodes\"].annotation.__args__[0]\n\n        # NOTE: The indexing is important here, and relies on the generic type being\n        # the SECOND of the two arguments in the `Union` annotation\n        page_node_metadata_field_annotation = page_nodes_field_class.model_fields[\n            \"metadata\"\n        ].annotation\n\n        if page_node_metadata_field_annotation == PageNodeMetadata:\n            return BaseMetadata\n\n        if isinstance(page_node_metadata_field_annotation, ForwardRef):\n            raise ValueError(\n                \"You cannot define PageNode with a ForwardRef for Generic metadata model types.\"\n            )\n\n        return page_node_metadata_field_annotation\n\n    @property\n    def persistance_path(self):\n        \"\"\"The base path to storage location.\"\"\"\n        return self._persistance_path\n\n    @persistance_path.setter\n    def persistance_path(self, path: str):\n        \"\"\"Set the base path to storage location.\"\"\"\n        self._persistance_path = path\n\n    @classmethod\n    def from_storage(cls, path: str, file_hash: str, **kwargs) -&gt; Self:\n        \"\"\"Load the document node from storage.\n\n        Args:\n            path (str): The base path to storage location.\n                - Example (S3): \"s3://bucket-name/key/to/folder\"\n                - Example (Local FS): \"/tmp/docprompt/storage\"\n            file_hash (str): The hash of the document.\n            **kwargs: Additional keyword arguments for fsspec FileSystem\n\n        Returns:\n            DocumentNode: The loaded document node.\n        \"\"\"\n\n        fs_manager = FileSystemManager(path, **kwargs)\n\n        pdf_bytes, metadata_bytes, page_metadata_bytes = fs_manager.read(\n            file_hash, **kwargs\n        )\n\n        doc = PdfDocument.from_bytes(pdf_bytes, name=fs_manager.get_pdf_name(file_hash))\n        node = cls.from_document(doc)\n\n        if metadata_bytes:\n            metadata_json = json.loads(metadata_bytes.decode(\"utf-8\"))\n            metadata = cls.metadata_class().from_owner(node, **metadata_json)\n        else:\n            metadata = cls.metadata_class().from_owner(node, **{})\n\n        if page_metadata_bytes:\n            page_metadata_json = [\n                json.loads(page_str)\n                for page_str in json.loads(page_metadata_bytes.decode(\"utf-8\"))\n            ]\n            page_metadata = [\n                cls.page_metadata_class()(**page) for page in page_metadata_json\n            ]\n        else:\n            page_metadata = [cls.page_metadata_class()(**{}) for _ in range(len(doc))]\n\n        # Store the metadata on the node and page nodes\n        node.metadata = metadata\n        for page, meta in zip(node.page_nodes, page_metadata):\n            meta.set_owner(page)\n            page.metadata = meta\n\n        # Make sure to set the persistance path on the node\n        node.persistance_path = path\n\n        return node\n\n    def persist(self, path: Optional[str] = None, **kwargs) -&gt; FileSidecarsPathManager:\n        \"\"\"Persist a document node to storage.\n\n        Args:\n            path (Optional[str]): Overwrites the current `persistance_path` property\n                - If `persistance_path` is not currently set, path must be provided.\n            **kwargs: Additional keyword arguments for fsspec FileSystem\n\n        Returns:\n            FileSidecarsPathManager: The file path manager for the persisted document node.\n        \"\"\"\n\n        path = path or self.persistance_path\n\n        if path is None:\n            raise ValueError(\"The path must be provided to persist the document node.\")\n\n        # Make sure to update the persistance path\n        self.persistance_path = path\n\n        fs_manager = FileSystemManager(path, **kwargs)\n\n        pdf_bytes = self.document.get_bytes()\n        metadata_bytes = bytes(self.metadata.model_dump_json(), encoding=\"utf-8\")\n        page_metadata_bytes = bytes(\n            json.dumps([page.metadata.model_dump_json() for page in self.page_nodes]),\n            encoding=\"utf-8\",\n        )\n\n        return fs_manager.write(\n            pdf_bytes, metadata_bytes, page_metadata_bytes, **kwargs\n        )\n</code></pre>"},{"location":"reference/schema/pipeline/node/#docprompt.schema.pipeline.node.DocumentNode.persistance_path","title":"<code>persistance_path</code>  <code>property</code> <code>writable</code>","text":"<p>The base path to storage location.</p>"},{"location":"reference/schema/pipeline/node/#docprompt.schema.pipeline.node.DocumentNode.from_storage","title":"<code>from_storage(path, file_hash, **kwargs)</code>  <code>classmethod</code>","text":"<p>Load the document node from storage.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The base path to storage location. - Example (S3): \"s3://bucket-name/key/to/folder\" - Example (Local FS): \"/tmp/docprompt/storage\"</p> required <code>file_hash</code> <code>str</code> <p>The hash of the document.</p> required <code>**kwargs</code> <p>Additional keyword arguments for fsspec FileSystem</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>DocumentNode</code> <code>Self</code> <p>The loaded document node.</p> Source code in <code>docprompt/schema/pipeline/node/document.py</code> <pre><code>@classmethod\ndef from_storage(cls, path: str, file_hash: str, **kwargs) -&gt; Self:\n    \"\"\"Load the document node from storage.\n\n    Args:\n        path (str): The base path to storage location.\n            - Example (S3): \"s3://bucket-name/key/to/folder\"\n            - Example (Local FS): \"/tmp/docprompt/storage\"\n        file_hash (str): The hash of the document.\n        **kwargs: Additional keyword arguments for fsspec FileSystem\n\n    Returns:\n        DocumentNode: The loaded document node.\n    \"\"\"\n\n    fs_manager = FileSystemManager(path, **kwargs)\n\n    pdf_bytes, metadata_bytes, page_metadata_bytes = fs_manager.read(\n        file_hash, **kwargs\n    )\n\n    doc = PdfDocument.from_bytes(pdf_bytes, name=fs_manager.get_pdf_name(file_hash))\n    node = cls.from_document(doc)\n\n    if metadata_bytes:\n        metadata_json = json.loads(metadata_bytes.decode(\"utf-8\"))\n        metadata = cls.metadata_class().from_owner(node, **metadata_json)\n    else:\n        metadata = cls.metadata_class().from_owner(node, **{})\n\n    if page_metadata_bytes:\n        page_metadata_json = [\n            json.loads(page_str)\n            for page_str in json.loads(page_metadata_bytes.decode(\"utf-8\"))\n        ]\n        page_metadata = [\n            cls.page_metadata_class()(**page) for page in page_metadata_json\n        ]\n    else:\n        page_metadata = [cls.page_metadata_class()(**{}) for _ in range(len(doc))]\n\n    # Store the metadata on the node and page nodes\n    node.metadata = metadata\n    for page, meta in zip(node.page_nodes, page_metadata):\n        meta.set_owner(page)\n        page.metadata = meta\n\n    # Make sure to set the persistance path on the node\n    node.persistance_path = path\n\n    return node\n</code></pre>"},{"location":"reference/schema/pipeline/node/#docprompt.schema.pipeline.node.DocumentNode.metadata_class","title":"<code>metadata_class()</code>  <code>classmethod</code>","text":"<p>Get the metadata class for instantiating metadata from the model.</p> Source code in <code>docprompt/schema/pipeline/node/document.py</code> <pre><code>@classmethod\ndef metadata_class(cls) -&gt; Type[BaseMetadata]:\n    \"\"\"Get the metadata class for instantiating metadata from the model.\"\"\"\n\n    fields = cls.model_fields\n\n    # NOTE: The indexing is important here, and relies on the generic type being\n    # the SECOND of the two arguments in the `Union` annotation\n    metadata_field_annotation = fields[\"metadata\"].annotation\n\n    # If no override has been provided to the metadata model, we want to retrieve\n    # it as a TypedDict\n    if metadata_field_annotation == DocumentNodeMetadata:\n        return BaseMetadata\n\n    if isinstance(metadata_field_annotation, ForwardRef):\n        raise ValueError(\n            \"You cannot define DocumentNode with a ForwardRef for Generic metadata model types.\"\n        )\n\n    # Get the overriden Generic type of th DocumentNodeMetadata\n    return metadata_field_annotation\n</code></pre>"},{"location":"reference/schema/pipeline/node/#docprompt.schema.pipeline.node.DocumentNode.page_metadata_class","title":"<code>page_metadata_class()</code>  <code>classmethod</code>","text":"<p>Get the metadata class for the page nodes in the document.</p> Source code in <code>docprompt/schema/pipeline/node/document.py</code> <pre><code>@classmethod\ndef page_metadata_class(cls) -&gt; Type[Union[dict, BaseModel]]:\n    \"\"\"Get the metadata class for the page nodes in the document.\"\"\"\n    fields = cls.model_fields\n\n    # NOTE: The indexing is important here, and it allows us to get the type of each\n    # page node in the `List` annotation\n    page_nodes_field_class = fields[\"page_nodes\"].annotation.__args__[0]\n\n    # NOTE: The indexing is important here, and relies on the generic type being\n    # the SECOND of the two arguments in the `Union` annotation\n    page_node_metadata_field_annotation = page_nodes_field_class.model_fields[\n        \"metadata\"\n    ].annotation\n\n    if page_node_metadata_field_annotation == PageNodeMetadata:\n        return BaseMetadata\n\n    if isinstance(page_node_metadata_field_annotation, ForwardRef):\n        raise ValueError(\n            \"You cannot define PageNode with a ForwardRef for Generic metadata model types.\"\n        )\n\n    return page_node_metadata_field_annotation\n</code></pre>"},{"location":"reference/schema/pipeline/node/#docprompt.schema.pipeline.node.DocumentNode.persist","title":"<code>persist(path=None, **kwargs)</code>","text":"<p>Persist a document node to storage.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Optional[str]</code> <p>Overwrites the current <code>persistance_path</code> property - If <code>persistance_path</code> is not currently set, path must be provided.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments for fsspec FileSystem</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>FileSidecarsPathManager</code> <code>FileSidecarsPathManager</code> <p>The file path manager for the persisted document node.</p> Source code in <code>docprompt/schema/pipeline/node/document.py</code> <pre><code>def persist(self, path: Optional[str] = None, **kwargs) -&gt; FileSidecarsPathManager:\n    \"\"\"Persist a document node to storage.\n\n    Args:\n        path (Optional[str]): Overwrites the current `persistance_path` property\n            - If `persistance_path` is not currently set, path must be provided.\n        **kwargs: Additional keyword arguments for fsspec FileSystem\n\n    Returns:\n        FileSidecarsPathManager: The file path manager for the persisted document node.\n    \"\"\"\n\n    path = path or self.persistance_path\n\n    if path is None:\n        raise ValueError(\"The path must be provided to persist the document node.\")\n\n    # Make sure to update the persistance path\n    self.persistance_path = path\n\n    fs_manager = FileSystemManager(path, **kwargs)\n\n    pdf_bytes = self.document.get_bytes()\n    metadata_bytes = bytes(self.metadata.model_dump_json(), encoding=\"utf-8\")\n    page_metadata_bytes = bytes(\n        json.dumps([page.metadata.model_dump_json() for page in self.page_nodes]),\n        encoding=\"utf-8\",\n    )\n\n    return fs_manager.write(\n        pdf_bytes, metadata_bytes, page_metadata_bytes, **kwargs\n    )\n</code></pre>"},{"location":"reference/schema/pipeline/node/#docprompt.schema.pipeline.node.DocumentNode.refresh_locator","title":"<code>refresh_locator()</code>","text":"<p>Refreshes the locator for this document node</p> Source code in <code>docprompt/schema/pipeline/node/document.py</code> <pre><code>def refresh_locator(self):\n    \"\"\"\n    Refreshes the locator for this document node\n    \"\"\"\n    from docprompt.provenance.search import DocumentProvenanceLocator\n\n    if any(not page.ocr_results.result for page in self.page_nodes):\n        raise ValueError(\n            \"Cannot create a locator for a document node with missing OCR results\"\n        )\n\n    self._locator = DocumentProvenanceLocator.from_document_node(self)\n\n    return self.locator\n</code></pre>"},{"location":"reference/schema/pipeline/node/#docprompt.schema.pipeline.node.PageNode","title":"<code>PageNode</code>","text":"<p>               Bases: <code>BaseNode</code>, <code>Generic[PageNodeMetadata]</code></p> <p>Represents a single page in a document, with some metadata</p> Source code in <code>docprompt/schema/pipeline/node/page.py</code> <pre><code>class PageNode(BaseNode, Generic[PageNodeMetadata]):\n    \"\"\"\n    Represents a single page in a document, with some metadata\n    \"\"\"\n\n    document: \"DocumentNode\" = Field(exclude=True, repr=False)\n    page_number: PositiveInt = Field(description=\"The page number\")\n    metadata: PageNodeMetadata = Field(\n        description=\"Application-specific metadata for the page\",\n        default_factory=BaseMetadata,\n    )\n    extra: Dict[str, Any] = Field(\n        description=\"Extra data that can be stored on the page node\",\n        default_factory=dict,\n    )\n\n    ocr_results: ResultContainer[OcrPageResult] = Field(\n        default_factory=_result_container_factory,\n        description=\"The OCR results for the page\",\n        repr=False,\n    )\n\n    _raster_cache: Dict[str, bytes] = PrivateAttr(default_factory=dict)\n\n    def __getstate__(self):\n        state = super().__getstate__()\n\n        state[\"__pydantic_private__\"][\"_raster_cache\"] = {}\n\n        return state\n\n    @property\n    def rasterizer(self):\n        return PageRasterizer(self._raster_cache, self)\n\n    def search(\n        self, query: str, refine_to_words: bool = True, require_exact_match: bool = True\n    ):\n        return self.document.locator.search(\n            query,\n            page_number=self.page_number,\n            refine_to_word=refine_to_words,\n            require_exact_match=require_exact_match,\n        )\n</code></pre>"},{"location":"reference/schema/pipeline/node/#docprompt.schema.pipeline.node.base","title":"<code>base</code>","text":""},{"location":"reference/schema/pipeline/node/#docprompt.schema.pipeline.node.base.BaseNode","title":"<code>BaseNode</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>The base node class is utilized for defining a basic yet flexible interface</p> Source code in <code>docprompt/schema/pipeline/node/base.py</code> <pre><code>class BaseNode(BaseModel):\n    \"\"\"The base node class is utilized for defining a basic yet flexible interface\"\"\"\n</code></pre>"},{"location":"reference/schema/pipeline/node/#docprompt.schema.pipeline.node.collection","title":"<code>collection</code>","text":""},{"location":"reference/schema/pipeline/node/#docprompt.schema.pipeline.node.collection.DocumentCollection","title":"<code>DocumentCollection</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[DocumentCollectionMetadata, DocumentNodeMetadata, PageNodeMetadata]</code></p> <p>Represents a collection of documents with some common metadata</p> Source code in <code>docprompt/schema/pipeline/node/collection.py</code> <pre><code>class DocumentCollection(\n    BaseModel,\n    Generic[DocumentCollectionMetadata, DocumentNodeMetadata, PageNodeMetadata],\n):\n    \"\"\"\n    Represents a collection of documents with some common metadata\n    \"\"\"\n\n    document_nodes: List[\"DocumentNode[DocumentNodeMetadata, PageNodeMetadata]\"]\n    metadata: DocumentCollectionMetadata = Field(..., default_factory=dict)\n</code></pre>"},{"location":"reference/schema/pipeline/node/#docprompt.schema.pipeline.node.document","title":"<code>document</code>","text":""},{"location":"reference/schema/pipeline/node/#docprompt.schema.pipeline.node.document.DocumentNode","title":"<code>DocumentNode</code>","text":"<p>               Bases: <code>BaseNode</code>, <code>Generic[DocumentNodeMetadata, PageNodeMetadata]</code></p> <p>Represents a single document, with some metadata</p> Source code in <code>docprompt/schema/pipeline/node/document.py</code> <pre><code>class DocumentNode(BaseNode, Generic[DocumentNodeMetadata, PageNodeMetadata]):\n    \"\"\"\n    Represents a single document, with some metadata\n    \"\"\"\n\n    document: PdfDocument\n    page_nodes: List[PageNode[PageNodeMetadata]] = Field(\n        description=\"The pages in the document\", default_factory=list, repr=False\n    )\n    metadata: DocumentNodeMetadata = Field(\n        description=\"Application-specific metadata for the document\",\n        default_factory=BaseMetadata,\n    )\n\n    _locator: Optional[\"DocumentProvenanceLocator\"] = PrivateAttr(default=None)\n\n    _persistance_path: Optional[str] = PrivateAttr(default=None)\n\n    def __getstate__(self):\n        state = super().__getstate__()\n\n        state[\"__pydantic_private__\"][\"_locator\"] = None\n\n        return state\n\n    def __len__(self):\n        return len(self.page_nodes)\n\n    def __getitem__(self, index):\n        return self.page_nodes[index]\n\n    def __iter__(self):\n        return iter(self.page_nodes)\n\n    @property\n    def rasterizer(self):\n        return DocumentRasterizer(self)\n\n    @property\n    def locator(self):\n        if self._locator is None:\n            self.refresh_locator()\n\n        return self._locator\n\n    def refresh_locator(self):\n        \"\"\"\n        Refreshes the locator for this document node\n        \"\"\"\n        from docprompt.provenance.search import DocumentProvenanceLocator\n\n        if any(not page.ocr_results.result for page in self.page_nodes):\n            raise ValueError(\n                \"Cannot create a locator for a document node with missing OCR results\"\n            )\n\n        self._locator = DocumentProvenanceLocator.from_document_node(self)\n\n        return self.locator\n\n    @classmethod\n    def from_document(\n        cls,\n        document: PdfDocument,\n        document_metadata: Optional[DocumentNodeMetadata] = None,\n        page_metadata: Optional[List[PageNodeMetadata]] = None,\n    ):\n        document_node: \"DocumentNode[DocumentNodeMetadata, PageNodeMetadata]\" = cls(\n            document=document,\n        )\n        document_node.metadata = document_metadata or cls.metadata_class().from_owner(\n            document_node, **{}\n        )\n\n        if page_metadata is not None and len(page_metadata) != len(document):\n            raise ValueError(\n                \"The number of page metadata items must match the number of pages in the document.\"\n            )\n\n        for page_number in range(1, len(document) + 1):\n            if page_metadata is not None:\n                page_node = PageNode(\n                    document=document_node,\n                    page_number=page_number,\n                    metadata=page_metadata[page_number - 1],\n                )\n            else:\n                page_node = PageNode(document=document_node, page_number=page_number)\n\n            document_node.page_nodes.append(page_node)\n\n        return document_node\n\n    @property\n    def file_hash(self):\n        return self.document.document_hash\n\n    @property\n    def document_name(self):\n        return self.document.name\n\n    @classmethod\n    def metadata_class(cls) -&gt; Type[BaseMetadata]:\n        \"\"\"Get the metadata class for instantiating metadata from the model.\"\"\"\n\n        fields = cls.model_fields\n\n        # NOTE: The indexing is important here, and relies on the generic type being\n        # the SECOND of the two arguments in the `Union` annotation\n        metadata_field_annotation = fields[\"metadata\"].annotation\n\n        # If no override has been provided to the metadata model, we want to retrieve\n        # it as a TypedDict\n        if metadata_field_annotation == DocumentNodeMetadata:\n            return BaseMetadata\n\n        if isinstance(metadata_field_annotation, ForwardRef):\n            raise ValueError(\n                \"You cannot define DocumentNode with a ForwardRef for Generic metadata model types.\"\n            )\n\n        # Get the overriden Generic type of th DocumentNodeMetadata\n        return metadata_field_annotation\n\n    @classmethod\n    def page_metadata_class(cls) -&gt; Type[Union[dict, BaseModel]]:\n        \"\"\"Get the metadata class for the page nodes in the document.\"\"\"\n        fields = cls.model_fields\n\n        # NOTE: The indexing is important here, and it allows us to get the type of each\n        # page node in the `List` annotation\n        page_nodes_field_class = fields[\"page_nodes\"].annotation.__args__[0]\n\n        # NOTE: The indexing is important here, and relies on the generic type being\n        # the SECOND of the two arguments in the `Union` annotation\n        page_node_metadata_field_annotation = page_nodes_field_class.model_fields[\n            \"metadata\"\n        ].annotation\n\n        if page_node_metadata_field_annotation == PageNodeMetadata:\n            return BaseMetadata\n\n        if isinstance(page_node_metadata_field_annotation, ForwardRef):\n            raise ValueError(\n                \"You cannot define PageNode with a ForwardRef for Generic metadata model types.\"\n            )\n\n        return page_node_metadata_field_annotation\n\n    @property\n    def persistance_path(self):\n        \"\"\"The base path to storage location.\"\"\"\n        return self._persistance_path\n\n    @persistance_path.setter\n    def persistance_path(self, path: str):\n        \"\"\"Set the base path to storage location.\"\"\"\n        self._persistance_path = path\n\n    @classmethod\n    def from_storage(cls, path: str, file_hash: str, **kwargs) -&gt; Self:\n        \"\"\"Load the document node from storage.\n\n        Args:\n            path (str): The base path to storage location.\n                - Example (S3): \"s3://bucket-name/key/to/folder\"\n                - Example (Local FS): \"/tmp/docprompt/storage\"\n            file_hash (str): The hash of the document.\n            **kwargs: Additional keyword arguments for fsspec FileSystem\n\n        Returns:\n            DocumentNode: The loaded document node.\n        \"\"\"\n\n        fs_manager = FileSystemManager(path, **kwargs)\n\n        pdf_bytes, metadata_bytes, page_metadata_bytes = fs_manager.read(\n            file_hash, **kwargs\n        )\n\n        doc = PdfDocument.from_bytes(pdf_bytes, name=fs_manager.get_pdf_name(file_hash))\n        node = cls.from_document(doc)\n\n        if metadata_bytes:\n            metadata_json = json.loads(metadata_bytes.decode(\"utf-8\"))\n            metadata = cls.metadata_class().from_owner(node, **metadata_json)\n        else:\n            metadata = cls.metadata_class().from_owner(node, **{})\n\n        if page_metadata_bytes:\n            page_metadata_json = [\n                json.loads(page_str)\n                for page_str in json.loads(page_metadata_bytes.decode(\"utf-8\"))\n            ]\n            page_metadata = [\n                cls.page_metadata_class()(**page) for page in page_metadata_json\n            ]\n        else:\n            page_metadata = [cls.page_metadata_class()(**{}) for _ in range(len(doc))]\n\n        # Store the metadata on the node and page nodes\n        node.metadata = metadata\n        for page, meta in zip(node.page_nodes, page_metadata):\n            meta.set_owner(page)\n            page.metadata = meta\n\n        # Make sure to set the persistance path on the node\n        node.persistance_path = path\n\n        return node\n\n    def persist(self, path: Optional[str] = None, **kwargs) -&gt; FileSidecarsPathManager:\n        \"\"\"Persist a document node to storage.\n\n        Args:\n            path (Optional[str]): Overwrites the current `persistance_path` property\n                - If `persistance_path` is not currently set, path must be provided.\n            **kwargs: Additional keyword arguments for fsspec FileSystem\n\n        Returns:\n            FileSidecarsPathManager: The file path manager for the persisted document node.\n        \"\"\"\n\n        path = path or self.persistance_path\n\n        if path is None:\n            raise ValueError(\"The path must be provided to persist the document node.\")\n\n        # Make sure to update the persistance path\n        self.persistance_path = path\n\n        fs_manager = FileSystemManager(path, **kwargs)\n\n        pdf_bytes = self.document.get_bytes()\n        metadata_bytes = bytes(self.metadata.model_dump_json(), encoding=\"utf-8\")\n        page_metadata_bytes = bytes(\n            json.dumps([page.metadata.model_dump_json() for page in self.page_nodes]),\n            encoding=\"utf-8\",\n        )\n\n        return fs_manager.write(\n            pdf_bytes, metadata_bytes, page_metadata_bytes, **kwargs\n        )\n</code></pre>"},{"location":"reference/schema/pipeline/node/#docprompt.schema.pipeline.node.document.DocumentNode.persistance_path","title":"<code>persistance_path</code>  <code>property</code> <code>writable</code>","text":"<p>The base path to storage location.</p>"},{"location":"reference/schema/pipeline/node/#docprompt.schema.pipeline.node.document.DocumentNode.from_storage","title":"<code>from_storage(path, file_hash, **kwargs)</code>  <code>classmethod</code>","text":"<p>Load the document node from storage.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The base path to storage location. - Example (S3): \"s3://bucket-name/key/to/folder\" - Example (Local FS): \"/tmp/docprompt/storage\"</p> required <code>file_hash</code> <code>str</code> <p>The hash of the document.</p> required <code>**kwargs</code> <p>Additional keyword arguments for fsspec FileSystem</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>DocumentNode</code> <code>Self</code> <p>The loaded document node.</p> Source code in <code>docprompt/schema/pipeline/node/document.py</code> <pre><code>@classmethod\ndef from_storage(cls, path: str, file_hash: str, **kwargs) -&gt; Self:\n    \"\"\"Load the document node from storage.\n\n    Args:\n        path (str): The base path to storage location.\n            - Example (S3): \"s3://bucket-name/key/to/folder\"\n            - Example (Local FS): \"/tmp/docprompt/storage\"\n        file_hash (str): The hash of the document.\n        **kwargs: Additional keyword arguments for fsspec FileSystem\n\n    Returns:\n        DocumentNode: The loaded document node.\n    \"\"\"\n\n    fs_manager = FileSystemManager(path, **kwargs)\n\n    pdf_bytes, metadata_bytes, page_metadata_bytes = fs_manager.read(\n        file_hash, **kwargs\n    )\n\n    doc = PdfDocument.from_bytes(pdf_bytes, name=fs_manager.get_pdf_name(file_hash))\n    node = cls.from_document(doc)\n\n    if metadata_bytes:\n        metadata_json = json.loads(metadata_bytes.decode(\"utf-8\"))\n        metadata = cls.metadata_class().from_owner(node, **metadata_json)\n    else:\n        metadata = cls.metadata_class().from_owner(node, **{})\n\n    if page_metadata_bytes:\n        page_metadata_json = [\n            json.loads(page_str)\n            for page_str in json.loads(page_metadata_bytes.decode(\"utf-8\"))\n        ]\n        page_metadata = [\n            cls.page_metadata_class()(**page) for page in page_metadata_json\n        ]\n    else:\n        page_metadata = [cls.page_metadata_class()(**{}) for _ in range(len(doc))]\n\n    # Store the metadata on the node and page nodes\n    node.metadata = metadata\n    for page, meta in zip(node.page_nodes, page_metadata):\n        meta.set_owner(page)\n        page.metadata = meta\n\n    # Make sure to set the persistance path on the node\n    node.persistance_path = path\n\n    return node\n</code></pre>"},{"location":"reference/schema/pipeline/node/#docprompt.schema.pipeline.node.document.DocumentNode.metadata_class","title":"<code>metadata_class()</code>  <code>classmethod</code>","text":"<p>Get the metadata class for instantiating metadata from the model.</p> Source code in <code>docprompt/schema/pipeline/node/document.py</code> <pre><code>@classmethod\ndef metadata_class(cls) -&gt; Type[BaseMetadata]:\n    \"\"\"Get the metadata class for instantiating metadata from the model.\"\"\"\n\n    fields = cls.model_fields\n\n    # NOTE: The indexing is important here, and relies on the generic type being\n    # the SECOND of the two arguments in the `Union` annotation\n    metadata_field_annotation = fields[\"metadata\"].annotation\n\n    # If no override has been provided to the metadata model, we want to retrieve\n    # it as a TypedDict\n    if metadata_field_annotation == DocumentNodeMetadata:\n        return BaseMetadata\n\n    if isinstance(metadata_field_annotation, ForwardRef):\n        raise ValueError(\n            \"You cannot define DocumentNode with a ForwardRef for Generic metadata model types.\"\n        )\n\n    # Get the overriden Generic type of th DocumentNodeMetadata\n    return metadata_field_annotation\n</code></pre>"},{"location":"reference/schema/pipeline/node/#docprompt.schema.pipeline.node.document.DocumentNode.page_metadata_class","title":"<code>page_metadata_class()</code>  <code>classmethod</code>","text":"<p>Get the metadata class for the page nodes in the document.</p> Source code in <code>docprompt/schema/pipeline/node/document.py</code> <pre><code>@classmethod\ndef page_metadata_class(cls) -&gt; Type[Union[dict, BaseModel]]:\n    \"\"\"Get the metadata class for the page nodes in the document.\"\"\"\n    fields = cls.model_fields\n\n    # NOTE: The indexing is important here, and it allows us to get the type of each\n    # page node in the `List` annotation\n    page_nodes_field_class = fields[\"page_nodes\"].annotation.__args__[0]\n\n    # NOTE: The indexing is important here, and relies on the generic type being\n    # the SECOND of the two arguments in the `Union` annotation\n    page_node_metadata_field_annotation = page_nodes_field_class.model_fields[\n        \"metadata\"\n    ].annotation\n\n    if page_node_metadata_field_annotation == PageNodeMetadata:\n        return BaseMetadata\n\n    if isinstance(page_node_metadata_field_annotation, ForwardRef):\n        raise ValueError(\n            \"You cannot define PageNode with a ForwardRef for Generic metadata model types.\"\n        )\n\n    return page_node_metadata_field_annotation\n</code></pre>"},{"location":"reference/schema/pipeline/node/#docprompt.schema.pipeline.node.document.DocumentNode.persist","title":"<code>persist(path=None, **kwargs)</code>","text":"<p>Persist a document node to storage.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Optional[str]</code> <p>Overwrites the current <code>persistance_path</code> property - If <code>persistance_path</code> is not currently set, path must be provided.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments for fsspec FileSystem</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>FileSidecarsPathManager</code> <code>FileSidecarsPathManager</code> <p>The file path manager for the persisted document node.</p> Source code in <code>docprompt/schema/pipeline/node/document.py</code> <pre><code>def persist(self, path: Optional[str] = None, **kwargs) -&gt; FileSidecarsPathManager:\n    \"\"\"Persist a document node to storage.\n\n    Args:\n        path (Optional[str]): Overwrites the current `persistance_path` property\n            - If `persistance_path` is not currently set, path must be provided.\n        **kwargs: Additional keyword arguments for fsspec FileSystem\n\n    Returns:\n        FileSidecarsPathManager: The file path manager for the persisted document node.\n    \"\"\"\n\n    path = path or self.persistance_path\n\n    if path is None:\n        raise ValueError(\"The path must be provided to persist the document node.\")\n\n    # Make sure to update the persistance path\n    self.persistance_path = path\n\n    fs_manager = FileSystemManager(path, **kwargs)\n\n    pdf_bytes = self.document.get_bytes()\n    metadata_bytes = bytes(self.metadata.model_dump_json(), encoding=\"utf-8\")\n    page_metadata_bytes = bytes(\n        json.dumps([page.metadata.model_dump_json() for page in self.page_nodes]),\n        encoding=\"utf-8\",\n    )\n\n    return fs_manager.write(\n        pdf_bytes, metadata_bytes, page_metadata_bytes, **kwargs\n    )\n</code></pre>"},{"location":"reference/schema/pipeline/node/#docprompt.schema.pipeline.node.document.DocumentNode.refresh_locator","title":"<code>refresh_locator()</code>","text":"<p>Refreshes the locator for this document node</p> Source code in <code>docprompt/schema/pipeline/node/document.py</code> <pre><code>def refresh_locator(self):\n    \"\"\"\n    Refreshes the locator for this document node\n    \"\"\"\n    from docprompt.provenance.search import DocumentProvenanceLocator\n\n    if any(not page.ocr_results.result for page in self.page_nodes):\n        raise ValueError(\n            \"Cannot create a locator for a document node with missing OCR results\"\n        )\n\n    self._locator = DocumentProvenanceLocator.from_document_node(self)\n\n    return self.locator\n</code></pre>"},{"location":"reference/schema/pipeline/node/#docprompt.schema.pipeline.node.page","title":"<code>page</code>","text":""},{"location":"reference/schema/pipeline/node/#docprompt.schema.pipeline.node.page.PageNode","title":"<code>PageNode</code>","text":"<p>               Bases: <code>BaseNode</code>, <code>Generic[PageNodeMetadata]</code></p> <p>Represents a single page in a document, with some metadata</p> Source code in <code>docprompt/schema/pipeline/node/page.py</code> <pre><code>class PageNode(BaseNode, Generic[PageNodeMetadata]):\n    \"\"\"\n    Represents a single page in a document, with some metadata\n    \"\"\"\n\n    document: \"DocumentNode\" = Field(exclude=True, repr=False)\n    page_number: PositiveInt = Field(description=\"The page number\")\n    metadata: PageNodeMetadata = Field(\n        description=\"Application-specific metadata for the page\",\n        default_factory=BaseMetadata,\n    )\n    extra: Dict[str, Any] = Field(\n        description=\"Extra data that can be stored on the page node\",\n        default_factory=dict,\n    )\n\n    ocr_results: ResultContainer[OcrPageResult] = Field(\n        default_factory=_result_container_factory,\n        description=\"The OCR results for the page\",\n        repr=False,\n    )\n\n    _raster_cache: Dict[str, bytes] = PrivateAttr(default_factory=dict)\n\n    def __getstate__(self):\n        state = super().__getstate__()\n\n        state[\"__pydantic_private__\"][\"_raster_cache\"] = {}\n\n        return state\n\n    @property\n    def rasterizer(self):\n        return PageRasterizer(self._raster_cache, self)\n\n    def search(\n        self, query: str, refine_to_words: bool = True, require_exact_match: bool = True\n    ):\n        return self.document.locator.search(\n            query,\n            page_number=self.page_number,\n            refine_to_word=refine_to_words,\n            require_exact_match=require_exact_match,\n        )\n</code></pre>"},{"location":"reference/schema/pipeline/node/#docprompt.schema.pipeline.node.typing","title":"<code>typing</code>","text":""},{"location":"reference/schema/pipeline/node/base/","title":"base","text":""},{"location":"reference/schema/pipeline/node/base/#docprompt.schema.pipeline.node.base.BaseNode","title":"<code>BaseNode</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>The base node class is utilized for defining a basic yet flexible interface</p> Source code in <code>docprompt/schema/pipeline/node/base.py</code> <pre><code>class BaseNode(BaseModel):\n    \"\"\"The base node class is utilized for defining a basic yet flexible interface\"\"\"\n</code></pre>"},{"location":"reference/schema/pipeline/node/collection/","title":"collection","text":""},{"location":"reference/schema/pipeline/node/collection/#docprompt.schema.pipeline.node.collection.DocumentCollection","title":"<code>DocumentCollection</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[DocumentCollectionMetadata, DocumentNodeMetadata, PageNodeMetadata]</code></p> <p>Represents a collection of documents with some common metadata</p> Source code in <code>docprompt/schema/pipeline/node/collection.py</code> <pre><code>class DocumentCollection(\n    BaseModel,\n    Generic[DocumentCollectionMetadata, DocumentNodeMetadata, PageNodeMetadata],\n):\n    \"\"\"\n    Represents a collection of documents with some common metadata\n    \"\"\"\n\n    document_nodes: List[\"DocumentNode[DocumentNodeMetadata, PageNodeMetadata]\"]\n    metadata: DocumentCollectionMetadata = Field(..., default_factory=dict)\n</code></pre>"},{"location":"reference/schema/pipeline/node/document/","title":"document","text":""},{"location":"reference/schema/pipeline/node/document/#docprompt.schema.pipeline.node.document.DocumentNode","title":"<code>DocumentNode</code>","text":"<p>               Bases: <code>BaseNode</code>, <code>Generic[DocumentNodeMetadata, PageNodeMetadata]</code></p> <p>Represents a single document, with some metadata</p> Source code in <code>docprompt/schema/pipeline/node/document.py</code> <pre><code>class DocumentNode(BaseNode, Generic[DocumentNodeMetadata, PageNodeMetadata]):\n    \"\"\"\n    Represents a single document, with some metadata\n    \"\"\"\n\n    document: PdfDocument\n    page_nodes: List[PageNode[PageNodeMetadata]] = Field(\n        description=\"The pages in the document\", default_factory=list, repr=False\n    )\n    metadata: DocumentNodeMetadata = Field(\n        description=\"Application-specific metadata for the document\",\n        default_factory=BaseMetadata,\n    )\n\n    _locator: Optional[\"DocumentProvenanceLocator\"] = PrivateAttr(default=None)\n\n    _persistance_path: Optional[str] = PrivateAttr(default=None)\n\n    def __getstate__(self):\n        state = super().__getstate__()\n\n        state[\"__pydantic_private__\"][\"_locator\"] = None\n\n        return state\n\n    def __len__(self):\n        return len(self.page_nodes)\n\n    def __getitem__(self, index):\n        return self.page_nodes[index]\n\n    def __iter__(self):\n        return iter(self.page_nodes)\n\n    @property\n    def rasterizer(self):\n        return DocumentRasterizer(self)\n\n    @property\n    def locator(self):\n        if self._locator is None:\n            self.refresh_locator()\n\n        return self._locator\n\n    def refresh_locator(self):\n        \"\"\"\n        Refreshes the locator for this document node\n        \"\"\"\n        from docprompt.provenance.search import DocumentProvenanceLocator\n\n        if any(not page.ocr_results.result for page in self.page_nodes):\n            raise ValueError(\n                \"Cannot create a locator for a document node with missing OCR results\"\n            )\n\n        self._locator = DocumentProvenanceLocator.from_document_node(self)\n\n        return self.locator\n\n    @classmethod\n    def from_document(\n        cls,\n        document: PdfDocument,\n        document_metadata: Optional[DocumentNodeMetadata] = None,\n        page_metadata: Optional[List[PageNodeMetadata]] = None,\n    ):\n        document_node: \"DocumentNode[DocumentNodeMetadata, PageNodeMetadata]\" = cls(\n            document=document,\n        )\n        document_node.metadata = document_metadata or cls.metadata_class().from_owner(\n            document_node, **{}\n        )\n\n        if page_metadata is not None and len(page_metadata) != len(document):\n            raise ValueError(\n                \"The number of page metadata items must match the number of pages in the document.\"\n            )\n\n        for page_number in range(1, len(document) + 1):\n            if page_metadata is not None:\n                page_node = PageNode(\n                    document=document_node,\n                    page_number=page_number,\n                    metadata=page_metadata[page_number - 1],\n                )\n            else:\n                page_node = PageNode(document=document_node, page_number=page_number)\n\n            document_node.page_nodes.append(page_node)\n\n        return document_node\n\n    @property\n    def file_hash(self):\n        return self.document.document_hash\n\n    @property\n    def document_name(self):\n        return self.document.name\n\n    @classmethod\n    def metadata_class(cls) -&gt; Type[BaseMetadata]:\n        \"\"\"Get the metadata class for instantiating metadata from the model.\"\"\"\n\n        fields = cls.model_fields\n\n        # NOTE: The indexing is important here, and relies on the generic type being\n        # the SECOND of the two arguments in the `Union` annotation\n        metadata_field_annotation = fields[\"metadata\"].annotation\n\n        # If no override has been provided to the metadata model, we want to retrieve\n        # it as a TypedDict\n        if metadata_field_annotation == DocumentNodeMetadata:\n            return BaseMetadata\n\n        if isinstance(metadata_field_annotation, ForwardRef):\n            raise ValueError(\n                \"You cannot define DocumentNode with a ForwardRef for Generic metadata model types.\"\n            )\n\n        # Get the overriden Generic type of th DocumentNodeMetadata\n        return metadata_field_annotation\n\n    @classmethod\n    def page_metadata_class(cls) -&gt; Type[Union[dict, BaseModel]]:\n        \"\"\"Get the metadata class for the page nodes in the document.\"\"\"\n        fields = cls.model_fields\n\n        # NOTE: The indexing is important here, and it allows us to get the type of each\n        # page node in the `List` annotation\n        page_nodes_field_class = fields[\"page_nodes\"].annotation.__args__[0]\n\n        # NOTE: The indexing is important here, and relies on the generic type being\n        # the SECOND of the two arguments in the `Union` annotation\n        page_node_metadata_field_annotation = page_nodes_field_class.model_fields[\n            \"metadata\"\n        ].annotation\n\n        if page_node_metadata_field_annotation == PageNodeMetadata:\n            return BaseMetadata\n\n        if isinstance(page_node_metadata_field_annotation, ForwardRef):\n            raise ValueError(\n                \"You cannot define PageNode with a ForwardRef for Generic metadata model types.\"\n            )\n\n        return page_node_metadata_field_annotation\n\n    @property\n    def persistance_path(self):\n        \"\"\"The base path to storage location.\"\"\"\n        return self._persistance_path\n\n    @persistance_path.setter\n    def persistance_path(self, path: str):\n        \"\"\"Set the base path to storage location.\"\"\"\n        self._persistance_path = path\n\n    @classmethod\n    def from_storage(cls, path: str, file_hash: str, **kwargs) -&gt; Self:\n        \"\"\"Load the document node from storage.\n\n        Args:\n            path (str): The base path to storage location.\n                - Example (S3): \"s3://bucket-name/key/to/folder\"\n                - Example (Local FS): \"/tmp/docprompt/storage\"\n            file_hash (str): The hash of the document.\n            **kwargs: Additional keyword arguments for fsspec FileSystem\n\n        Returns:\n            DocumentNode: The loaded document node.\n        \"\"\"\n\n        fs_manager = FileSystemManager(path, **kwargs)\n\n        pdf_bytes, metadata_bytes, page_metadata_bytes = fs_manager.read(\n            file_hash, **kwargs\n        )\n\n        doc = PdfDocument.from_bytes(pdf_bytes, name=fs_manager.get_pdf_name(file_hash))\n        node = cls.from_document(doc)\n\n        if metadata_bytes:\n            metadata_json = json.loads(metadata_bytes.decode(\"utf-8\"))\n            metadata = cls.metadata_class().from_owner(node, **metadata_json)\n        else:\n            metadata = cls.metadata_class().from_owner(node, **{})\n\n        if page_metadata_bytes:\n            page_metadata_json = [\n                json.loads(page_str)\n                for page_str in json.loads(page_metadata_bytes.decode(\"utf-8\"))\n            ]\n            page_metadata = [\n                cls.page_metadata_class()(**page) for page in page_metadata_json\n            ]\n        else:\n            page_metadata = [cls.page_metadata_class()(**{}) for _ in range(len(doc))]\n\n        # Store the metadata on the node and page nodes\n        node.metadata = metadata\n        for page, meta in zip(node.page_nodes, page_metadata):\n            meta.set_owner(page)\n            page.metadata = meta\n\n        # Make sure to set the persistance path on the node\n        node.persistance_path = path\n\n        return node\n\n    def persist(self, path: Optional[str] = None, **kwargs) -&gt; FileSidecarsPathManager:\n        \"\"\"Persist a document node to storage.\n\n        Args:\n            path (Optional[str]): Overwrites the current `persistance_path` property\n                - If `persistance_path` is not currently set, path must be provided.\n            **kwargs: Additional keyword arguments for fsspec FileSystem\n\n        Returns:\n            FileSidecarsPathManager: The file path manager for the persisted document node.\n        \"\"\"\n\n        path = path or self.persistance_path\n\n        if path is None:\n            raise ValueError(\"The path must be provided to persist the document node.\")\n\n        # Make sure to update the persistance path\n        self.persistance_path = path\n\n        fs_manager = FileSystemManager(path, **kwargs)\n\n        pdf_bytes = self.document.get_bytes()\n        metadata_bytes = bytes(self.metadata.model_dump_json(), encoding=\"utf-8\")\n        page_metadata_bytes = bytes(\n            json.dumps([page.metadata.model_dump_json() for page in self.page_nodes]),\n            encoding=\"utf-8\",\n        )\n\n        return fs_manager.write(\n            pdf_bytes, metadata_bytes, page_metadata_bytes, **kwargs\n        )\n</code></pre>"},{"location":"reference/schema/pipeline/node/document/#docprompt.schema.pipeline.node.document.DocumentNode.persistance_path","title":"<code>persistance_path</code>  <code>property</code> <code>writable</code>","text":"<p>The base path to storage location.</p>"},{"location":"reference/schema/pipeline/node/document/#docprompt.schema.pipeline.node.document.DocumentNode.from_storage","title":"<code>from_storage(path, file_hash, **kwargs)</code>  <code>classmethod</code>","text":"<p>Load the document node from storage.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The base path to storage location. - Example (S3): \"s3://bucket-name/key/to/folder\" - Example (Local FS): \"/tmp/docprompt/storage\"</p> required <code>file_hash</code> <code>str</code> <p>The hash of the document.</p> required <code>**kwargs</code> <p>Additional keyword arguments for fsspec FileSystem</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>DocumentNode</code> <code>Self</code> <p>The loaded document node.</p> Source code in <code>docprompt/schema/pipeline/node/document.py</code> <pre><code>@classmethod\ndef from_storage(cls, path: str, file_hash: str, **kwargs) -&gt; Self:\n    \"\"\"Load the document node from storage.\n\n    Args:\n        path (str): The base path to storage location.\n            - Example (S3): \"s3://bucket-name/key/to/folder\"\n            - Example (Local FS): \"/tmp/docprompt/storage\"\n        file_hash (str): The hash of the document.\n        **kwargs: Additional keyword arguments for fsspec FileSystem\n\n    Returns:\n        DocumentNode: The loaded document node.\n    \"\"\"\n\n    fs_manager = FileSystemManager(path, **kwargs)\n\n    pdf_bytes, metadata_bytes, page_metadata_bytes = fs_manager.read(\n        file_hash, **kwargs\n    )\n\n    doc = PdfDocument.from_bytes(pdf_bytes, name=fs_manager.get_pdf_name(file_hash))\n    node = cls.from_document(doc)\n\n    if metadata_bytes:\n        metadata_json = json.loads(metadata_bytes.decode(\"utf-8\"))\n        metadata = cls.metadata_class().from_owner(node, **metadata_json)\n    else:\n        metadata = cls.metadata_class().from_owner(node, **{})\n\n    if page_metadata_bytes:\n        page_metadata_json = [\n            json.loads(page_str)\n            for page_str in json.loads(page_metadata_bytes.decode(\"utf-8\"))\n        ]\n        page_metadata = [\n            cls.page_metadata_class()(**page) for page in page_metadata_json\n        ]\n    else:\n        page_metadata = [cls.page_metadata_class()(**{}) for _ in range(len(doc))]\n\n    # Store the metadata on the node and page nodes\n    node.metadata = metadata\n    for page, meta in zip(node.page_nodes, page_metadata):\n        meta.set_owner(page)\n        page.metadata = meta\n\n    # Make sure to set the persistance path on the node\n    node.persistance_path = path\n\n    return node\n</code></pre>"},{"location":"reference/schema/pipeline/node/document/#docprompt.schema.pipeline.node.document.DocumentNode.metadata_class","title":"<code>metadata_class()</code>  <code>classmethod</code>","text":"<p>Get the metadata class for instantiating metadata from the model.</p> Source code in <code>docprompt/schema/pipeline/node/document.py</code> <pre><code>@classmethod\ndef metadata_class(cls) -&gt; Type[BaseMetadata]:\n    \"\"\"Get the metadata class for instantiating metadata from the model.\"\"\"\n\n    fields = cls.model_fields\n\n    # NOTE: The indexing is important here, and relies on the generic type being\n    # the SECOND of the two arguments in the `Union` annotation\n    metadata_field_annotation = fields[\"metadata\"].annotation\n\n    # If no override has been provided to the metadata model, we want to retrieve\n    # it as a TypedDict\n    if metadata_field_annotation == DocumentNodeMetadata:\n        return BaseMetadata\n\n    if isinstance(metadata_field_annotation, ForwardRef):\n        raise ValueError(\n            \"You cannot define DocumentNode with a ForwardRef for Generic metadata model types.\"\n        )\n\n    # Get the overriden Generic type of th DocumentNodeMetadata\n    return metadata_field_annotation\n</code></pre>"},{"location":"reference/schema/pipeline/node/document/#docprompt.schema.pipeline.node.document.DocumentNode.page_metadata_class","title":"<code>page_metadata_class()</code>  <code>classmethod</code>","text":"<p>Get the metadata class for the page nodes in the document.</p> Source code in <code>docprompt/schema/pipeline/node/document.py</code> <pre><code>@classmethod\ndef page_metadata_class(cls) -&gt; Type[Union[dict, BaseModel]]:\n    \"\"\"Get the metadata class for the page nodes in the document.\"\"\"\n    fields = cls.model_fields\n\n    # NOTE: The indexing is important here, and it allows us to get the type of each\n    # page node in the `List` annotation\n    page_nodes_field_class = fields[\"page_nodes\"].annotation.__args__[0]\n\n    # NOTE: The indexing is important here, and relies on the generic type being\n    # the SECOND of the two arguments in the `Union` annotation\n    page_node_metadata_field_annotation = page_nodes_field_class.model_fields[\n        \"metadata\"\n    ].annotation\n\n    if page_node_metadata_field_annotation == PageNodeMetadata:\n        return BaseMetadata\n\n    if isinstance(page_node_metadata_field_annotation, ForwardRef):\n        raise ValueError(\n            \"You cannot define PageNode with a ForwardRef for Generic metadata model types.\"\n        )\n\n    return page_node_metadata_field_annotation\n</code></pre>"},{"location":"reference/schema/pipeline/node/document/#docprompt.schema.pipeline.node.document.DocumentNode.persist","title":"<code>persist(path=None, **kwargs)</code>","text":"<p>Persist a document node to storage.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Optional[str]</code> <p>Overwrites the current <code>persistance_path</code> property - If <code>persistance_path</code> is not currently set, path must be provided.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments for fsspec FileSystem</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>FileSidecarsPathManager</code> <code>FileSidecarsPathManager</code> <p>The file path manager for the persisted document node.</p> Source code in <code>docprompt/schema/pipeline/node/document.py</code> <pre><code>def persist(self, path: Optional[str] = None, **kwargs) -&gt; FileSidecarsPathManager:\n    \"\"\"Persist a document node to storage.\n\n    Args:\n        path (Optional[str]): Overwrites the current `persistance_path` property\n            - If `persistance_path` is not currently set, path must be provided.\n        **kwargs: Additional keyword arguments for fsspec FileSystem\n\n    Returns:\n        FileSidecarsPathManager: The file path manager for the persisted document node.\n    \"\"\"\n\n    path = path or self.persistance_path\n\n    if path is None:\n        raise ValueError(\"The path must be provided to persist the document node.\")\n\n    # Make sure to update the persistance path\n    self.persistance_path = path\n\n    fs_manager = FileSystemManager(path, **kwargs)\n\n    pdf_bytes = self.document.get_bytes()\n    metadata_bytes = bytes(self.metadata.model_dump_json(), encoding=\"utf-8\")\n    page_metadata_bytes = bytes(\n        json.dumps([page.metadata.model_dump_json() for page in self.page_nodes]),\n        encoding=\"utf-8\",\n    )\n\n    return fs_manager.write(\n        pdf_bytes, metadata_bytes, page_metadata_bytes, **kwargs\n    )\n</code></pre>"},{"location":"reference/schema/pipeline/node/document/#docprompt.schema.pipeline.node.document.DocumentNode.refresh_locator","title":"<code>refresh_locator()</code>","text":"<p>Refreshes the locator for this document node</p> Source code in <code>docprompt/schema/pipeline/node/document.py</code> <pre><code>def refresh_locator(self):\n    \"\"\"\n    Refreshes the locator for this document node\n    \"\"\"\n    from docprompt.provenance.search import DocumentProvenanceLocator\n\n    if any(not page.ocr_results.result for page in self.page_nodes):\n        raise ValueError(\n            \"Cannot create a locator for a document node with missing OCR results\"\n        )\n\n    self._locator = DocumentProvenanceLocator.from_document_node(self)\n\n    return self.locator\n</code></pre>"},{"location":"reference/schema/pipeline/node/image/","title":"image","text":""},{"location":"reference/schema/pipeline/node/page/","title":"page","text":""},{"location":"reference/schema/pipeline/node/page/#docprompt.schema.pipeline.node.page.PageNode","title":"<code>PageNode</code>","text":"<p>               Bases: <code>BaseNode</code>, <code>Generic[PageNodeMetadata]</code></p> <p>Represents a single page in a document, with some metadata</p> Source code in <code>docprompt/schema/pipeline/node/page.py</code> <pre><code>class PageNode(BaseNode, Generic[PageNodeMetadata]):\n    \"\"\"\n    Represents a single page in a document, with some metadata\n    \"\"\"\n\n    document: \"DocumentNode\" = Field(exclude=True, repr=False)\n    page_number: PositiveInt = Field(description=\"The page number\")\n    metadata: PageNodeMetadata = Field(\n        description=\"Application-specific metadata for the page\",\n        default_factory=BaseMetadata,\n    )\n    extra: Dict[str, Any] = Field(\n        description=\"Extra data that can be stored on the page node\",\n        default_factory=dict,\n    )\n\n    ocr_results: ResultContainer[OcrPageResult] = Field(\n        default_factory=_result_container_factory,\n        description=\"The OCR results for the page\",\n        repr=False,\n    )\n\n    _raster_cache: Dict[str, bytes] = PrivateAttr(default_factory=dict)\n\n    def __getstate__(self):\n        state = super().__getstate__()\n\n        state[\"__pydantic_private__\"][\"_raster_cache\"] = {}\n\n        return state\n\n    @property\n    def rasterizer(self):\n        return PageRasterizer(self._raster_cache, self)\n\n    def search(\n        self, query: str, refine_to_words: bool = True, require_exact_match: bool = True\n    ):\n        return self.document.locator.search(\n            query,\n            page_number=self.page_number,\n            refine_to_word=refine_to_words,\n            require_exact_match=require_exact_match,\n        )\n</code></pre>"},{"location":"reference/schema/pipeline/node/typing/","title":"typing","text":""},{"location":"reference/tasks/","title":"Index","text":""},{"location":"reference/tasks/#docprompt.tasks.base","title":"<code>base</code>","text":""},{"location":"reference/tasks/#docprompt.tasks.base.AbstractDocumentTaskProvider","title":"<code>AbstractDocumentTaskProvider</code>","text":"<p>               Bases: <code>AbstractTaskProvider</code></p> <p>A task provider performs a specific, repeatable task on a document.</p> Source code in <code>docprompt/tasks/base.py</code> <pre><code>class AbstractDocumentTaskProvider(AbstractTaskProvider):\n    \"\"\"\n    A task provider performs a specific, repeatable task on a document.\n    \"\"\"\n\n    capabilities: ClassVar[List[DocumentLevelCapabilities]]\n\n    # NOTE: We need the stubs defined here for the flexible decorators to work\n    # for now\n\n    class Meta:\n        abstract = True\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.base.AbstractPageTaskProvider","title":"<code>AbstractPageTaskProvider</code>","text":"<p>               Bases: <code>AbstractTaskProvider</code></p> <p>A page task provider performs a specific, repeatable task on a page.</p> Source code in <code>docprompt/tasks/base.py</code> <pre><code>class AbstractPageTaskProvider(AbstractTaskProvider):\n    \"\"\"\n    A page task provider performs a specific, repeatable task on a page.\n    \"\"\"\n\n    capabilities: ClassVar[List[PageLevelCapabilities]]\n\n    # NOTE: We need the stubs defined here for the flexible decorators to work\n    # for now\n\n    class Meta:\n        abstract = True\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.base.AbstractTaskProvider","title":"<code>AbstractTaskProvider</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[TTaskInput, TTaskConfig, TTaskResult]</code></p> <p>A task provider performs a specific, repeatable task on a document or its pages.</p> <p>NOTE: Either the <code>process_document_pages</code> or <code>aprocess_document_pages</code> method must be implemented in a valid subclass. The <code>process_document_pages</code> method is explicitly defined, while the <code>aprocess_document_pages</code> method is an async version of the same method.</p> <p>If you wish to provide seperate implementations for sync and async, you can define both methods individually, and they will each use their own custom implementation when called. Otherwise, if you only implement one or the other of a flexible method pair, the other will automatically be generated and provided for you at runtime.</p> Source code in <code>docprompt/tasks/base.py</code> <pre><code>@flexible_methods(\n    (\"process_document_node\", \"aprocess_document_node\"),\n    (\"_invoke\", \"_ainvoke\"),\n)\nclass AbstractTaskProvider(BaseModel, Generic[TTaskInput, TTaskConfig, TTaskResult]):\n    \"\"\"\n    A task provider performs a specific, repeatable task on a document or its pages.\n\n    NOTE: Either the `process_document_pages` or `aprocess_document_pages` method must be implemented in\n    a valid subclass. The `process_document_pages` method is explicitly defined, while the `aprocess_document_pages`\n    method is an async version of the same method.\n\n    If you wish to provide seperate implementations for sync and async, you can define both methods individually, and\n    they will each use their own custom implementation when called. Otherwise, if you only implement one or the other of\n    a flexible method pair, the other will automatically be generated and provided for you at runtime.\n    \"\"\"\n\n    name: ClassVar[str]\n    capabilities: ClassVar[List[Capabilites]]\n\n    # TODO: Potentially utilize context here during instantiation from Factory??\n    _default_invoke_kwargs: Dict[str, str] = PrivateAttr()\n\n    class Meta:\n        \"\"\"The meta class is utilized by the flexible methods decorator.\n\n        For all classes that are not concrete implementations, we should set the\n        abstract attribute to True, which will prevent the check from failing when\n        the flexible methods decorator is looking for the implementation of the\n        methods.\n        \"\"\"\n\n        abstract = True\n\n    def __init__(self, invoke_kwargs: Dict[str, str] = None, **data):\n        with init_context({\"invoke_kwargs\": invoke_kwargs or {}}):\n            self.__pydantic_validator__.validate_python(\n                data,\n                self_instance=self,\n                context=_init_context_var.get(),\n            )\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def validate_class_vars(cls, data: Any) -&gt; Any:\n        \"\"\"\n        Ensure that the class has a name and capabilities defined.\n        \"\"\"\n\n        if not hasattr(cls, \"name\"):\n            raise ValueError(\"Task providers must have a name defined\")\n\n        if not hasattr(cls, \"capabilities\"):\n            raise ValueError(\"Task providers must have capabilities defined\")\n\n        if not cls.capabilities:\n            raise ValueError(\"Task providers must have at least one capability defined\")\n\n        return data\n\n    @model_validator(mode=\"after\")\n    def set_invoke_kwargs(self, info: ValidationInfo) -&gt; Self:\n        \"\"\"\n        Set the default invoke kwargs for the task provider.\n        \"\"\"\n        self._default_invoke_kwargs = info.context[\"invoke_kwargs\"]\n        return self\n\n    async def _ainvoke(\n        self,\n        input: Iterable[TTaskInput],\n        config: Optional[TTaskConfig] = None,\n        **kwargs,\n    ) -&gt; List[TTaskResult]:\n        raise NotImplementedError\n\n    async def ainvoke(\n        self,\n        input: Iterable[TTaskInput],\n        config: Optional[TTaskConfig] = None,\n        **kwargs,\n    ) -&gt; List[TTaskResult]:\n        invoke_kwargs = {\n            **self._default_invoke_kwargs,\n            **kwargs,\n        }\n\n        return await self._ainvoke(input, config, **invoke_kwargs)\n\n    def _invoke(\n        self,\n        input: Iterable[TTaskInput],\n        config: Optional[TTaskConfig] = None,\n        **kwargs,\n    ) -&gt; List[TTaskResult]:\n        raise NotImplementedError\n\n    def invoke(\n        self,\n        input: Iterable[TTaskInput],\n        config: Optional[TTaskConfig] = None,\n        **kwargs,\n    ) -&gt; List[TTaskResult]:\n        invoke_kwargs = {\n            **self._default_invoke_kwargs,\n            **kwargs,\n        }\n\n        return self._invoke(input, config, **invoke_kwargs)\n\n    def process_document_node(\n        self,\n        document_node: \"DocumentNode\",\n        task_config: Optional[TTaskConfig] = None,\n        start: Optional[int] = None,\n        stop: Optional[int] = None,\n        contribute_to_document: bool = True,\n        **kwargs,\n    ) -&gt; Dict[int, TTaskResult]:\n        raise NotImplementedError\n\n    async def aprocess_document_node(\n        self,\n        document_node: \"DocumentNode\",\n        task_config: Optional[TTaskConfig] = None,\n        start: Optional[int] = None,\n        stop: Optional[int] = None,\n        contribute_to_document: bool = True,\n        **kwargs,\n    ) -&gt; Dict[int, TTaskResult]:\n        raise NotImplementedError\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.base.AbstractTaskProvider.Meta","title":"<code>Meta</code>","text":"<p>The meta class is utilized by the flexible methods decorator.</p> <p>For all classes that are not concrete implementations, we should set the abstract attribute to True, which will prevent the check from failing when the flexible methods decorator is looking for the implementation of the methods.</p> Source code in <code>docprompt/tasks/base.py</code> <pre><code>class Meta:\n    \"\"\"The meta class is utilized by the flexible methods decorator.\n\n    For all classes that are not concrete implementations, we should set the\n    abstract attribute to True, which will prevent the check from failing when\n    the flexible methods decorator is looking for the implementation of the\n    methods.\n    \"\"\"\n\n    abstract = True\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.base.AbstractTaskProvider.set_invoke_kwargs","title":"<code>set_invoke_kwargs(info)</code>","text":"<p>Set the default invoke kwargs for the task provider.</p> Source code in <code>docprompt/tasks/base.py</code> <pre><code>@model_validator(mode=\"after\")\ndef set_invoke_kwargs(self, info: ValidationInfo) -&gt; Self:\n    \"\"\"\n    Set the default invoke kwargs for the task provider.\n    \"\"\"\n    self._default_invoke_kwargs = info.context[\"invoke_kwargs\"]\n    return self\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.base.AbstractTaskProvider.validate_class_vars","title":"<code>validate_class_vars(data)</code>  <code>classmethod</code>","text":"<p>Ensure that the class has a name and capabilities defined.</p> Source code in <code>docprompt/tasks/base.py</code> <pre><code>@model_validator(mode=\"before\")\n@classmethod\ndef validate_class_vars(cls, data: Any) -&gt; Any:\n    \"\"\"\n    Ensure that the class has a name and capabilities defined.\n    \"\"\"\n\n    if not hasattr(cls, \"name\"):\n        raise ValueError(\"Task providers must have a name defined\")\n\n    if not hasattr(cls, \"capabilities\"):\n        raise ValueError(\"Task providers must have capabilities defined\")\n\n    if not cls.capabilities:\n        raise ValueError(\"Task providers must have at least one capability defined\")\n\n    return data\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.capabilities","title":"<code>capabilities</code>","text":""},{"location":"reference/tasks/#docprompt.tasks.capabilities.PageLevelCapabilities","title":"<code>PageLevelCapabilities</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Represents a capability that a provider can fulfill</p> Source code in <code>docprompt/tasks/capabilities.py</code> <pre><code>class PageLevelCapabilities(str, Enum):\n    \"\"\"\n    Represents a capability that a provider can fulfill\n    \"\"\"\n\n    PAGE_RASTERIZATION = \"page-rasterization\"\n    PAGE_LAYOUT_OCR = \"page-layout-ocr\"\n    PAGE_TEXT_OCR = \"page-text-ocr\"\n    PAGE_CLASSIFICATION = \"page-classification\"\n    PAGE_MARKERIZATION = \"page-markerization\"\n    PAGE_SEGMENTATION = \"page-segmentation\"\n    PAGE_VQA = \"page-vqa\"\n    PAGE_TABLE_IDENTIFICATION = \"page-table-identification\"\n    PAGE_TABLE_EXTRACTION = \"page-table-extraction\"\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.classification","title":"<code>classification</code>","text":""},{"location":"reference/tasks/#docprompt.tasks.classification.anthropic","title":"<code>anthropic</code>","text":"<p>The antrhopic implementation of page level calssification.</p>"},{"location":"reference/tasks/#docprompt.tasks.classification.anthropic.AnthropicClassificationProvider","title":"<code>AnthropicClassificationProvider</code>","text":"<p>               Bases: <code>BaseClassificationProvider</code></p> <p>The Anthropic implementation of unscored page classification.</p> Source code in <code>docprompt/tasks/classification/anthropic.py</code> <pre><code>class AnthropicClassificationProvider(BaseClassificationProvider):\n    \"\"\"The Anthropic implementation of unscored page classification.\"\"\"\n\n    name = \"anthropic\"\n\n    async def _ainvoke(\n        self, input: Iterable[bytes], config: ClassificationConfig = None, **kwargs\n    ) -&gt; List[ClassificationOutput]:\n        messages = _prepare_messages(input, config)\n\n        parser = AnthropicPageClassificationOutputParser.from_task_input(\n            config, provider_name=self.name\n        )\n\n        completions = await inference.run_batch_inference_anthropic(messages)\n\n        return [parser.parse(res) for res in completions]\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.classification.anthropic.AnthropicPageClassificationOutputParser","title":"<code>AnthropicPageClassificationOutputParser</code>","text":"<p>               Bases: <code>BasePageClassificationOutputParser</code></p> <p>The output parser for the page classification system.</p> Source code in <code>docprompt/tasks/classification/anthropic.py</code> <pre><code>class AnthropicPageClassificationOutputParser(BasePageClassificationOutputParser):\n    \"\"\"The output parser for the page classification system.\"\"\"\n\n    def parse(self, text: str) -&gt; ClassificationOutput:\n        \"\"\"Parse the results of the classification task.\"\"\"\n        pattern = re.compile(r\"Answer: (.+)\")\n        match = pattern.search(text)\n\n        result = self.resolve_match(match)\n\n        if self.confidence:\n            conf_pattern = re.compile(r\"Confidence: (.+)\")\n            conf_match = conf_pattern.search(text)\n            conf_result = self.resolve_confidence(conf_match)\n\n            return ClassificationOutput(\n                type=self.type,\n                labels=result,\n                score=conf_result,\n                provider_name=self.name,\n            )\n\n        return ClassificationOutput(\n            type=self.type, labels=result, provider_name=self.name\n        )\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.classification.anthropic.AnthropicPageClassificationOutputParser.parse","title":"<code>parse(text)</code>","text":"<p>Parse the results of the classification task.</p> Source code in <code>docprompt/tasks/classification/anthropic.py</code> <pre><code>def parse(self, text: str) -&gt; ClassificationOutput:\n    \"\"\"Parse the results of the classification task.\"\"\"\n    pattern = re.compile(r\"Answer: (.+)\")\n    match = pattern.search(text)\n\n    result = self.resolve_match(match)\n\n    if self.confidence:\n        conf_pattern = re.compile(r\"Confidence: (.+)\")\n        conf_match = conf_pattern.search(text)\n        conf_result = self.resolve_confidence(conf_match)\n\n        return ClassificationOutput(\n            type=self.type,\n            labels=result,\n            score=conf_result,\n            provider_name=self.name,\n        )\n\n    return ClassificationOutput(\n        type=self.type, labels=result, provider_name=self.name\n    )\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.classification.base","title":"<code>base</code>","text":""},{"location":"reference/tasks/#docprompt.tasks.classification.base.BaseClassificationProvider","title":"<code>BaseClassificationProvider</code>","text":"<p>               Bases: <code>AbstractPageTaskProvider[bytes, ClassificationConfig, ClassificationOutput]</code></p> <p>The base classification provider.</p> Source code in <code>docprompt/tasks/classification/base.py</code> <pre><code>class BaseClassificationProvider(\n    AbstractPageTaskProvider[bytes, ClassificationConfig, ClassificationOutput]\n):\n    \"\"\"\n    The base classification provider.\n    \"\"\"\n\n    capabilities = [PageLevelCapabilities.PAGE_CLASSIFICATION]\n\n    class Meta:\n        abstract = True\n\n    def process_document_node(\n        self,\n        document_node: \"DocumentNode\",\n        task_config: ClassificationConfig = None,\n        start: Optional[int] = None,\n        stop: Optional[int] = None,\n        contribute_to_document: bool = True,\n        **kwargs,\n    ):\n        assert (\n            task_config is not None\n        ), \"task_config must be provided for classification tasks\"\n\n        raster_bytes = []\n        for page_number in range(start or 1, (stop or len(document_node)) + 1):\n            image_bytes = document_node.page_nodes[\n                page_number - 1\n            ].rasterizer.rasterize(\"default\")\n            raster_bytes.append(image_bytes)\n\n        # TODO: This is a somewhat dangerous way of requiring these kwargs to be drilled\n        # through, potentially a decorator solution to be had here\n        kwargs = {**self._default_invoke_kwargs, **kwargs}\n        results = self._invoke(raster_bytes, config=task_config, **kwargs)\n\n        return {\n            i: res\n            for i, res in zip(\n                range(start or 1, (stop or len(document_node)) + 1), results\n            )\n        }\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.classification.base.BasePageClassificationOutputParser","title":"<code>BasePageClassificationOutputParser</code>","text":"<p>               Bases: <code>ABC</code>, <code>BaseOutputParser[ClassificationConfig, ClassificationOutput]</code></p> <p>The output parser for the page classification system.</p> Source code in <code>docprompt/tasks/classification/base.py</code> <pre><code>class BasePageClassificationOutputParser(\n    ABC, BaseOutputParser[ClassificationConfig, ClassificationOutput]\n):\n    \"\"\"The output parser for the page classification system.\"\"\"\n\n    name: str = Field(...)\n    type: ClassificationTypes = Field(...)\n    labels: LabelType = Field(...)\n    confidence: bool = Field(False)\n\n    @classmethod\n    def from_task_input(cls, task_input: ClassificationConfig, provider_name: str):\n        return cls(\n            type=task_input.type,\n            name=provider_name,\n            labels=task_input.labels,\n            confidence=task_input.confidence,\n        )\n\n    def resolve_match(self, _match: Union[re.Match, None]) -&gt; LabelType:\n        \"\"\"Get the regex pattern for the output parser.\"\"\"\n\n        if not _match:\n            raise ValueError(\"Could not find the answer in the text.\")\n\n        val = _match.group(1)\n        if self.type == ClassificationTypes.BINARY:\n            if val not in self.labels:\n                raise ValueError(f\"Invalid label: {val}\")\n            return val\n\n        elif self.type == ClassificationTypes.SINGLE_LABEL:\n            if val not in self.labels:\n                raise ValueError(f\"Invalid label: {val}\")\n            return val\n\n        elif self.type == ClassificationTypes.MULTI_LABEL:\n            labels = val.split(\", \")\n            for label in labels:\n                if label not in self.labels:\n                    raise ValueError(f\"Invalid label: {label}\")\n            return labels\n        else:\n            raise ValueError(f\"Invalid classification type: {self.type}\")\n\n    def resolve_confidence(self, _match: Union[re.Match, None]) -&gt; ConfidenceLevel:\n        \"\"\"Get the confidence level from the text.\"\"\"\n\n        if not _match:\n            return None\n\n        val = _match.group(1).lower()\n\n        return ConfidenceLevel(val)\n\n    @abstractmethod\n    def parse(self, text: str) -&gt; ClassificationOutput: ...\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.classification.base.BasePageClassificationOutputParser.resolve_confidence","title":"<code>resolve_confidence(_match)</code>","text":"<p>Get the confidence level from the text.</p> Source code in <code>docprompt/tasks/classification/base.py</code> <pre><code>def resolve_confidence(self, _match: Union[re.Match, None]) -&gt; ConfidenceLevel:\n    \"\"\"Get the confidence level from the text.\"\"\"\n\n    if not _match:\n        return None\n\n    val = _match.group(1).lower()\n\n    return ConfidenceLevel(val)\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.classification.base.BasePageClassificationOutputParser.resolve_match","title":"<code>resolve_match(_match)</code>","text":"<p>Get the regex pattern for the output parser.</p> Source code in <code>docprompt/tasks/classification/base.py</code> <pre><code>def resolve_match(self, _match: Union[re.Match, None]) -&gt; LabelType:\n    \"\"\"Get the regex pattern for the output parser.\"\"\"\n\n    if not _match:\n        raise ValueError(\"Could not find the answer in the text.\")\n\n    val = _match.group(1)\n    if self.type == ClassificationTypes.BINARY:\n        if val not in self.labels:\n            raise ValueError(f\"Invalid label: {val}\")\n        return val\n\n    elif self.type == ClassificationTypes.SINGLE_LABEL:\n        if val not in self.labels:\n            raise ValueError(f\"Invalid label: {val}\")\n        return val\n\n    elif self.type == ClassificationTypes.MULTI_LABEL:\n        labels = val.split(\", \")\n        for label in labels:\n            if label not in self.labels:\n                raise ValueError(f\"Invalid label: {label}\")\n        return labels\n    else:\n        raise ValueError(f\"Invalid classification type: {self.type}\")\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.classification.base.ClassificationConfig","title":"<code>ClassificationConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>docprompt/tasks/classification/base.py</code> <pre><code>class ClassificationConfig(BaseModel):\n    type: ClassificationTypes\n    labels: LabelType\n    descriptions: Optional[List[str]] = Field(\n        None, description=\"The descriptions for each label (if any).\"\n    )\n\n    instructions: Optional[str] = Field(\n        None,\n        description=\"Additional instructions to pass to the LLM for the task. Required for Binary Classification.\",\n    )\n\n    confidence: bool = Field(False)\n\n    @model_validator(mode=\"before\")\n    def validate_label_bindings(cls, data: Any) -&gt; Any:\n        \"\"\"Validate the the label/description bindings based on the type.\"\"\"\n\n        classification_type = data.get(\"type\", None)\n        if classification_type == ClassificationTypes.SINGLE_LABEL:\n            labels = data.get(\"labels\", None)\n            if not labels:\n                raise ValueError(\n                    \"labels must be provided for single_label classification\"\n                )\n            return data\n\n        elif classification_type == ClassificationTypes.BINARY:\n            instructions = data.get(\"instructions\", None)\n            if not instructions:\n                raise ValueError(\n                    \"instructions must be provided for binary classification\"\n                )\n            data[\"labels\"] = [\"YES\", \"NO\"]\n            return data\n\n        elif classification_type == ClassificationTypes.MULTI_LABEL:\n            labels = data.get(\"labels\", None)\n            if not labels:\n                raise ValueError(\n                    \"labels must be provided for multi_label classification\"\n                )\n            return data\n\n    @model_validator(mode=\"after\")\n    def validate_descriptions_length(self):\n        if self.descriptions is not None:\n            labels = self.labels\n            if labels is not None and len(self.descriptions) != len(labels):\n                raise ValueError(\"descriptions must have the same length as labels\")\n        return self\n\n    @property\n    def formatted_labels(self):\n        \"\"\"Produce the formatted labels for the prompt template.\"\"\"\n        raw_labels = self.labels\n        if self.descriptions:\n            for label, description in zip(raw_labels, self.descriptions):\n                yield f\"{label}: {description}\"\n        else:\n            yield from raw_labels\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.classification.base.ClassificationConfig.formatted_labels","title":"<code>formatted_labels</code>  <code>property</code>","text":"<p>Produce the formatted labels for the prompt template.</p>"},{"location":"reference/tasks/#docprompt.tasks.classification.base.ClassificationConfig.validate_label_bindings","title":"<code>validate_label_bindings(data)</code>","text":"<p>Validate the the label/description bindings based on the type.</p> Source code in <code>docprompt/tasks/classification/base.py</code> <pre><code>@model_validator(mode=\"before\")\ndef validate_label_bindings(cls, data: Any) -&gt; Any:\n    \"\"\"Validate the the label/description bindings based on the type.\"\"\"\n\n    classification_type = data.get(\"type\", None)\n    if classification_type == ClassificationTypes.SINGLE_LABEL:\n        labels = data.get(\"labels\", None)\n        if not labels:\n            raise ValueError(\n                \"labels must be provided for single_label classification\"\n            )\n        return data\n\n    elif classification_type == ClassificationTypes.BINARY:\n        instructions = data.get(\"instructions\", None)\n        if not instructions:\n            raise ValueError(\n                \"instructions must be provided for binary classification\"\n            )\n        data[\"labels\"] = [\"YES\", \"NO\"]\n        return data\n\n    elif classification_type == ClassificationTypes.MULTI_LABEL:\n        labels = data.get(\"labels\", None)\n        if not labels:\n            raise ValueError(\n                \"labels must be provided for multi_label classification\"\n            )\n        return data\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.classification.base.ConfidenceLevel","title":"<code>ConfidenceLevel</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>The confidence level of the classification.</p> Source code in <code>docprompt/tasks/classification/base.py</code> <pre><code>class ConfidenceLevel(str, Enum):\n    \"\"\"The confidence level of the classification.\"\"\"\n\n    LOW = \"low\"\n    MEDIUM = \"medium\"\n    HIGH = \"high\"\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.credentials","title":"<code>credentials</code>","text":"<p>The credentials module defines a simple model schema for storing credentials.</p>"},{"location":"reference/tasks/#docprompt.tasks.credentials.APIKeyCredential","title":"<code>APIKeyCredential</code>","text":"<p>               Bases: <code>BaseCredentials</code></p> <p>The API key credential model.</p> Source code in <code>docprompt/tasks/credentials.py</code> <pre><code>class APIKeyCredential(BaseCredentials):\n    \"\"\"The API key credential model.\"\"\"\n\n    api_key: SecretStr = Field(...)\n\n    def __init__(self, environ_path: Optional[str] = None, **data):\n        api_key = data.get(\"api_key\", None)\n        if api_key is None and environ_path:\n            api_key = os.environ.get(environ_path, None)\n        super().__init__(api_key=api_key)\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.credentials.AWSCredentials","title":"<code>AWSCredentials</code>","text":"<p>               Bases: <code>BaseCredentials</code></p> <p>The AWS credentials model.</p> Source code in <code>docprompt/tasks/credentials.py</code> <pre><code>class AWSCredentials(BaseCredentials):\n    \"\"\"The AWS credentials model.\"\"\"\n\n    aws_access_key_id: Optional[SecretStr] = Field(None)\n    aws_secret_access_key: Optional[SecretStr] = Field(None)\n    aws_session_token: Optional[SecretStr] = Field(None)\n    aws_region: Optional[str] = Field(None)\n\n    def __init__(self, **data):\n        aws_access_key_id = data.get(\n            \"aws_access_key_id\", os.environ.get(\"AWS_ACCESS_KEY_ID\", None)\n        )\n        aws_secret_access_key = data.get(\n            \"aws_secret_access_key\", os.environ.get(\"AWS_SECRET_ACCESS_KEY\", None)\n        )\n        aws_session_token = data.get(\n            \"aws_session_token\", os.environ.get(\"AWS_SESSION_TOKEN\", None)\n        )\n        aws_region = data.get(\"aws_region\", os.environ.get(\"AWS_DEFAULT_REGION\", None))\n        super().__init__(\n            aws_access_key_id=aws_access_key_id,\n            aws_secret_access_key=aws_secret_access_key,\n            aws_session_token=aws_session_token,\n            aws_region=aws_region,\n        )\n\n    @model_validator(mode=\"after\")\n    def _validate_aws_credentials(self) -&gt; Self:\n        \"\"\"Ensure the provided AWS credentials are valid.\"\"\"\n\n        key_pair_is_set = self.aws_access_key_id and self.aws_secret_access_key\n\n        if not key_pair_is_set and not self.aws_session_token:\n            raise ValueError(\n                \"You must provide either an AWS session token or an access key and secret key.\"\n            )\n\n        if key_pair_is_set and not self.aws_region:\n            raise ValueError(\n                \"You must provide an AWS region when using an access key and secret key.\"\n            )\n\n        if key_pair_is_set and self.aws_session_token:\n            raise ValueError(\n                \"You cannot provide both an AWS session token and an access key and secret key.\"\n            )\n\n        return self\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.credentials.BaseCredentials","title":"<code>BaseCredentials</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>The base credentials model.</p> Source code in <code>docprompt/tasks/credentials.py</code> <pre><code>class BaseCredentials(BaseModel):\n    \"\"\"The base credentials model.\"\"\"\n\n    @property\n    def kwargs(self) -&gt; Dict[str, str]:\n        \"\"\"Return the credentials as a dictionary with secrets exposed.\"\"\"\n        data = self.model_dump(exclude_none=True)\n        for key, value in data.items():\n            if isinstance(value, SecretStr):\n                data[key] = value.get_secret_value()\n        return data\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.credentials.BaseCredentials.kwargs","title":"<code>kwargs: Dict[str, str]</code>  <code>property</code>","text":"<p>Return the credentials as a dictionary with secrets exposed.</p>"},{"location":"reference/tasks/#docprompt.tasks.credentials.GCPServiceFileCredentials","title":"<code>GCPServiceFileCredentials</code>","text":"<p>               Bases: <code>BaseCredentials</code></p> <p>The GCP service account credentials model.</p> Source code in <code>docprompt/tasks/credentials.py</code> <pre><code>class GCPServiceFileCredentials(BaseCredentials):\n    \"\"\"The GCP service account credentials model.\"\"\"\n\n    service_account_info: Optional[Dict[str, str]] = Field(None)\n    service_account_file: Optional[str] = Field(None)\n\n    def __init__(self, **data):\n        service_account_info = data.get(\"service_account_info\", None)\n        service_account_file = data.get(\n            \"service_account_file\", os.environ.get(\"GCP_SERVICE_ACCOUNT_FILE\", None)\n        )\n\n        super().__init__(\n            service_account_info=service_account_info,\n            service_account_file=service_account_file,\n        )\n\n    @model_validator(mode=\"after\")\n    def _validate_gcp_credentials(self) -&gt; Self:\n        \"\"\"Ensure the provided GCP credentials are valid.\"\"\"\n        if self.service_account_info is None and self.service_account_file is None:\n            raise ValueError(\n                \"You must provide either service_account_info or service_account_file. You may set the `GCP_SERVICE_ACCOUNT_FILE` environment variable to the path of the service account file.\"\n            )\n        if (\n            self.service_account_info is not None\n            and self.service_account_file is not None\n        ):\n            raise ValueError(\n                \"You must provide either service_account_info or service_account_file, not both\"\n            )\n        return self\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.factory","title":"<code>factory</code>","text":"<p>Define the base factory for creating task providers.</p>"},{"location":"reference/tasks/#docprompt.tasks.factory.AbstractTaskMixin","title":"<code>AbstractTaskMixin</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all task mixins.</p> Source code in <code>docprompt/tasks/factory.py</code> <pre><code>class AbstractTaskMixin(ABC):\n    \"\"\"Base class for all task mixins.\"\"\"\n\n    tags: ClassVar[List[Union[PageLevelCapabilities, DocumentLevelCapabilities]]]\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.factory.AbstractTaskProviderFactory","title":"<code>AbstractTaskProviderFactory</code>","text":"<p>               Bases: <code>ABC</code>, <code>BaseModel</code></p> <p>The abstract interface for a provider task factory.</p> <p>We need to define the basic interface for how we can create task providers. The task provider factory is responsible for allowing the creation of task providers for specific backends, (i.e. Anthropic, OpenAI, etc.)</p> Source code in <code>docprompt/tasks/factory.py</code> <pre><code>class AbstractTaskProviderFactory(ABC, BaseModel):\n    \"\"\"The abstract interface for a provider task factory.\n\n    We need to define the basic interface for how we can create task providers. The task provider factory\n    is responsible for allowing the creation of task providers for specific backends, (i.e. Anthropic, OpenAI, etc.)\n    \"\"\"\n\n    def __init__(self, **data):\n        with init_context({\"payload\": data}):\n            self.__pydantic_validator__.validate_python(\n                data,\n                self_instance=self,\n                context=_init_context_var.get(),\n            )\n\n    @abstractmethod\n    @model_validator(mode=\"after\")\n    def _validate_provider(self) -&gt; Self:\n        \"\"\"Validate the provider before returning it.\n\n        This method needs to handle credential validation, to ensure that the provider is properly\n        configured and can be utilized for the tasks it can be used to provide.\n        \"\"\"\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.factory.AmazonTaskProviderFactory","title":"<code>AmazonTaskProviderFactory</code>","text":"<p>               Bases: <code>AbstractTaskProviderFactory</code>, <code>PageOCRMixin</code></p> <p>The task provider factory for Amazon.</p> Source code in <code>docprompt/tasks/factory.py</code> <pre><code>class AmazonTaskProviderFactory(AbstractTaskProviderFactory, PageOCRMixin):\n    \"\"\"The task provider factory for Amazon.\"\"\"\n\n    @model_validator(mode=\"after\")\n    def _validate_provider(self, info: ValidationInfo) -&gt; Self:\n        \"\"\"Validate the provider before returning it.\"\"\"\n        _payload = info.context[\"payload\"]\n        self._credentials = AWSCredentials(**_payload)\n\n    def get_page_ocr_provider(self, **kwargs) -&gt; TTaskProvider:\n        \"\"\"Get the page OCR provider.\"\"\"\n        from docprompt.tasks.ocr.amazon import AmazonTextractOCRProvider\n\n        return AmazonTextractOCRProvider(\n            invoke_kwargs=self._credentials.kwargs, **kwargs\n        )\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.factory.AmazonTaskProviderFactory.get_page_ocr_provider","title":"<code>get_page_ocr_provider(**kwargs)</code>","text":"<p>Get the page OCR provider.</p> Source code in <code>docprompt/tasks/factory.py</code> <pre><code>def get_page_ocr_provider(self, **kwargs) -&gt; TTaskProvider:\n    \"\"\"Get the page OCR provider.\"\"\"\n    from docprompt.tasks.ocr.amazon import AmazonTextractOCRProvider\n\n    return AmazonTextractOCRProvider(\n        invoke_kwargs=self._credentials.kwargs, **kwargs\n    )\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.factory.AnthropicTaskProviderFactory","title":"<code>AnthropicTaskProviderFactory</code>","text":"<p>               Bases: <code>AbstractTaskProviderFactory</code>, <code>PageClassificationMixin</code>, <code>PageMarkerizationMixin</code>, <code>PageTableExtractionMixin</code></p> <p>The task provider factory for Anthropic.</p> <p>NOTE: We can either utilize the standard Anthropic API or we can utilize AWS Bedrock. In the event that a user wants to utilize the standard Anthropic API.</p> Source code in <code>docprompt/tasks/factory.py</code> <pre><code>class AnthropicTaskProviderFactory(\n    AbstractTaskProviderFactory,\n    PageClassificationMixin,\n    PageMarkerizationMixin,\n    PageTableExtractionMixin,\n):\n    \"\"\"The task provider factory for Anthropic.\n\n    NOTE: We can either utilize the standard Anthropic API or we can utilize AWS Bedrock. In the event\n    that a user wants to utilize the standard Anthropic API.\n    \"\"\"\n\n    _credentials: APIKeyCredential = PrivateAttr()\n\n    @model_validator(mode=\"after\")\n    def _validate_provider(self, info: ValidationInfo) -&gt; Self:\n        \"\"\"Validate the provider before returning it.\"\"\"\n        _payload = info.context[\"payload\"]\n        self._credentials = APIKeyCredential(\n            environ_path=\"ANTHROPIC_API_KEY\", **_payload\n        )\n        return self\n\n    def get_page_classification_provider(self, **kwargs) -&gt; TTaskProvider:\n        \"\"\"Get the page classification provider.\"\"\"\n        from docprompt.tasks.classification.anthropic import (\n            AnthropicClassificationProvider,\n        )\n\n        return AnthropicClassificationProvider(invoke_kwargs=self._credentials.kwargs)\n\n    def get_page_table_extraction_provider(self, **kwargs) -&gt; TTaskProvider:\n        \"\"\"Get the page table extraction provider.\"\"\"\n        from docprompt.tasks.table_extraction.anthropic import (\n            AnthropicTableExtractionProvider,\n        )\n\n        return AnthropicTableExtractionProvider(\n            invoke_kwargs=self._credentials.kwargs, **kwargs\n        )\n\n    def get_page_markerization_provider(self, **kwargs) -&gt; TTaskProvider:\n        \"\"\"Get the page markerization provider.\"\"\"\n        from docprompt.tasks.markerize.anthropic import AnthropicMarkerizeProvider\n\n        return AnthropicMarkerizeProvider(\n            invoke_kwargs=self._credentials.kwargs, **kwargs\n        )\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.factory.AnthropicTaskProviderFactory.get_page_classification_provider","title":"<code>get_page_classification_provider(**kwargs)</code>","text":"<p>Get the page classification provider.</p> Source code in <code>docprompt/tasks/factory.py</code> <pre><code>def get_page_classification_provider(self, **kwargs) -&gt; TTaskProvider:\n    \"\"\"Get the page classification provider.\"\"\"\n    from docprompt.tasks.classification.anthropic import (\n        AnthropicClassificationProvider,\n    )\n\n    return AnthropicClassificationProvider(invoke_kwargs=self._credentials.kwargs)\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.factory.AnthropicTaskProviderFactory.get_page_markerization_provider","title":"<code>get_page_markerization_provider(**kwargs)</code>","text":"<p>Get the page markerization provider.</p> Source code in <code>docprompt/tasks/factory.py</code> <pre><code>def get_page_markerization_provider(self, **kwargs) -&gt; TTaskProvider:\n    \"\"\"Get the page markerization provider.\"\"\"\n    from docprompt.tasks.markerize.anthropic import AnthropicMarkerizeProvider\n\n    return AnthropicMarkerizeProvider(\n        invoke_kwargs=self._credentials.kwargs, **kwargs\n    )\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.factory.AnthropicTaskProviderFactory.get_page_table_extraction_provider","title":"<code>get_page_table_extraction_provider(**kwargs)</code>","text":"<p>Get the page table extraction provider.</p> Source code in <code>docprompt/tasks/factory.py</code> <pre><code>def get_page_table_extraction_provider(self, **kwargs) -&gt; TTaskProvider:\n    \"\"\"Get the page table extraction provider.\"\"\"\n    from docprompt.tasks.table_extraction.anthropic import (\n        AnthropicTableExtractionProvider,\n    )\n\n    return AnthropicTableExtractionProvider(\n        invoke_kwargs=self._credentials.kwargs, **kwargs\n    )\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.factory.DocumentVQAMixin","title":"<code>DocumentVQAMixin</code>","text":"<p>               Bases: <code>AbstractTaskMixin</code>, <code>Generic[TTaskProvider]</code></p> <p>Mixin for multi-page document VQA task.</p> Source code in <code>docprompt/tasks/factory.py</code> <pre><code>class DocumentVQAMixin(AbstractTaskMixin, Generic[TTaskProvider]):\n    \"\"\"Mixin for multi-page document VQA task.\"\"\"\n\n    tags = [DocumentLevelCapabilities.DOCUMENT_VQA]\n\n    @abstractmethod\n    def get_document_vqa_provider(self, *args, **kwargs) -&gt; TTaskProvider:\n        \"\"\"Perform multi-page document VQA.\"\"\"\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.factory.DocumentVQAMixin.get_document_vqa_provider","title":"<code>get_document_vqa_provider(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Perform multi-page document VQA.</p> Source code in <code>docprompt/tasks/factory.py</code> <pre><code>@abstractmethod\ndef get_document_vqa_provider(self, *args, **kwargs) -&gt; TTaskProvider:\n    \"\"\"Perform multi-page document VQA.\"\"\"\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.factory.GCPTaskProviderFactory","title":"<code>GCPTaskProviderFactory</code>","text":"<p>               Bases: <code>AbstractTaskProviderFactory</code>, <code>PageOCRMixin</code></p> <p>The task provider factory for GCP.</p> Source code in <code>docprompt/tasks/factory.py</code> <pre><code>class GCPTaskProviderFactory(\n    AbstractTaskProviderFactory,\n    PageOCRMixin,\n):\n    \"\"\"The task provider factory for GCP.\"\"\"\n\n    @model_validator(mode=\"after\")\n    def _validate_provider(self, info: ValidationInfo) -&gt; Self:\n        \"\"\"Validate the provider before returning it.\"\"\"\n        _payload = info.context[\"payload\"]\n        self._credentials = GCPServiceFileCredentials(**_payload)\n        return self\n\n    def get_page_ocr_provider(\n        self, project_id: str, processor_id: str, **kwargs\n    ) -&gt; TTaskProvider:\n        \"\"\"Get the page OCR provider.\"\"\"\n        from docprompt.tasks.ocr.gcp import GoogleOcrProvider\n\n        return GoogleOcrProvider(\n            project_id, processor_id, invoke_kwargs=self._credentials.kwargs, **kwargs\n        )\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.factory.GCPTaskProviderFactory.get_page_ocr_provider","title":"<code>get_page_ocr_provider(project_id, processor_id, **kwargs)</code>","text":"<p>Get the page OCR provider.</p> Source code in <code>docprompt/tasks/factory.py</code> <pre><code>def get_page_ocr_provider(\n    self, project_id: str, processor_id: str, **kwargs\n) -&gt; TTaskProvider:\n    \"\"\"Get the page OCR provider.\"\"\"\n    from docprompt.tasks.ocr.gcp import GoogleOcrProvider\n\n    return GoogleOcrProvider(\n        project_id, processor_id, invoke_kwargs=self._credentials.kwargs, **kwargs\n    )\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.factory.PageClassificationMixin","title":"<code>PageClassificationMixin</code>","text":"<p>               Bases: <code>AbstractTaskMixin</code>, <code>Generic[TTaskProvider]</code></p> <p>Mixin for page classification task.</p> Source code in <code>docprompt/tasks/factory.py</code> <pre><code>class PageClassificationMixin(AbstractTaskMixin, Generic[TTaskProvider]):\n    \"\"\"Mixin for page classification task.\"\"\"\n\n    tags = [PageLevelCapabilities.PAGE_CLASSIFICATION]\n\n    @abstractmethod\n    def get_page_classification_provider(self, *args, **kwargs) -&gt; TTaskProvider:\n        \"\"\"Perform page classification.\"\"\"\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.factory.PageClassificationMixin.get_page_classification_provider","title":"<code>get_page_classification_provider(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Perform page classification.</p> Source code in <code>docprompt/tasks/factory.py</code> <pre><code>@abstractmethod\ndef get_page_classification_provider(self, *args, **kwargs) -&gt; TTaskProvider:\n    \"\"\"Perform page classification.\"\"\"\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.factory.PageMarkerizationMixin","title":"<code>PageMarkerizationMixin</code>","text":"<p>               Bases: <code>AbstractTaskMixin</code>, <code>Generic[TTaskProvider]</code></p> <p>Mixin for page markerization task.</p> Source code in <code>docprompt/tasks/factory.py</code> <pre><code>class PageMarkerizationMixin(AbstractTaskMixin, Generic[TTaskProvider]):\n    \"\"\"Mixin for page markerization task.\"\"\"\n\n    tags = [PageLevelCapabilities.PAGE_MARKERIZATION]\n\n    @abstractmethod\n    def get_page_markerization_provider(self, *args, **kwargs) -&gt; TTaskProvider:\n        \"\"\"Perform page markerization.\"\"\"\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.factory.PageMarkerizationMixin.get_page_markerization_provider","title":"<code>get_page_markerization_provider(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Perform page markerization.</p> Source code in <code>docprompt/tasks/factory.py</code> <pre><code>@abstractmethod\ndef get_page_markerization_provider(self, *args, **kwargs) -&gt; TTaskProvider:\n    \"\"\"Perform page markerization.\"\"\"\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.factory.PageOCRMixin","title":"<code>PageOCRMixin</code>","text":"<p>               Bases: <code>AbstractTaskMixin</code>, <code>Generic[TTaskProvider]</code></p> <p>Mixin for page OCR task.</p> Source code in <code>docprompt/tasks/factory.py</code> <pre><code>class PageOCRMixin(AbstractTaskMixin, Generic[TTaskProvider]):\n    \"\"\"Mixin for page OCR task.\"\"\"\n\n    tags = [PageLevelCapabilities.PAGE_LAYOUT_OCR, PageLevelCapabilities.PAGE_TEXT_OCR]\n\n    @abstractmethod\n    def get_page_ocr_provider(self, *args, **kwargs) -&gt; TTaskProvider:\n        \"\"\"Perform OCR on a page.\"\"\"\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.factory.PageOCRMixin.get_page_ocr_provider","title":"<code>get_page_ocr_provider(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Perform OCR on a page.</p> Source code in <code>docprompt/tasks/factory.py</code> <pre><code>@abstractmethod\ndef get_page_ocr_provider(self, *args, **kwargs) -&gt; TTaskProvider:\n    \"\"\"Perform OCR on a page.\"\"\"\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.factory.PageRasterizationMixin","title":"<code>PageRasterizationMixin</code>","text":"<p>               Bases: <code>AbstractTaskMixin</code>, <code>Generic[TTaskProvider]</code></p> <p>Mixin for page rasterization task.</p> Source code in <code>docprompt/tasks/factory.py</code> <pre><code>class PageRasterizationMixin(AbstractTaskMixin, Generic[TTaskProvider]):\n    \"\"\"Mixin for page rasterization task.\"\"\"\n\n    tags = [PageLevelCapabilities.PAGE_RASTERIZATION]\n\n    @abstractmethod\n    def get_rasterize_page_provider(self, **kwargs) -&gt; TTaskProvider:\n        \"\"\"Perform page rasterization.\"\"\"\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.factory.PageRasterizationMixin.get_rasterize_page_provider","title":"<code>get_rasterize_page_provider(**kwargs)</code>  <code>abstractmethod</code>","text":"<p>Perform page rasterization.</p> Source code in <code>docprompt/tasks/factory.py</code> <pre><code>@abstractmethod\ndef get_rasterize_page_provider(self, **kwargs) -&gt; TTaskProvider:\n    \"\"\"Perform page rasterization.\"\"\"\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.factory.PageSegmentationMixin","title":"<code>PageSegmentationMixin</code>","text":"<p>               Bases: <code>AbstractTaskMixin</code>, <code>Generic[TTaskProvider]</code></p> <p>Mixin for page segmentation task.</p> Source code in <code>docprompt/tasks/factory.py</code> <pre><code>class PageSegmentationMixin(AbstractTaskMixin, Generic[TTaskProvider]):\n    \"\"\"Mixin for page segmentation task.\"\"\"\n\n    tags = [PageLevelCapabilities.PAGE_SEGMENTATION]\n\n    @abstractmethod\n    def get_page_segmentation_provider(self, *args, **kwargs) -&gt; TTaskProvider:\n        \"\"\"Perform page segmentation.\"\"\"\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.factory.PageSegmentationMixin.get_page_segmentation_provider","title":"<code>get_page_segmentation_provider(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Perform page segmentation.</p> Source code in <code>docprompt/tasks/factory.py</code> <pre><code>@abstractmethod\ndef get_page_segmentation_provider(self, *args, **kwargs) -&gt; TTaskProvider:\n    \"\"\"Perform page segmentation.\"\"\"\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.factory.PageTableExtractionMixin","title":"<code>PageTableExtractionMixin</code>","text":"<p>               Bases: <code>AbstractTaskMixin</code>, <code>Generic[TTaskProvider]</code></p> <p>Mixin for page table extraction task.</p> Source code in <code>docprompt/tasks/factory.py</code> <pre><code>class PageTableExtractionMixin(AbstractTaskMixin, Generic[TTaskProvider]):\n    \"\"\"Mixin for page table extraction task.\"\"\"\n\n    tags = [PageLevelCapabilities.PAGE_TABLE_EXTRACTION]\n\n    @abstractmethod\n    def get_page_table_extraction_provider(self, *args, **kwargs) -&gt; TTaskProvider:\n        \"\"\"Extract tables from a page.\"\"\"\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.factory.PageTableExtractionMixin.get_page_table_extraction_provider","title":"<code>get_page_table_extraction_provider(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Extract tables from a page.</p> Source code in <code>docprompt/tasks/factory.py</code> <pre><code>@abstractmethod\ndef get_page_table_extraction_provider(self, *args, **kwargs) -&gt; TTaskProvider:\n    \"\"\"Extract tables from a page.\"\"\"\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.factory.PageTableIdentificationMixin","title":"<code>PageTableIdentificationMixin</code>","text":"<p>               Bases: <code>AbstractTaskMixin</code>, <code>Generic[TTaskProvider]</code></p> <p>Mixin for page table identification task.</p> Source code in <code>docprompt/tasks/factory.py</code> <pre><code>class PageTableIdentificationMixin(AbstractTaskMixin, Generic[TTaskProvider]):\n    \"\"\"Mixin for page table identification task.\"\"\"\n\n    tags = [PageLevelCapabilities.PAGE_TABLE_IDENTIFICATION]\n\n    @abstractmethod\n    def get_page_table_identification_provider(self, *args, **kwargs) -&gt; TTaskProvider:\n        \"\"\"Perform page table identification.\"\"\"\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.factory.PageTableIdentificationMixin.get_page_table_identification_provider","title":"<code>get_page_table_identification_provider(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Perform page table identification.</p> Source code in <code>docprompt/tasks/factory.py</code> <pre><code>@abstractmethod\ndef get_page_table_identification_provider(self, *args, **kwargs) -&gt; TTaskProvider:\n    \"\"\"Perform page table identification.\"\"\"\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.factory.PageVQAMixin","title":"<code>PageVQAMixin</code>","text":"<p>               Bases: <code>AbstractTaskMixin</code>, <code>Generic[TTaskProvider]</code></p> <p>Mixin for page VQA task.</p> Source code in <code>docprompt/tasks/factory.py</code> <pre><code>class PageVQAMixin(AbstractTaskMixin, Generic[TTaskProvider]):\n    \"\"\"Mixin for page VQA task.\"\"\"\n\n    tags = [PageLevelCapabilities.PAGE_VQA]\n\n    @abstractmethod\n    def get_page_vqa_provider(self, *args, **kwargs) -&gt; TTaskProvider:\n        \"\"\"Perform page VQA.\"\"\"\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.factory.PageVQAMixin.get_page_vqa_provider","title":"<code>get_page_vqa_provider(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Perform page VQA.</p> Source code in <code>docprompt/tasks/factory.py</code> <pre><code>@abstractmethod\ndef get_page_vqa_provider(self, *args, **kwargs) -&gt; TTaskProvider:\n    \"\"\"Perform page VQA.\"\"\"\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.markerize","title":"<code>markerize</code>","text":""},{"location":"reference/tasks/#docprompt.tasks.markerize.anthropic","title":"<code>anthropic</code>","text":""},{"location":"reference/tasks/#docprompt.tasks.markerize.base","title":"<code>base</code>","text":""},{"location":"reference/tasks/#docprompt.tasks.message","title":"<code>message</code>","text":"<p>The core primatives for any language model interfacing. Docprompt uses these for the prompt garden, but supports free conversion to and from these types from other libaries.</p>"},{"location":"reference/tasks/#docprompt.tasks.message.OpenAIMessage","title":"<code>OpenAIMessage</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>docprompt/tasks/message.py</code> <pre><code>class OpenAIMessage(BaseModel):\n    role: Literal[\"system\", \"user\", \"assistant\"]\n    content: Union[str, List[OpenAIComplexContent]]\n\n    def to_langchain_message(self):\n        try:\n            from langchain.schema import AIMessage, HumanMessage, SystemMessage\n        except ImportError:\n            raise ImportError(\n                \"Could not import langchain.schema. Install with `docprompt[langchain]`\"\n            )\n\n        role_mapping = {\n            \"system\": SystemMessage,\n            \"user\": HumanMessage,\n            \"assistant\": AIMessage,\n        }\n\n        dumped = self.model_dump(mode=\"json\", exclude_unset=True, exclude_none=True)\n\n        return role_mapping[self.role](content=dumped[\"content\"])\n\n    def to_openai(self):\n        return self.model_dump(mode=\"json\", exclude_unset=True, exclude_none=True)\n\n    def to_llamaindex_chat_message(self):\n        try:\n            from llama_index.core.base.llms.types import ChatMessage, MessageRole\n        except ImportError:\n            raise ImportError(\n                \"Could not import llama_index.core. Install with `docprompt[llamaindex]`\"\n            )\n\n        role_mapping = {\n            \"system\": MessageRole.SYSTEM,\n            \"user\": MessageRole.USER,\n            \"assistant\": MessageRole.ASSISTANT,\n        }\n\n        dumped = self.model_dump(mode=\"json\", exclude_unset=True, exclude_none=True)\n\n        return ChatMessage.from_str(\n            content=dumped[\"content\"], role=role_mapping[self.role]\n        )\n\n    @classmethod\n    def from_image_uri(cls, image_uri: str) -&gt; \"OpenAIMessage\":\n        \"\"\"Create an image message from a URI.\n\n        Args:\n            role: The role of the message.\n            image_uri: The URI of the image.\n        \"\"\"\n        image_url = OpenAIImageURL(url=image_uri)\n        content = OpenAIComplexContent(type=\"image_url\", image_url=image_url)\n        message = cls(role=\"user\", content=[content])\n        return message\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.message.OpenAIMessage.from_image_uri","title":"<code>from_image_uri(image_uri)</code>  <code>classmethod</code>","text":"<p>Create an image message from a URI.</p> <p>Parameters:</p> Name Type Description Default <code>role</code> <p>The role of the message.</p> required <code>image_uri</code> <code>str</code> <p>The URI of the image.</p> required Source code in <code>docprompt/tasks/message.py</code> <pre><code>@classmethod\ndef from_image_uri(cls, image_uri: str) -&gt; \"OpenAIMessage\":\n    \"\"\"Create an image message from a URI.\n\n    Args:\n        role: The role of the message.\n        image_uri: The URI of the image.\n    \"\"\"\n    image_url = OpenAIImageURL(url=image_uri)\n    content = OpenAIComplexContent(type=\"image_url\", image_url=image_url)\n    message = cls(role=\"user\", content=[content])\n    return message\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.ocr","title":"<code>ocr</code>","text":""},{"location":"reference/tasks/#docprompt.tasks.ocr.amazon","title":"<code>amazon</code>","text":""},{"location":"reference/tasks/#docprompt.tasks.ocr.base","title":"<code>base</code>","text":""},{"location":"reference/tasks/#docprompt.tasks.ocr.gcp","title":"<code>gcp</code>","text":""},{"location":"reference/tasks/#docprompt.tasks.ocr.gcp.GoogleOcrProvider","title":"<code>GoogleOcrProvider</code>","text":"<p>               Bases: <code>BaseOCRProvider</code></p> Source code in <code>docprompt/tasks/ocr/gcp.py</code> <pre><code>class GoogleOcrProvider(BaseOCRProvider):\n    name = \"gcp_documentai\"\n\n    capabilities = [\n        PageLevelCapabilities.PAGE_TEXT_OCR,\n        PageLevelCapabilities.PAGE_LAYOUT_OCR,\n        PageLevelCapabilities.PAGE_RASTERIZATION,\n    ]\n\n    max_bytes_per_request: ClassVar[int] = (\n        1024 * 1024 * 20\n    )  # 20MB is the max size for a single sync request\n    max_page_count: ClassVar[int] = 15\n\n    project_id: str = Field(...)\n    processor_id: str = Field(...)\n\n    service_account_info: Optional[Dict[str, str]] = Field(None)\n    service_account_file: Optional[str] = Field(None)\n    location: str = Field(\"us\")\n    max_workers: int = Field(multiprocessing.cpu_count() * 2)\n    exclude_bounding_poly: bool = Field(False)\n    return_images: bool = Field(False)\n    return_image_quality_scores: bool = Field(False)\n\n    _documentai: \"documentai.DocumentProcessorServiceClient\" = PrivateAttr()\n\n    def __init__(\n        self,\n        project_id: str,\n        processor_id: str,\n        **kwargs,\n    ):\n        super().__init__(project_id=project_id, processor_id=processor_id, **kwargs)\n\n        self.service_account_info = self._default_invoke_kwargs.get(\n            \"service_account_info\", None\n        )\n        self.service_account_file = self._default_invoke_kwargs.get(\n            \"service_account_file\", None\n        )\n\n        try:\n            from google.cloud import documentai\n\n            self._documentai = documentai\n        except ImportError:\n            raise ImportError(\n                \"Please install 'google-cloud-documentai' to use the GoogleCloudVisionTextExtractionProvider\"\n            )\n\n    def get_documentai_client(self, client_option_kwargs: dict = {}, **kwargs):\n        from google.api_core.client_options import ClientOptions\n\n        opts = ClientOptions(\n            **{\n                \"api_endpoint\": \"us-documentai.googleapis.com\",\n                **client_option_kwargs,\n            }\n        )\n\n        base_service_client_kwargs = {\n            **kwargs,\n            \"client_options\": opts,\n        }\n\n        if self.service_account_info is not None:\n            return self._documentai.DocumentProcessorServiceClient.from_service_account_info(\n                info=self.service_account_info,\n                **base_service_client_kwargs,\n            )\n        elif self.service_account_file is not None:\n            with service_account_file_read_lock:\n                return self._documentai.DocumentProcessorServiceClient.from_service_account_file(\n                    filename=self.service_account_file,\n                    **base_service_client_kwargs,\n                )\n        else:\n            raise ValueError(\"Missing account info and service file path.\")\n\n    def _get_process_options(self):\n        if not self.return_image_quality_scores:\n            return None\n\n        return self._documentai.ProcessOptions(\n            ocr_config=self._documentai.OcrConfig(\n                enable_image_quality_scores=True,\n            )\n        )\n\n    def _process_document_sync(self, document: Document):\n        \"\"\"\n        Split the document into chunks of 15 pages or less, and process each chunk\n        synchronously.\n        \"\"\"\n        client = self.get_documentai_client()\n        processor_name = client.processor_path(\n            project=self.project_id,\n            location=self.location,\n            processor=self.processor_id,\n        )\n\n        documents: List[\"documentai.Document\"] = []\n\n        file_bytes = document.get_bytes()\n\n        @default_retry_decorator\n        def process_byte_chunk(split_bytes: bytes) -&gt; \"documentai.Document\":\n            raw_document = self._documentai.RawDocument(\n                content=split_bytes,\n                mime_type=\"application/pdf\",\n            )\n\n            field_mask = (\n                \"text,pages.layout,pages.words,pages.lines,pages.tokens,pages.blocks\"\n            )\n\n            if self.return_images:\n                field_mask += \",pages.image\"\n\n            if self.return_image_quality_scores:\n                field_mask += \",image_quality_scores\"\n\n            request = self._documentai.ProcessRequest(\n                name=processor_name,\n                raw_document=raw_document,\n                process_options=self._get_process_options(),\n            )\n\n            result = client.process_document(request=request)\n\n            return result.document\n\n        with tqdm.tqdm(\n            total=len(file_bytes), unit=\"B\", unit_scale=True, desc=\"Processing document\"\n        ) as pbar:\n            for split_bytes in pdf_split_iter_with_max_bytes(\n                file_bytes,\n                max_page_count=self.max_page_count,\n                max_bytes=self.max_bytes_per_request,\n            ):\n                document = process_byte_chunk(split_bytes)\n\n                documents.append(document)\n\n                pbar.update(len(split_bytes))\n\n        return gcp_documents_to_result(\n            documents,\n            self.name,\n            document_name=document.name,\n            file_hash=document.document_hash,\n            exclude_bounding_poly=self.exclude_bounding_poly,\n            return_images=self.return_images,\n        )\n\n    def _process_document_concurrent(\n        self,\n        document: Document,\n        start: Optional[int] = None,\n        stop: Optional[int] = None,\n        include_raster: bool = False,\n    ):\n        # Process page chunks concurrently\n        client = self.get_documentai_client()\n        processor_name = client.processor_path(\n            project=self.project_id,\n            location=self.location,\n            processor=self.processor_id,\n        )\n\n        file_bytes = document.file_bytes\n\n        if document.bytes_per_page &gt; 1024 * 1024 * 2:\n            logger.info(\"Document has few pages but is large, compressing first\")\n            file_bytes = document.to_compressed_bytes()\n\n        logger.info(\"Splitting document into chunks...\")\n        document_byte_splits = list(\n            pdf_split_iter_with_max_bytes(\n                file_bytes,\n                max_page_count=self.max_page_count,\n                max_bytes=self.max_bytes_per_request,\n            )\n        )\n\n        max_workers = min(len(document_byte_splits), self.max_workers)\n\n        @default_retry_decorator\n        def process_byte_chunk(split_bytes: bytes):\n            raw_document = self._documentai.RawDocument(\n                content=split_bytes,\n                mime_type=\"application/pdf\",\n            )\n\n            field_mask = (\n                \"text,pages.layout,pages.words,pages.lines,pages.tokens,pages.blocks\"\n            )\n\n            if self.return_images:\n                field_mask += \",pages.image\"\n\n            if self.return_image_quality_scores:\n                field_mask += \",image_quality_scores\"\n\n            request = self._documentai.ProcessRequest(\n                name=processor_name,\n                raw_document=raw_document,\n                process_options=self._get_process_options(),\n            )\n\n            result = client.process_document(request=request)\n\n            document = result.document\n\n            return document\n\n        logger.info(f\"Processing {len(document_byte_splits)} chunks...\")\n        with tqdm.tqdm(\n            total=len(document_byte_splits), desc=\"Processing document\"\n        ) as pbar:\n            with ThreadPoolExecutor(max_workers=max_workers) as executor:\n                future_to_index = {\n                    executor.submit(process_byte_chunk, split): index\n                    for index, split in enumerate(document_byte_splits)\n                }\n\n                documents: List[\"documentai.Document\"] = [None] * len(\n                    document_byte_splits\n                )  # type: ignore\n\n                for future in as_completed(future_to_index):\n                    index = future_to_index[future]\n                    documents[index] = future.result()\n                    pbar.update(1)\n\n        logger.info(\"Recombining OCR results...\")\n        return gcp_documents_to_result(\n            documents,\n            self.name,\n            document_name=document.name,\n            file_hash=document.document_hash,\n            exclude_bounding_poly=self.exclude_bounding_poly,\n            return_images=self.return_images,\n        )\n\n    def _invoke(\n        self,\n        input: List[PdfDocument],\n        config: None = None,\n        start: Optional[int] = None,\n        stop: Optional[int] = None,\n        **kwargs,\n    ):\n        if len(input) != 1:\n            raise ValueError(\n                \"GoogleOcrProvider only supports processing a single document at a time.\"\n            )\n\n        return self._process_document_concurrent(input[0], start=start, stop=stop)\n\n    def process_document_node(\n        self,\n        document_node: \"DocumentNode\",\n        task_config: None = None,\n        start: Optional[int] = None,\n        stop: Optional[int] = None,\n        contribute_to_document: bool = True,\n        **kwargs,\n    ) -&gt; Dict[int, OcrPageResult]:\n        base_result = self.invoke(\n            [document_node.document.file_bytes], start=start, stop=stop, **kwargs\n        )\n\n        # For OCR, we also need to populate the ocr_results for powered search\n        self._populate_ocr_results(document_node, base_result)\n\n        return base_result\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.ocr.gcp.text_from_layout","title":"<code>text_from_layout(layout, document_text, offset=0)</code>","text":"<p>Offset is used to account for the fact that text references are relative to the entire document.</p> Source code in <code>docprompt/tasks/ocr/gcp.py</code> <pre><code>def text_from_layout(\n    layout: Union[\"documentai.Document.Page.Layout\", \"documentai.Document.Page.Token\"],\n    document_text: str,\n    offset: int = 0,\n) -&gt; str:\n    \"\"\"\n    Offset is used to account for the fact that text references\n    are relative to the entire document.\n    \"\"\"\n    working_text = \"\"\n\n    for segment in sorted(layout.text_anchor.text_segments, key=lambda x: x.end_index):\n        start = getattr(segment, \"start_index\", 0)\n        end = segment.end_index\n\n        working_text += document_text[start - offset : end - offset]\n\n    return working_text\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.ocr.result","title":"<code>result</code>","text":""},{"location":"reference/tasks/#docprompt.tasks.ocr.tesseract","title":"<code>tesseract</code>","text":""},{"location":"reference/tasks/#docprompt.tasks.parser","title":"<code>parser</code>","text":"<p>The base output parser that seeks to mimic the langhain implementation.</p>"},{"location":"reference/tasks/#docprompt.tasks.parser.BaseOutputParser","title":"<code>BaseOutputParser</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[TTaskInput, TTaskOutput]</code></p> <p>The output parser for the page classification system.</p> Source code in <code>docprompt/tasks/parser.py</code> <pre><code>class BaseOutputParser(BaseModel, Generic[TTaskInput, TTaskOutput]):\n    \"\"\"The output parser for the page classification system.\"\"\"\n\n    @abstractmethod\n    def from_task_input(\n        cls, task_input: TTaskInput\n    ) -&gt; \"BaseOutputParser[TTaskInput, TTaskOutput]\":\n        \"\"\"Create an output parser from the task input.\"\"\"\n\n    @abstractmethod\n    def parse(self, text: str) -&gt; TTaskOutput:\n        \"\"\"Parse the results of the classification task.\"\"\"\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.parser.BaseOutputParser.from_task_input","title":"<code>from_task_input(task_input)</code>  <code>abstractmethod</code>","text":"<p>Create an output parser from the task input.</p> Source code in <code>docprompt/tasks/parser.py</code> <pre><code>@abstractmethod\ndef from_task_input(\n    cls, task_input: TTaskInput\n) -&gt; \"BaseOutputParser[TTaskInput, TTaskOutput]\":\n    \"\"\"Create an output parser from the task input.\"\"\"\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.parser.BaseOutputParser.parse","title":"<code>parse(text)</code>  <code>abstractmethod</code>","text":"<p>Parse the results of the classification task.</p> Source code in <code>docprompt/tasks/parser.py</code> <pre><code>@abstractmethod\ndef parse(self, text: str) -&gt; TTaskOutput:\n    \"\"\"Parse the results of the classification task.\"\"\"\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.result","title":"<code>result</code>","text":""},{"location":"reference/tasks/#docprompt.tasks.result.BaseResult","title":"<code>BaseResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>docprompt/tasks/result.py</code> <pre><code>class BaseResult(BaseModel):\n    provider_name: str = Field(\n        description=\"The name of the provider which produced the result\"\n    )\n    when: datetime = Field(\n        default_factory=datetime.now, description=\"The time the result was produced\"\n    )\n\n    task_name: ClassVar[str]\n\n    @property\n    def task_key(self):\n        return f\"{self.provider_name}_{self.task_name}\"\n\n    @abstractmethod\n    def contribute_to_document_node(\n        self, document_node: \"DocumentNode\", **kwargs\n    ) -&gt; None:\n        \"\"\"\n        Contribute this task result to the document node or a specific page node.\n\n        :param document_node: The DocumentNode to contribute to\n        :param page_number: If provided, contribute to a specific page. If None, contribute to the document.\n        \"\"\"\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.result.BaseResult.contribute_to_document_node","title":"<code>contribute_to_document_node(document_node, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Contribute this task result to the document node or a specific page node.</p> <p>:param document_node: The DocumentNode to contribute to :param page_number: If provided, contribute to a specific page. If None, contribute to the document.</p> Source code in <code>docprompt/tasks/result.py</code> <pre><code>@abstractmethod\ndef contribute_to_document_node(\n    self, document_node: \"DocumentNode\", **kwargs\n) -&gt; None:\n    \"\"\"\n    Contribute this task result to the document node or a specific page node.\n\n    :param document_node: The DocumentNode to contribute to\n    :param page_number: If provided, contribute to a specific page. If None, contribute to the document.\n    \"\"\"\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.result.ResultContainer","title":"<code>ResultContainer</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[PageOrDocumentTaskResult]</code></p> <p>Represents a container for results of a task</p> Source code in <code>docprompt/tasks/result.py</code> <pre><code>class ResultContainer(BaseModel, Generic[PageOrDocumentTaskResult]):\n    \"\"\"\n    Represents a container for results of a task\n    \"\"\"\n\n    results: Dict[str, PageOrDocumentTaskResult] = Field(\n        description=\"The results of the task, keyed by provider\", default_factory=dict\n    )\n\n    @property\n    def result(self):\n        return next(iter(self.results.values()), None)\n</code></pre>"},{"location":"reference/tasks/#docprompt.tasks.table_extraction","title":"<code>table_extraction</code>","text":""},{"location":"reference/tasks/#docprompt.tasks.table_extraction.anthropic","title":"<code>anthropic</code>","text":""},{"location":"reference/tasks/#docprompt.tasks.table_extraction.base","title":"<code>base</code>","text":""},{"location":"reference/tasks/#docprompt.tasks.table_extraction.schema","title":"<code>schema</code>","text":""},{"location":"reference/tasks/base/","title":"base","text":""},{"location":"reference/tasks/base/#docprompt.tasks.base.AbstractDocumentTaskProvider","title":"<code>AbstractDocumentTaskProvider</code>","text":"<p>               Bases: <code>AbstractTaskProvider</code></p> <p>A task provider performs a specific, repeatable task on a document.</p> Source code in <code>docprompt/tasks/base.py</code> <pre><code>class AbstractDocumentTaskProvider(AbstractTaskProvider):\n    \"\"\"\n    A task provider performs a specific, repeatable task on a document.\n    \"\"\"\n\n    capabilities: ClassVar[List[DocumentLevelCapabilities]]\n\n    # NOTE: We need the stubs defined here for the flexible decorators to work\n    # for now\n\n    class Meta:\n        abstract = True\n</code></pre>"},{"location":"reference/tasks/base/#docprompt.tasks.base.AbstractPageTaskProvider","title":"<code>AbstractPageTaskProvider</code>","text":"<p>               Bases: <code>AbstractTaskProvider</code></p> <p>A page task provider performs a specific, repeatable task on a page.</p> Source code in <code>docprompt/tasks/base.py</code> <pre><code>class AbstractPageTaskProvider(AbstractTaskProvider):\n    \"\"\"\n    A page task provider performs a specific, repeatable task on a page.\n    \"\"\"\n\n    capabilities: ClassVar[List[PageLevelCapabilities]]\n\n    # NOTE: We need the stubs defined here for the flexible decorators to work\n    # for now\n\n    class Meta:\n        abstract = True\n</code></pre>"},{"location":"reference/tasks/base/#docprompt.tasks.base.AbstractTaskProvider","title":"<code>AbstractTaskProvider</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[TTaskInput, TTaskConfig, TTaskResult]</code></p> <p>A task provider performs a specific, repeatable task on a document or its pages.</p> <p>NOTE: Either the <code>process_document_pages</code> or <code>aprocess_document_pages</code> method must be implemented in a valid subclass. The <code>process_document_pages</code> method is explicitly defined, while the <code>aprocess_document_pages</code> method is an async version of the same method.</p> <p>If you wish to provide seperate implementations for sync and async, you can define both methods individually, and they will each use their own custom implementation when called. Otherwise, if you only implement one or the other of a flexible method pair, the other will automatically be generated and provided for you at runtime.</p> Source code in <code>docprompt/tasks/base.py</code> <pre><code>@flexible_methods(\n    (\"process_document_node\", \"aprocess_document_node\"),\n    (\"_invoke\", \"_ainvoke\"),\n)\nclass AbstractTaskProvider(BaseModel, Generic[TTaskInput, TTaskConfig, TTaskResult]):\n    \"\"\"\n    A task provider performs a specific, repeatable task on a document or its pages.\n\n    NOTE: Either the `process_document_pages` or `aprocess_document_pages` method must be implemented in\n    a valid subclass. The `process_document_pages` method is explicitly defined, while the `aprocess_document_pages`\n    method is an async version of the same method.\n\n    If you wish to provide seperate implementations for sync and async, you can define both methods individually, and\n    they will each use their own custom implementation when called. Otherwise, if you only implement one or the other of\n    a flexible method pair, the other will automatically be generated and provided for you at runtime.\n    \"\"\"\n\n    name: ClassVar[str]\n    capabilities: ClassVar[List[Capabilites]]\n\n    # TODO: Potentially utilize context here during instantiation from Factory??\n    _default_invoke_kwargs: Dict[str, str] = PrivateAttr()\n\n    class Meta:\n        \"\"\"The meta class is utilized by the flexible methods decorator.\n\n        For all classes that are not concrete implementations, we should set the\n        abstract attribute to True, which will prevent the check from failing when\n        the flexible methods decorator is looking for the implementation of the\n        methods.\n        \"\"\"\n\n        abstract = True\n\n    def __init__(self, invoke_kwargs: Dict[str, str] = None, **data):\n        with init_context({\"invoke_kwargs\": invoke_kwargs or {}}):\n            self.__pydantic_validator__.validate_python(\n                data,\n                self_instance=self,\n                context=_init_context_var.get(),\n            )\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def validate_class_vars(cls, data: Any) -&gt; Any:\n        \"\"\"\n        Ensure that the class has a name and capabilities defined.\n        \"\"\"\n\n        if not hasattr(cls, \"name\"):\n            raise ValueError(\"Task providers must have a name defined\")\n\n        if not hasattr(cls, \"capabilities\"):\n            raise ValueError(\"Task providers must have capabilities defined\")\n\n        if not cls.capabilities:\n            raise ValueError(\"Task providers must have at least one capability defined\")\n\n        return data\n\n    @model_validator(mode=\"after\")\n    def set_invoke_kwargs(self, info: ValidationInfo) -&gt; Self:\n        \"\"\"\n        Set the default invoke kwargs for the task provider.\n        \"\"\"\n        self._default_invoke_kwargs = info.context[\"invoke_kwargs\"]\n        return self\n\n    async def _ainvoke(\n        self,\n        input: Iterable[TTaskInput],\n        config: Optional[TTaskConfig] = None,\n        **kwargs,\n    ) -&gt; List[TTaskResult]:\n        raise NotImplementedError\n\n    async def ainvoke(\n        self,\n        input: Iterable[TTaskInput],\n        config: Optional[TTaskConfig] = None,\n        **kwargs,\n    ) -&gt; List[TTaskResult]:\n        invoke_kwargs = {\n            **self._default_invoke_kwargs,\n            **kwargs,\n        }\n\n        return await self._ainvoke(input, config, **invoke_kwargs)\n\n    def _invoke(\n        self,\n        input: Iterable[TTaskInput],\n        config: Optional[TTaskConfig] = None,\n        **kwargs,\n    ) -&gt; List[TTaskResult]:\n        raise NotImplementedError\n\n    def invoke(\n        self,\n        input: Iterable[TTaskInput],\n        config: Optional[TTaskConfig] = None,\n        **kwargs,\n    ) -&gt; List[TTaskResult]:\n        invoke_kwargs = {\n            **self._default_invoke_kwargs,\n            **kwargs,\n        }\n\n        return self._invoke(input, config, **invoke_kwargs)\n\n    def process_document_node(\n        self,\n        document_node: \"DocumentNode\",\n        task_config: Optional[TTaskConfig] = None,\n        start: Optional[int] = None,\n        stop: Optional[int] = None,\n        contribute_to_document: bool = True,\n        **kwargs,\n    ) -&gt; Dict[int, TTaskResult]:\n        raise NotImplementedError\n\n    async def aprocess_document_node(\n        self,\n        document_node: \"DocumentNode\",\n        task_config: Optional[TTaskConfig] = None,\n        start: Optional[int] = None,\n        stop: Optional[int] = None,\n        contribute_to_document: bool = True,\n        **kwargs,\n    ) -&gt; Dict[int, TTaskResult]:\n        raise NotImplementedError\n</code></pre>"},{"location":"reference/tasks/base/#docprompt.tasks.base.AbstractTaskProvider.Meta","title":"<code>Meta</code>","text":"<p>The meta class is utilized by the flexible methods decorator.</p> <p>For all classes that are not concrete implementations, we should set the abstract attribute to True, which will prevent the check from failing when the flexible methods decorator is looking for the implementation of the methods.</p> Source code in <code>docprompt/tasks/base.py</code> <pre><code>class Meta:\n    \"\"\"The meta class is utilized by the flexible methods decorator.\n\n    For all classes that are not concrete implementations, we should set the\n    abstract attribute to True, which will prevent the check from failing when\n    the flexible methods decorator is looking for the implementation of the\n    methods.\n    \"\"\"\n\n    abstract = True\n</code></pre>"},{"location":"reference/tasks/base/#docprompt.tasks.base.AbstractTaskProvider.set_invoke_kwargs","title":"<code>set_invoke_kwargs(info)</code>","text":"<p>Set the default invoke kwargs for the task provider.</p> Source code in <code>docprompt/tasks/base.py</code> <pre><code>@model_validator(mode=\"after\")\ndef set_invoke_kwargs(self, info: ValidationInfo) -&gt; Self:\n    \"\"\"\n    Set the default invoke kwargs for the task provider.\n    \"\"\"\n    self._default_invoke_kwargs = info.context[\"invoke_kwargs\"]\n    return self\n</code></pre>"},{"location":"reference/tasks/base/#docprompt.tasks.base.AbstractTaskProvider.validate_class_vars","title":"<code>validate_class_vars(data)</code>  <code>classmethod</code>","text":"<p>Ensure that the class has a name and capabilities defined.</p> Source code in <code>docprompt/tasks/base.py</code> <pre><code>@model_validator(mode=\"before\")\n@classmethod\ndef validate_class_vars(cls, data: Any) -&gt; Any:\n    \"\"\"\n    Ensure that the class has a name and capabilities defined.\n    \"\"\"\n\n    if not hasattr(cls, \"name\"):\n        raise ValueError(\"Task providers must have a name defined\")\n\n    if not hasattr(cls, \"capabilities\"):\n        raise ValueError(\"Task providers must have capabilities defined\")\n\n    if not cls.capabilities:\n        raise ValueError(\"Task providers must have at least one capability defined\")\n\n    return data\n</code></pre>"},{"location":"reference/tasks/capabilities/","title":"capabilities","text":""},{"location":"reference/tasks/capabilities/#docprompt.tasks.capabilities.PageLevelCapabilities","title":"<code>PageLevelCapabilities</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Represents a capability that a provider can fulfill</p> Source code in <code>docprompt/tasks/capabilities.py</code> <pre><code>class PageLevelCapabilities(str, Enum):\n    \"\"\"\n    Represents a capability that a provider can fulfill\n    \"\"\"\n\n    PAGE_RASTERIZATION = \"page-rasterization\"\n    PAGE_LAYOUT_OCR = \"page-layout-ocr\"\n    PAGE_TEXT_OCR = \"page-text-ocr\"\n    PAGE_CLASSIFICATION = \"page-classification\"\n    PAGE_MARKERIZATION = \"page-markerization\"\n    PAGE_SEGMENTATION = \"page-segmentation\"\n    PAGE_VQA = \"page-vqa\"\n    PAGE_TABLE_IDENTIFICATION = \"page-table-identification\"\n    PAGE_TABLE_EXTRACTION = \"page-table-extraction\"\n</code></pre>"},{"location":"reference/tasks/credentials/","title":"credentials","text":"<p>The credentials module defines a simple model schema for storing credentials.</p>"},{"location":"reference/tasks/credentials/#docprompt.tasks.credentials.APIKeyCredential","title":"<code>APIKeyCredential</code>","text":"<p>               Bases: <code>BaseCredentials</code></p> <p>The API key credential model.</p> Source code in <code>docprompt/tasks/credentials.py</code> <pre><code>class APIKeyCredential(BaseCredentials):\n    \"\"\"The API key credential model.\"\"\"\n\n    api_key: SecretStr = Field(...)\n\n    def __init__(self, environ_path: Optional[str] = None, **data):\n        api_key = data.get(\"api_key\", None)\n        if api_key is None and environ_path:\n            api_key = os.environ.get(environ_path, None)\n        super().__init__(api_key=api_key)\n</code></pre>"},{"location":"reference/tasks/credentials/#docprompt.tasks.credentials.AWSCredentials","title":"<code>AWSCredentials</code>","text":"<p>               Bases: <code>BaseCredentials</code></p> <p>The AWS credentials model.</p> Source code in <code>docprompt/tasks/credentials.py</code> <pre><code>class AWSCredentials(BaseCredentials):\n    \"\"\"The AWS credentials model.\"\"\"\n\n    aws_access_key_id: Optional[SecretStr] = Field(None)\n    aws_secret_access_key: Optional[SecretStr] = Field(None)\n    aws_session_token: Optional[SecretStr] = Field(None)\n    aws_region: Optional[str] = Field(None)\n\n    def __init__(self, **data):\n        aws_access_key_id = data.get(\n            \"aws_access_key_id\", os.environ.get(\"AWS_ACCESS_KEY_ID\", None)\n        )\n        aws_secret_access_key = data.get(\n            \"aws_secret_access_key\", os.environ.get(\"AWS_SECRET_ACCESS_KEY\", None)\n        )\n        aws_session_token = data.get(\n            \"aws_session_token\", os.environ.get(\"AWS_SESSION_TOKEN\", None)\n        )\n        aws_region = data.get(\"aws_region\", os.environ.get(\"AWS_DEFAULT_REGION\", None))\n        super().__init__(\n            aws_access_key_id=aws_access_key_id,\n            aws_secret_access_key=aws_secret_access_key,\n            aws_session_token=aws_session_token,\n            aws_region=aws_region,\n        )\n\n    @model_validator(mode=\"after\")\n    def _validate_aws_credentials(self) -&gt; Self:\n        \"\"\"Ensure the provided AWS credentials are valid.\"\"\"\n\n        key_pair_is_set = self.aws_access_key_id and self.aws_secret_access_key\n\n        if not key_pair_is_set and not self.aws_session_token:\n            raise ValueError(\n                \"You must provide either an AWS session token or an access key and secret key.\"\n            )\n\n        if key_pair_is_set and not self.aws_region:\n            raise ValueError(\n                \"You must provide an AWS region when using an access key and secret key.\"\n            )\n\n        if key_pair_is_set and self.aws_session_token:\n            raise ValueError(\n                \"You cannot provide both an AWS session token and an access key and secret key.\"\n            )\n\n        return self\n</code></pre>"},{"location":"reference/tasks/credentials/#docprompt.tasks.credentials.BaseCredentials","title":"<code>BaseCredentials</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>The base credentials model.</p> Source code in <code>docprompt/tasks/credentials.py</code> <pre><code>class BaseCredentials(BaseModel):\n    \"\"\"The base credentials model.\"\"\"\n\n    @property\n    def kwargs(self) -&gt; Dict[str, str]:\n        \"\"\"Return the credentials as a dictionary with secrets exposed.\"\"\"\n        data = self.model_dump(exclude_none=True)\n        for key, value in data.items():\n            if isinstance(value, SecretStr):\n                data[key] = value.get_secret_value()\n        return data\n</code></pre>"},{"location":"reference/tasks/credentials/#docprompt.tasks.credentials.BaseCredentials.kwargs","title":"<code>kwargs: Dict[str, str]</code>  <code>property</code>","text":"<p>Return the credentials as a dictionary with secrets exposed.</p>"},{"location":"reference/tasks/credentials/#docprompt.tasks.credentials.GCPServiceFileCredentials","title":"<code>GCPServiceFileCredentials</code>","text":"<p>               Bases: <code>BaseCredentials</code></p> <p>The GCP service account credentials model.</p> Source code in <code>docprompt/tasks/credentials.py</code> <pre><code>class GCPServiceFileCredentials(BaseCredentials):\n    \"\"\"The GCP service account credentials model.\"\"\"\n\n    service_account_info: Optional[Dict[str, str]] = Field(None)\n    service_account_file: Optional[str] = Field(None)\n\n    def __init__(self, **data):\n        service_account_info = data.get(\"service_account_info\", None)\n        service_account_file = data.get(\n            \"service_account_file\", os.environ.get(\"GCP_SERVICE_ACCOUNT_FILE\", None)\n        )\n\n        super().__init__(\n            service_account_info=service_account_info,\n            service_account_file=service_account_file,\n        )\n\n    @model_validator(mode=\"after\")\n    def _validate_gcp_credentials(self) -&gt; Self:\n        \"\"\"Ensure the provided GCP credentials are valid.\"\"\"\n        if self.service_account_info is None and self.service_account_file is None:\n            raise ValueError(\n                \"You must provide either service_account_info or service_account_file. You may set the `GCP_SERVICE_ACCOUNT_FILE` environment variable to the path of the service account file.\"\n            )\n        if (\n            self.service_account_info is not None\n            and self.service_account_file is not None\n        ):\n            raise ValueError(\n                \"You must provide either service_account_info or service_account_file, not both\"\n            )\n        return self\n</code></pre>"},{"location":"reference/tasks/factory/","title":"factory","text":"<p>Define the base factory for creating task providers.</p>"},{"location":"reference/tasks/factory/#docprompt.tasks.factory.AbstractTaskMixin","title":"<code>AbstractTaskMixin</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all task mixins.</p> Source code in <code>docprompt/tasks/factory.py</code> <pre><code>class AbstractTaskMixin(ABC):\n    \"\"\"Base class for all task mixins.\"\"\"\n\n    tags: ClassVar[List[Union[PageLevelCapabilities, DocumentLevelCapabilities]]]\n</code></pre>"},{"location":"reference/tasks/factory/#docprompt.tasks.factory.AbstractTaskProviderFactory","title":"<code>AbstractTaskProviderFactory</code>","text":"<p>               Bases: <code>ABC</code>, <code>BaseModel</code></p> <p>The abstract interface for a provider task factory.</p> <p>We need to define the basic interface for how we can create task providers. The task provider factory is responsible for allowing the creation of task providers for specific backends, (i.e. Anthropic, OpenAI, etc.)</p> Source code in <code>docprompt/tasks/factory.py</code> <pre><code>class AbstractTaskProviderFactory(ABC, BaseModel):\n    \"\"\"The abstract interface for a provider task factory.\n\n    We need to define the basic interface for how we can create task providers. The task provider factory\n    is responsible for allowing the creation of task providers for specific backends, (i.e. Anthropic, OpenAI, etc.)\n    \"\"\"\n\n    def __init__(self, **data):\n        with init_context({\"payload\": data}):\n            self.__pydantic_validator__.validate_python(\n                data,\n                self_instance=self,\n                context=_init_context_var.get(),\n            )\n\n    @abstractmethod\n    @model_validator(mode=\"after\")\n    def _validate_provider(self) -&gt; Self:\n        \"\"\"Validate the provider before returning it.\n\n        This method needs to handle credential validation, to ensure that the provider is properly\n        configured and can be utilized for the tasks it can be used to provide.\n        \"\"\"\n</code></pre>"},{"location":"reference/tasks/factory/#docprompt.tasks.factory.AmazonTaskProviderFactory","title":"<code>AmazonTaskProviderFactory</code>","text":"<p>               Bases: <code>AbstractTaskProviderFactory</code>, <code>PageOCRMixin</code></p> <p>The task provider factory for Amazon.</p> Source code in <code>docprompt/tasks/factory.py</code> <pre><code>class AmazonTaskProviderFactory(AbstractTaskProviderFactory, PageOCRMixin):\n    \"\"\"The task provider factory for Amazon.\"\"\"\n\n    @model_validator(mode=\"after\")\n    def _validate_provider(self, info: ValidationInfo) -&gt; Self:\n        \"\"\"Validate the provider before returning it.\"\"\"\n        _payload = info.context[\"payload\"]\n        self._credentials = AWSCredentials(**_payload)\n\n    def get_page_ocr_provider(self, **kwargs) -&gt; TTaskProvider:\n        \"\"\"Get the page OCR provider.\"\"\"\n        from docprompt.tasks.ocr.amazon import AmazonTextractOCRProvider\n\n        return AmazonTextractOCRProvider(\n            invoke_kwargs=self._credentials.kwargs, **kwargs\n        )\n</code></pre>"},{"location":"reference/tasks/factory/#docprompt.tasks.factory.AmazonTaskProviderFactory.get_page_ocr_provider","title":"<code>get_page_ocr_provider(**kwargs)</code>","text":"<p>Get the page OCR provider.</p> Source code in <code>docprompt/tasks/factory.py</code> <pre><code>def get_page_ocr_provider(self, **kwargs) -&gt; TTaskProvider:\n    \"\"\"Get the page OCR provider.\"\"\"\n    from docprompt.tasks.ocr.amazon import AmazonTextractOCRProvider\n\n    return AmazonTextractOCRProvider(\n        invoke_kwargs=self._credentials.kwargs, **kwargs\n    )\n</code></pre>"},{"location":"reference/tasks/factory/#docprompt.tasks.factory.AnthropicTaskProviderFactory","title":"<code>AnthropicTaskProviderFactory</code>","text":"<p>               Bases: <code>AbstractTaskProviderFactory</code>, <code>PageClassificationMixin</code>, <code>PageMarkerizationMixin</code>, <code>PageTableExtractionMixin</code></p> <p>The task provider factory for Anthropic.</p> <p>NOTE: We can either utilize the standard Anthropic API or we can utilize AWS Bedrock. In the event that a user wants to utilize the standard Anthropic API.</p> Source code in <code>docprompt/tasks/factory.py</code> <pre><code>class AnthropicTaskProviderFactory(\n    AbstractTaskProviderFactory,\n    PageClassificationMixin,\n    PageMarkerizationMixin,\n    PageTableExtractionMixin,\n):\n    \"\"\"The task provider factory for Anthropic.\n\n    NOTE: We can either utilize the standard Anthropic API or we can utilize AWS Bedrock. In the event\n    that a user wants to utilize the standard Anthropic API.\n    \"\"\"\n\n    _credentials: APIKeyCredential = PrivateAttr()\n\n    @model_validator(mode=\"after\")\n    def _validate_provider(self, info: ValidationInfo) -&gt; Self:\n        \"\"\"Validate the provider before returning it.\"\"\"\n        _payload = info.context[\"payload\"]\n        self._credentials = APIKeyCredential(\n            environ_path=\"ANTHROPIC_API_KEY\", **_payload\n        )\n        return self\n\n    def get_page_classification_provider(self, **kwargs) -&gt; TTaskProvider:\n        \"\"\"Get the page classification provider.\"\"\"\n        from docprompt.tasks.classification.anthropic import (\n            AnthropicClassificationProvider,\n        )\n\n        return AnthropicClassificationProvider(invoke_kwargs=self._credentials.kwargs)\n\n    def get_page_table_extraction_provider(self, **kwargs) -&gt; TTaskProvider:\n        \"\"\"Get the page table extraction provider.\"\"\"\n        from docprompt.tasks.table_extraction.anthropic import (\n            AnthropicTableExtractionProvider,\n        )\n\n        return AnthropicTableExtractionProvider(\n            invoke_kwargs=self._credentials.kwargs, **kwargs\n        )\n\n    def get_page_markerization_provider(self, **kwargs) -&gt; TTaskProvider:\n        \"\"\"Get the page markerization provider.\"\"\"\n        from docprompt.tasks.markerize.anthropic import AnthropicMarkerizeProvider\n\n        return AnthropicMarkerizeProvider(\n            invoke_kwargs=self._credentials.kwargs, **kwargs\n        )\n</code></pre>"},{"location":"reference/tasks/factory/#docprompt.tasks.factory.AnthropicTaskProviderFactory.get_page_classification_provider","title":"<code>get_page_classification_provider(**kwargs)</code>","text":"<p>Get the page classification provider.</p> Source code in <code>docprompt/tasks/factory.py</code> <pre><code>def get_page_classification_provider(self, **kwargs) -&gt; TTaskProvider:\n    \"\"\"Get the page classification provider.\"\"\"\n    from docprompt.tasks.classification.anthropic import (\n        AnthropicClassificationProvider,\n    )\n\n    return AnthropicClassificationProvider(invoke_kwargs=self._credentials.kwargs)\n</code></pre>"},{"location":"reference/tasks/factory/#docprompt.tasks.factory.AnthropicTaskProviderFactory.get_page_markerization_provider","title":"<code>get_page_markerization_provider(**kwargs)</code>","text":"<p>Get the page markerization provider.</p> Source code in <code>docprompt/tasks/factory.py</code> <pre><code>def get_page_markerization_provider(self, **kwargs) -&gt; TTaskProvider:\n    \"\"\"Get the page markerization provider.\"\"\"\n    from docprompt.tasks.markerize.anthropic import AnthropicMarkerizeProvider\n\n    return AnthropicMarkerizeProvider(\n        invoke_kwargs=self._credentials.kwargs, **kwargs\n    )\n</code></pre>"},{"location":"reference/tasks/factory/#docprompt.tasks.factory.AnthropicTaskProviderFactory.get_page_table_extraction_provider","title":"<code>get_page_table_extraction_provider(**kwargs)</code>","text":"<p>Get the page table extraction provider.</p> Source code in <code>docprompt/tasks/factory.py</code> <pre><code>def get_page_table_extraction_provider(self, **kwargs) -&gt; TTaskProvider:\n    \"\"\"Get the page table extraction provider.\"\"\"\n    from docprompt.tasks.table_extraction.anthropic import (\n        AnthropicTableExtractionProvider,\n    )\n\n    return AnthropicTableExtractionProvider(\n        invoke_kwargs=self._credentials.kwargs, **kwargs\n    )\n</code></pre>"},{"location":"reference/tasks/factory/#docprompt.tasks.factory.DocumentVQAMixin","title":"<code>DocumentVQAMixin</code>","text":"<p>               Bases: <code>AbstractTaskMixin</code>, <code>Generic[TTaskProvider]</code></p> <p>Mixin for multi-page document VQA task.</p> Source code in <code>docprompt/tasks/factory.py</code> <pre><code>class DocumentVQAMixin(AbstractTaskMixin, Generic[TTaskProvider]):\n    \"\"\"Mixin for multi-page document VQA task.\"\"\"\n\n    tags = [DocumentLevelCapabilities.DOCUMENT_VQA]\n\n    @abstractmethod\n    def get_document_vqa_provider(self, *args, **kwargs) -&gt; TTaskProvider:\n        \"\"\"Perform multi-page document VQA.\"\"\"\n</code></pre>"},{"location":"reference/tasks/factory/#docprompt.tasks.factory.DocumentVQAMixin.get_document_vqa_provider","title":"<code>get_document_vqa_provider(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Perform multi-page document VQA.</p> Source code in <code>docprompt/tasks/factory.py</code> <pre><code>@abstractmethod\ndef get_document_vqa_provider(self, *args, **kwargs) -&gt; TTaskProvider:\n    \"\"\"Perform multi-page document VQA.\"\"\"\n</code></pre>"},{"location":"reference/tasks/factory/#docprompt.tasks.factory.GCPTaskProviderFactory","title":"<code>GCPTaskProviderFactory</code>","text":"<p>               Bases: <code>AbstractTaskProviderFactory</code>, <code>PageOCRMixin</code></p> <p>The task provider factory for GCP.</p> Source code in <code>docprompt/tasks/factory.py</code> <pre><code>class GCPTaskProviderFactory(\n    AbstractTaskProviderFactory,\n    PageOCRMixin,\n):\n    \"\"\"The task provider factory for GCP.\"\"\"\n\n    @model_validator(mode=\"after\")\n    def _validate_provider(self, info: ValidationInfo) -&gt; Self:\n        \"\"\"Validate the provider before returning it.\"\"\"\n        _payload = info.context[\"payload\"]\n        self._credentials = GCPServiceFileCredentials(**_payload)\n        return self\n\n    def get_page_ocr_provider(\n        self, project_id: str, processor_id: str, **kwargs\n    ) -&gt; TTaskProvider:\n        \"\"\"Get the page OCR provider.\"\"\"\n        from docprompt.tasks.ocr.gcp import GoogleOcrProvider\n\n        return GoogleOcrProvider(\n            project_id, processor_id, invoke_kwargs=self._credentials.kwargs, **kwargs\n        )\n</code></pre>"},{"location":"reference/tasks/factory/#docprompt.tasks.factory.GCPTaskProviderFactory.get_page_ocr_provider","title":"<code>get_page_ocr_provider(project_id, processor_id, **kwargs)</code>","text":"<p>Get the page OCR provider.</p> Source code in <code>docprompt/tasks/factory.py</code> <pre><code>def get_page_ocr_provider(\n    self, project_id: str, processor_id: str, **kwargs\n) -&gt; TTaskProvider:\n    \"\"\"Get the page OCR provider.\"\"\"\n    from docprompt.tasks.ocr.gcp import GoogleOcrProvider\n\n    return GoogleOcrProvider(\n        project_id, processor_id, invoke_kwargs=self._credentials.kwargs, **kwargs\n    )\n</code></pre>"},{"location":"reference/tasks/factory/#docprompt.tasks.factory.PageClassificationMixin","title":"<code>PageClassificationMixin</code>","text":"<p>               Bases: <code>AbstractTaskMixin</code>, <code>Generic[TTaskProvider]</code></p> <p>Mixin for page classification task.</p> Source code in <code>docprompt/tasks/factory.py</code> <pre><code>class PageClassificationMixin(AbstractTaskMixin, Generic[TTaskProvider]):\n    \"\"\"Mixin for page classification task.\"\"\"\n\n    tags = [PageLevelCapabilities.PAGE_CLASSIFICATION]\n\n    @abstractmethod\n    def get_page_classification_provider(self, *args, **kwargs) -&gt; TTaskProvider:\n        \"\"\"Perform page classification.\"\"\"\n</code></pre>"},{"location":"reference/tasks/factory/#docprompt.tasks.factory.PageClassificationMixin.get_page_classification_provider","title":"<code>get_page_classification_provider(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Perform page classification.</p> Source code in <code>docprompt/tasks/factory.py</code> <pre><code>@abstractmethod\ndef get_page_classification_provider(self, *args, **kwargs) -&gt; TTaskProvider:\n    \"\"\"Perform page classification.\"\"\"\n</code></pre>"},{"location":"reference/tasks/factory/#docprompt.tasks.factory.PageMarkerizationMixin","title":"<code>PageMarkerizationMixin</code>","text":"<p>               Bases: <code>AbstractTaskMixin</code>, <code>Generic[TTaskProvider]</code></p> <p>Mixin for page markerization task.</p> Source code in <code>docprompt/tasks/factory.py</code> <pre><code>class PageMarkerizationMixin(AbstractTaskMixin, Generic[TTaskProvider]):\n    \"\"\"Mixin for page markerization task.\"\"\"\n\n    tags = [PageLevelCapabilities.PAGE_MARKERIZATION]\n\n    @abstractmethod\n    def get_page_markerization_provider(self, *args, **kwargs) -&gt; TTaskProvider:\n        \"\"\"Perform page markerization.\"\"\"\n</code></pre>"},{"location":"reference/tasks/factory/#docprompt.tasks.factory.PageMarkerizationMixin.get_page_markerization_provider","title":"<code>get_page_markerization_provider(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Perform page markerization.</p> Source code in <code>docprompt/tasks/factory.py</code> <pre><code>@abstractmethod\ndef get_page_markerization_provider(self, *args, **kwargs) -&gt; TTaskProvider:\n    \"\"\"Perform page markerization.\"\"\"\n</code></pre>"},{"location":"reference/tasks/factory/#docprompt.tasks.factory.PageOCRMixin","title":"<code>PageOCRMixin</code>","text":"<p>               Bases: <code>AbstractTaskMixin</code>, <code>Generic[TTaskProvider]</code></p> <p>Mixin for page OCR task.</p> Source code in <code>docprompt/tasks/factory.py</code> <pre><code>class PageOCRMixin(AbstractTaskMixin, Generic[TTaskProvider]):\n    \"\"\"Mixin for page OCR task.\"\"\"\n\n    tags = [PageLevelCapabilities.PAGE_LAYOUT_OCR, PageLevelCapabilities.PAGE_TEXT_OCR]\n\n    @abstractmethod\n    def get_page_ocr_provider(self, *args, **kwargs) -&gt; TTaskProvider:\n        \"\"\"Perform OCR on a page.\"\"\"\n</code></pre>"},{"location":"reference/tasks/factory/#docprompt.tasks.factory.PageOCRMixin.get_page_ocr_provider","title":"<code>get_page_ocr_provider(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Perform OCR on a page.</p> Source code in <code>docprompt/tasks/factory.py</code> <pre><code>@abstractmethod\ndef get_page_ocr_provider(self, *args, **kwargs) -&gt; TTaskProvider:\n    \"\"\"Perform OCR on a page.\"\"\"\n</code></pre>"},{"location":"reference/tasks/factory/#docprompt.tasks.factory.PageRasterizationMixin","title":"<code>PageRasterizationMixin</code>","text":"<p>               Bases: <code>AbstractTaskMixin</code>, <code>Generic[TTaskProvider]</code></p> <p>Mixin for page rasterization task.</p> Source code in <code>docprompt/tasks/factory.py</code> <pre><code>class PageRasterizationMixin(AbstractTaskMixin, Generic[TTaskProvider]):\n    \"\"\"Mixin for page rasterization task.\"\"\"\n\n    tags = [PageLevelCapabilities.PAGE_RASTERIZATION]\n\n    @abstractmethod\n    def get_rasterize_page_provider(self, **kwargs) -&gt; TTaskProvider:\n        \"\"\"Perform page rasterization.\"\"\"\n</code></pre>"},{"location":"reference/tasks/factory/#docprompt.tasks.factory.PageRasterizationMixin.get_rasterize_page_provider","title":"<code>get_rasterize_page_provider(**kwargs)</code>  <code>abstractmethod</code>","text":"<p>Perform page rasterization.</p> Source code in <code>docprompt/tasks/factory.py</code> <pre><code>@abstractmethod\ndef get_rasterize_page_provider(self, **kwargs) -&gt; TTaskProvider:\n    \"\"\"Perform page rasterization.\"\"\"\n</code></pre>"},{"location":"reference/tasks/factory/#docprompt.tasks.factory.PageSegmentationMixin","title":"<code>PageSegmentationMixin</code>","text":"<p>               Bases: <code>AbstractTaskMixin</code>, <code>Generic[TTaskProvider]</code></p> <p>Mixin for page segmentation task.</p> Source code in <code>docprompt/tasks/factory.py</code> <pre><code>class PageSegmentationMixin(AbstractTaskMixin, Generic[TTaskProvider]):\n    \"\"\"Mixin for page segmentation task.\"\"\"\n\n    tags = [PageLevelCapabilities.PAGE_SEGMENTATION]\n\n    @abstractmethod\n    def get_page_segmentation_provider(self, *args, **kwargs) -&gt; TTaskProvider:\n        \"\"\"Perform page segmentation.\"\"\"\n</code></pre>"},{"location":"reference/tasks/factory/#docprompt.tasks.factory.PageSegmentationMixin.get_page_segmentation_provider","title":"<code>get_page_segmentation_provider(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Perform page segmentation.</p> Source code in <code>docprompt/tasks/factory.py</code> <pre><code>@abstractmethod\ndef get_page_segmentation_provider(self, *args, **kwargs) -&gt; TTaskProvider:\n    \"\"\"Perform page segmentation.\"\"\"\n</code></pre>"},{"location":"reference/tasks/factory/#docprompt.tasks.factory.PageTableExtractionMixin","title":"<code>PageTableExtractionMixin</code>","text":"<p>               Bases: <code>AbstractTaskMixin</code>, <code>Generic[TTaskProvider]</code></p> <p>Mixin for page table extraction task.</p> Source code in <code>docprompt/tasks/factory.py</code> <pre><code>class PageTableExtractionMixin(AbstractTaskMixin, Generic[TTaskProvider]):\n    \"\"\"Mixin for page table extraction task.\"\"\"\n\n    tags = [PageLevelCapabilities.PAGE_TABLE_EXTRACTION]\n\n    @abstractmethod\n    def get_page_table_extraction_provider(self, *args, **kwargs) -&gt; TTaskProvider:\n        \"\"\"Extract tables from a page.\"\"\"\n</code></pre>"},{"location":"reference/tasks/factory/#docprompt.tasks.factory.PageTableExtractionMixin.get_page_table_extraction_provider","title":"<code>get_page_table_extraction_provider(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Extract tables from a page.</p> Source code in <code>docprompt/tasks/factory.py</code> <pre><code>@abstractmethod\ndef get_page_table_extraction_provider(self, *args, **kwargs) -&gt; TTaskProvider:\n    \"\"\"Extract tables from a page.\"\"\"\n</code></pre>"},{"location":"reference/tasks/factory/#docprompt.tasks.factory.PageTableIdentificationMixin","title":"<code>PageTableIdentificationMixin</code>","text":"<p>               Bases: <code>AbstractTaskMixin</code>, <code>Generic[TTaskProvider]</code></p> <p>Mixin for page table identification task.</p> Source code in <code>docprompt/tasks/factory.py</code> <pre><code>class PageTableIdentificationMixin(AbstractTaskMixin, Generic[TTaskProvider]):\n    \"\"\"Mixin for page table identification task.\"\"\"\n\n    tags = [PageLevelCapabilities.PAGE_TABLE_IDENTIFICATION]\n\n    @abstractmethod\n    def get_page_table_identification_provider(self, *args, **kwargs) -&gt; TTaskProvider:\n        \"\"\"Perform page table identification.\"\"\"\n</code></pre>"},{"location":"reference/tasks/factory/#docprompt.tasks.factory.PageTableIdentificationMixin.get_page_table_identification_provider","title":"<code>get_page_table_identification_provider(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Perform page table identification.</p> Source code in <code>docprompt/tasks/factory.py</code> <pre><code>@abstractmethod\ndef get_page_table_identification_provider(self, *args, **kwargs) -&gt; TTaskProvider:\n    \"\"\"Perform page table identification.\"\"\"\n</code></pre>"},{"location":"reference/tasks/factory/#docprompt.tasks.factory.PageVQAMixin","title":"<code>PageVQAMixin</code>","text":"<p>               Bases: <code>AbstractTaskMixin</code>, <code>Generic[TTaskProvider]</code></p> <p>Mixin for page VQA task.</p> Source code in <code>docprompt/tasks/factory.py</code> <pre><code>class PageVQAMixin(AbstractTaskMixin, Generic[TTaskProvider]):\n    \"\"\"Mixin for page VQA task.\"\"\"\n\n    tags = [PageLevelCapabilities.PAGE_VQA]\n\n    @abstractmethod\n    def get_page_vqa_provider(self, *args, **kwargs) -&gt; TTaskProvider:\n        \"\"\"Perform page VQA.\"\"\"\n</code></pre>"},{"location":"reference/tasks/factory/#docprompt.tasks.factory.PageVQAMixin.get_page_vqa_provider","title":"<code>get_page_vqa_provider(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Perform page VQA.</p> Source code in <code>docprompt/tasks/factory.py</code> <pre><code>@abstractmethod\ndef get_page_vqa_provider(self, *args, **kwargs) -&gt; TTaskProvider:\n    \"\"\"Perform page VQA.\"\"\"\n</code></pre>"},{"location":"reference/tasks/message/","title":"message","text":"<p>The core primatives for any language model interfacing. Docprompt uses these for the prompt garden, but supports free conversion to and from these types from other libaries.</p>"},{"location":"reference/tasks/message/#docprompt.tasks.message.OpenAIMessage","title":"<code>OpenAIMessage</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>docprompt/tasks/message.py</code> <pre><code>class OpenAIMessage(BaseModel):\n    role: Literal[\"system\", \"user\", \"assistant\"]\n    content: Union[str, List[OpenAIComplexContent]]\n\n    def to_langchain_message(self):\n        try:\n            from langchain.schema import AIMessage, HumanMessage, SystemMessage\n        except ImportError:\n            raise ImportError(\n                \"Could not import langchain.schema. Install with `docprompt[langchain]`\"\n            )\n\n        role_mapping = {\n            \"system\": SystemMessage,\n            \"user\": HumanMessage,\n            \"assistant\": AIMessage,\n        }\n\n        dumped = self.model_dump(mode=\"json\", exclude_unset=True, exclude_none=True)\n\n        return role_mapping[self.role](content=dumped[\"content\"])\n\n    def to_openai(self):\n        return self.model_dump(mode=\"json\", exclude_unset=True, exclude_none=True)\n\n    def to_llamaindex_chat_message(self):\n        try:\n            from llama_index.core.base.llms.types import ChatMessage, MessageRole\n        except ImportError:\n            raise ImportError(\n                \"Could not import llama_index.core. Install with `docprompt[llamaindex]`\"\n            )\n\n        role_mapping = {\n            \"system\": MessageRole.SYSTEM,\n            \"user\": MessageRole.USER,\n            \"assistant\": MessageRole.ASSISTANT,\n        }\n\n        dumped = self.model_dump(mode=\"json\", exclude_unset=True, exclude_none=True)\n\n        return ChatMessage.from_str(\n            content=dumped[\"content\"], role=role_mapping[self.role]\n        )\n\n    @classmethod\n    def from_image_uri(cls, image_uri: str) -&gt; \"OpenAIMessage\":\n        \"\"\"Create an image message from a URI.\n\n        Args:\n            role: The role of the message.\n            image_uri: The URI of the image.\n        \"\"\"\n        image_url = OpenAIImageURL(url=image_uri)\n        content = OpenAIComplexContent(type=\"image_url\", image_url=image_url)\n        message = cls(role=\"user\", content=[content])\n        return message\n</code></pre>"},{"location":"reference/tasks/message/#docprompt.tasks.message.OpenAIMessage.from_image_uri","title":"<code>from_image_uri(image_uri)</code>  <code>classmethod</code>","text":"<p>Create an image message from a URI.</p> <p>Parameters:</p> Name Type Description Default <code>role</code> <p>The role of the message.</p> required <code>image_uri</code> <code>str</code> <p>The URI of the image.</p> required Source code in <code>docprompt/tasks/message.py</code> <pre><code>@classmethod\ndef from_image_uri(cls, image_uri: str) -&gt; \"OpenAIMessage\":\n    \"\"\"Create an image message from a URI.\n\n    Args:\n        role: The role of the message.\n        image_uri: The URI of the image.\n    \"\"\"\n    image_url = OpenAIImageURL(url=image_uri)\n    content = OpenAIComplexContent(type=\"image_url\", image_url=image_url)\n    message = cls(role=\"user\", content=[content])\n    return message\n</code></pre>"},{"location":"reference/tasks/parser/","title":"parser","text":"<p>The base output parser that seeks to mimic the langhain implementation.</p>"},{"location":"reference/tasks/parser/#docprompt.tasks.parser.BaseOutputParser","title":"<code>BaseOutputParser</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[TTaskInput, TTaskOutput]</code></p> <p>The output parser for the page classification system.</p> Source code in <code>docprompt/tasks/parser.py</code> <pre><code>class BaseOutputParser(BaseModel, Generic[TTaskInput, TTaskOutput]):\n    \"\"\"The output parser for the page classification system.\"\"\"\n\n    @abstractmethod\n    def from_task_input(\n        cls, task_input: TTaskInput\n    ) -&gt; \"BaseOutputParser[TTaskInput, TTaskOutput]\":\n        \"\"\"Create an output parser from the task input.\"\"\"\n\n    @abstractmethod\n    def parse(self, text: str) -&gt; TTaskOutput:\n        \"\"\"Parse the results of the classification task.\"\"\"\n</code></pre>"},{"location":"reference/tasks/parser/#docprompt.tasks.parser.BaseOutputParser.from_task_input","title":"<code>from_task_input(task_input)</code>  <code>abstractmethod</code>","text":"<p>Create an output parser from the task input.</p> Source code in <code>docprompt/tasks/parser.py</code> <pre><code>@abstractmethod\ndef from_task_input(\n    cls, task_input: TTaskInput\n) -&gt; \"BaseOutputParser[TTaskInput, TTaskOutput]\":\n    \"\"\"Create an output parser from the task input.\"\"\"\n</code></pre>"},{"location":"reference/tasks/parser/#docprompt.tasks.parser.BaseOutputParser.parse","title":"<code>parse(text)</code>  <code>abstractmethod</code>","text":"<p>Parse the results of the classification task.</p> Source code in <code>docprompt/tasks/parser.py</code> <pre><code>@abstractmethod\ndef parse(self, text: str) -&gt; TTaskOutput:\n    \"\"\"Parse the results of the classification task.\"\"\"\n</code></pre>"},{"location":"reference/tasks/result/","title":"result","text":""},{"location":"reference/tasks/result/#docprompt.tasks.result.BaseResult","title":"<code>BaseResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>docprompt/tasks/result.py</code> <pre><code>class BaseResult(BaseModel):\n    provider_name: str = Field(\n        description=\"The name of the provider which produced the result\"\n    )\n    when: datetime = Field(\n        default_factory=datetime.now, description=\"The time the result was produced\"\n    )\n\n    task_name: ClassVar[str]\n\n    @property\n    def task_key(self):\n        return f\"{self.provider_name}_{self.task_name}\"\n\n    @abstractmethod\n    def contribute_to_document_node(\n        self, document_node: \"DocumentNode\", **kwargs\n    ) -&gt; None:\n        \"\"\"\n        Contribute this task result to the document node or a specific page node.\n\n        :param document_node: The DocumentNode to contribute to\n        :param page_number: If provided, contribute to a specific page. If None, contribute to the document.\n        \"\"\"\n</code></pre>"},{"location":"reference/tasks/result/#docprompt.tasks.result.BaseResult.contribute_to_document_node","title":"<code>contribute_to_document_node(document_node, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Contribute this task result to the document node or a specific page node.</p> <p>:param document_node: The DocumentNode to contribute to :param page_number: If provided, contribute to a specific page. If None, contribute to the document.</p> Source code in <code>docprompt/tasks/result.py</code> <pre><code>@abstractmethod\ndef contribute_to_document_node(\n    self, document_node: \"DocumentNode\", **kwargs\n) -&gt; None:\n    \"\"\"\n    Contribute this task result to the document node or a specific page node.\n\n    :param document_node: The DocumentNode to contribute to\n    :param page_number: If provided, contribute to a specific page. If None, contribute to the document.\n    \"\"\"\n</code></pre>"},{"location":"reference/tasks/result/#docprompt.tasks.result.ResultContainer","title":"<code>ResultContainer</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[PageOrDocumentTaskResult]</code></p> <p>Represents a container for results of a task</p> Source code in <code>docprompt/tasks/result.py</code> <pre><code>class ResultContainer(BaseModel, Generic[PageOrDocumentTaskResult]):\n    \"\"\"\n    Represents a container for results of a task\n    \"\"\"\n\n    results: Dict[str, PageOrDocumentTaskResult] = Field(\n        description=\"The results of the task, keyed by provider\", default_factory=dict\n    )\n\n    @property\n    def result(self):\n        return next(iter(self.results.values()), None)\n</code></pre>"},{"location":"reference/tasks/util/","title":"util","text":""},{"location":"reference/tasks/classification/","title":"Index","text":""},{"location":"reference/tasks/classification/#docprompt.tasks.classification.anthropic","title":"<code>anthropic</code>","text":"<p>The antrhopic implementation of page level calssification.</p>"},{"location":"reference/tasks/classification/#docprompt.tasks.classification.anthropic.AnthropicClassificationProvider","title":"<code>AnthropicClassificationProvider</code>","text":"<p>               Bases: <code>BaseClassificationProvider</code></p> <p>The Anthropic implementation of unscored page classification.</p> Source code in <code>docprompt/tasks/classification/anthropic.py</code> <pre><code>class AnthropicClassificationProvider(BaseClassificationProvider):\n    \"\"\"The Anthropic implementation of unscored page classification.\"\"\"\n\n    name = \"anthropic\"\n\n    async def _ainvoke(\n        self, input: Iterable[bytes], config: ClassificationConfig = None, **kwargs\n    ) -&gt; List[ClassificationOutput]:\n        messages = _prepare_messages(input, config)\n\n        parser = AnthropicPageClassificationOutputParser.from_task_input(\n            config, provider_name=self.name\n        )\n\n        completions = await inference.run_batch_inference_anthropic(messages)\n\n        return [parser.parse(res) for res in completions]\n</code></pre>"},{"location":"reference/tasks/classification/#docprompt.tasks.classification.anthropic.AnthropicPageClassificationOutputParser","title":"<code>AnthropicPageClassificationOutputParser</code>","text":"<p>               Bases: <code>BasePageClassificationOutputParser</code></p> <p>The output parser for the page classification system.</p> Source code in <code>docprompt/tasks/classification/anthropic.py</code> <pre><code>class AnthropicPageClassificationOutputParser(BasePageClassificationOutputParser):\n    \"\"\"The output parser for the page classification system.\"\"\"\n\n    def parse(self, text: str) -&gt; ClassificationOutput:\n        \"\"\"Parse the results of the classification task.\"\"\"\n        pattern = re.compile(r\"Answer: (.+)\")\n        match = pattern.search(text)\n\n        result = self.resolve_match(match)\n\n        if self.confidence:\n            conf_pattern = re.compile(r\"Confidence: (.+)\")\n            conf_match = conf_pattern.search(text)\n            conf_result = self.resolve_confidence(conf_match)\n\n            return ClassificationOutput(\n                type=self.type,\n                labels=result,\n                score=conf_result,\n                provider_name=self.name,\n            )\n\n        return ClassificationOutput(\n            type=self.type, labels=result, provider_name=self.name\n        )\n</code></pre>"},{"location":"reference/tasks/classification/#docprompt.tasks.classification.anthropic.AnthropicPageClassificationOutputParser.parse","title":"<code>parse(text)</code>","text":"<p>Parse the results of the classification task.</p> Source code in <code>docprompt/tasks/classification/anthropic.py</code> <pre><code>def parse(self, text: str) -&gt; ClassificationOutput:\n    \"\"\"Parse the results of the classification task.\"\"\"\n    pattern = re.compile(r\"Answer: (.+)\")\n    match = pattern.search(text)\n\n    result = self.resolve_match(match)\n\n    if self.confidence:\n        conf_pattern = re.compile(r\"Confidence: (.+)\")\n        conf_match = conf_pattern.search(text)\n        conf_result = self.resolve_confidence(conf_match)\n\n        return ClassificationOutput(\n            type=self.type,\n            labels=result,\n            score=conf_result,\n            provider_name=self.name,\n        )\n\n    return ClassificationOutput(\n        type=self.type, labels=result, provider_name=self.name\n    )\n</code></pre>"},{"location":"reference/tasks/classification/#docprompt.tasks.classification.base","title":"<code>base</code>","text":""},{"location":"reference/tasks/classification/#docprompt.tasks.classification.base.BaseClassificationProvider","title":"<code>BaseClassificationProvider</code>","text":"<p>               Bases: <code>AbstractPageTaskProvider[bytes, ClassificationConfig, ClassificationOutput]</code></p> <p>The base classification provider.</p> Source code in <code>docprompt/tasks/classification/base.py</code> <pre><code>class BaseClassificationProvider(\n    AbstractPageTaskProvider[bytes, ClassificationConfig, ClassificationOutput]\n):\n    \"\"\"\n    The base classification provider.\n    \"\"\"\n\n    capabilities = [PageLevelCapabilities.PAGE_CLASSIFICATION]\n\n    class Meta:\n        abstract = True\n\n    def process_document_node(\n        self,\n        document_node: \"DocumentNode\",\n        task_config: ClassificationConfig = None,\n        start: Optional[int] = None,\n        stop: Optional[int] = None,\n        contribute_to_document: bool = True,\n        **kwargs,\n    ):\n        assert (\n            task_config is not None\n        ), \"task_config must be provided for classification tasks\"\n\n        raster_bytes = []\n        for page_number in range(start or 1, (stop or len(document_node)) + 1):\n            image_bytes = document_node.page_nodes[\n                page_number - 1\n            ].rasterizer.rasterize(\"default\")\n            raster_bytes.append(image_bytes)\n\n        # TODO: This is a somewhat dangerous way of requiring these kwargs to be drilled\n        # through, potentially a decorator solution to be had here\n        kwargs = {**self._default_invoke_kwargs, **kwargs}\n        results = self._invoke(raster_bytes, config=task_config, **kwargs)\n\n        return {\n            i: res\n            for i, res in zip(\n                range(start or 1, (stop or len(document_node)) + 1), results\n            )\n        }\n</code></pre>"},{"location":"reference/tasks/classification/#docprompt.tasks.classification.base.BasePageClassificationOutputParser","title":"<code>BasePageClassificationOutputParser</code>","text":"<p>               Bases: <code>ABC</code>, <code>BaseOutputParser[ClassificationConfig, ClassificationOutput]</code></p> <p>The output parser for the page classification system.</p> Source code in <code>docprompt/tasks/classification/base.py</code> <pre><code>class BasePageClassificationOutputParser(\n    ABC, BaseOutputParser[ClassificationConfig, ClassificationOutput]\n):\n    \"\"\"The output parser for the page classification system.\"\"\"\n\n    name: str = Field(...)\n    type: ClassificationTypes = Field(...)\n    labels: LabelType = Field(...)\n    confidence: bool = Field(False)\n\n    @classmethod\n    def from_task_input(cls, task_input: ClassificationConfig, provider_name: str):\n        return cls(\n            type=task_input.type,\n            name=provider_name,\n            labels=task_input.labels,\n            confidence=task_input.confidence,\n        )\n\n    def resolve_match(self, _match: Union[re.Match, None]) -&gt; LabelType:\n        \"\"\"Get the regex pattern for the output parser.\"\"\"\n\n        if not _match:\n            raise ValueError(\"Could not find the answer in the text.\")\n\n        val = _match.group(1)\n        if self.type == ClassificationTypes.BINARY:\n            if val not in self.labels:\n                raise ValueError(f\"Invalid label: {val}\")\n            return val\n\n        elif self.type == ClassificationTypes.SINGLE_LABEL:\n            if val not in self.labels:\n                raise ValueError(f\"Invalid label: {val}\")\n            return val\n\n        elif self.type == ClassificationTypes.MULTI_LABEL:\n            labels = val.split(\", \")\n            for label in labels:\n                if label not in self.labels:\n                    raise ValueError(f\"Invalid label: {label}\")\n            return labels\n        else:\n            raise ValueError(f\"Invalid classification type: {self.type}\")\n\n    def resolve_confidence(self, _match: Union[re.Match, None]) -&gt; ConfidenceLevel:\n        \"\"\"Get the confidence level from the text.\"\"\"\n\n        if not _match:\n            return None\n\n        val = _match.group(1).lower()\n\n        return ConfidenceLevel(val)\n\n    @abstractmethod\n    def parse(self, text: str) -&gt; ClassificationOutput: ...\n</code></pre>"},{"location":"reference/tasks/classification/#docprompt.tasks.classification.base.BasePageClassificationOutputParser.resolve_confidence","title":"<code>resolve_confidence(_match)</code>","text":"<p>Get the confidence level from the text.</p> Source code in <code>docprompt/tasks/classification/base.py</code> <pre><code>def resolve_confidence(self, _match: Union[re.Match, None]) -&gt; ConfidenceLevel:\n    \"\"\"Get the confidence level from the text.\"\"\"\n\n    if not _match:\n        return None\n\n    val = _match.group(1).lower()\n\n    return ConfidenceLevel(val)\n</code></pre>"},{"location":"reference/tasks/classification/#docprompt.tasks.classification.base.BasePageClassificationOutputParser.resolve_match","title":"<code>resolve_match(_match)</code>","text":"<p>Get the regex pattern for the output parser.</p> Source code in <code>docprompt/tasks/classification/base.py</code> <pre><code>def resolve_match(self, _match: Union[re.Match, None]) -&gt; LabelType:\n    \"\"\"Get the regex pattern for the output parser.\"\"\"\n\n    if not _match:\n        raise ValueError(\"Could not find the answer in the text.\")\n\n    val = _match.group(1)\n    if self.type == ClassificationTypes.BINARY:\n        if val not in self.labels:\n            raise ValueError(f\"Invalid label: {val}\")\n        return val\n\n    elif self.type == ClassificationTypes.SINGLE_LABEL:\n        if val not in self.labels:\n            raise ValueError(f\"Invalid label: {val}\")\n        return val\n\n    elif self.type == ClassificationTypes.MULTI_LABEL:\n        labels = val.split(\", \")\n        for label in labels:\n            if label not in self.labels:\n                raise ValueError(f\"Invalid label: {label}\")\n        return labels\n    else:\n        raise ValueError(f\"Invalid classification type: {self.type}\")\n</code></pre>"},{"location":"reference/tasks/classification/#docprompt.tasks.classification.base.ClassificationConfig","title":"<code>ClassificationConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>docprompt/tasks/classification/base.py</code> <pre><code>class ClassificationConfig(BaseModel):\n    type: ClassificationTypes\n    labels: LabelType\n    descriptions: Optional[List[str]] = Field(\n        None, description=\"The descriptions for each label (if any).\"\n    )\n\n    instructions: Optional[str] = Field(\n        None,\n        description=\"Additional instructions to pass to the LLM for the task. Required for Binary Classification.\",\n    )\n\n    confidence: bool = Field(False)\n\n    @model_validator(mode=\"before\")\n    def validate_label_bindings(cls, data: Any) -&gt; Any:\n        \"\"\"Validate the the label/description bindings based on the type.\"\"\"\n\n        classification_type = data.get(\"type\", None)\n        if classification_type == ClassificationTypes.SINGLE_LABEL:\n            labels = data.get(\"labels\", None)\n            if not labels:\n                raise ValueError(\n                    \"labels must be provided for single_label classification\"\n                )\n            return data\n\n        elif classification_type == ClassificationTypes.BINARY:\n            instructions = data.get(\"instructions\", None)\n            if not instructions:\n                raise ValueError(\n                    \"instructions must be provided for binary classification\"\n                )\n            data[\"labels\"] = [\"YES\", \"NO\"]\n            return data\n\n        elif classification_type == ClassificationTypes.MULTI_LABEL:\n            labels = data.get(\"labels\", None)\n            if not labels:\n                raise ValueError(\n                    \"labels must be provided for multi_label classification\"\n                )\n            return data\n\n    @model_validator(mode=\"after\")\n    def validate_descriptions_length(self):\n        if self.descriptions is not None:\n            labels = self.labels\n            if labels is not None and len(self.descriptions) != len(labels):\n                raise ValueError(\"descriptions must have the same length as labels\")\n        return self\n\n    @property\n    def formatted_labels(self):\n        \"\"\"Produce the formatted labels for the prompt template.\"\"\"\n        raw_labels = self.labels\n        if self.descriptions:\n            for label, description in zip(raw_labels, self.descriptions):\n                yield f\"{label}: {description}\"\n        else:\n            yield from raw_labels\n</code></pre>"},{"location":"reference/tasks/classification/#docprompt.tasks.classification.base.ClassificationConfig.formatted_labels","title":"<code>formatted_labels</code>  <code>property</code>","text":"<p>Produce the formatted labels for the prompt template.</p>"},{"location":"reference/tasks/classification/#docprompt.tasks.classification.base.ClassificationConfig.validate_label_bindings","title":"<code>validate_label_bindings(data)</code>","text":"<p>Validate the the label/description bindings based on the type.</p> Source code in <code>docprompt/tasks/classification/base.py</code> <pre><code>@model_validator(mode=\"before\")\ndef validate_label_bindings(cls, data: Any) -&gt; Any:\n    \"\"\"Validate the the label/description bindings based on the type.\"\"\"\n\n    classification_type = data.get(\"type\", None)\n    if classification_type == ClassificationTypes.SINGLE_LABEL:\n        labels = data.get(\"labels\", None)\n        if not labels:\n            raise ValueError(\n                \"labels must be provided for single_label classification\"\n            )\n        return data\n\n    elif classification_type == ClassificationTypes.BINARY:\n        instructions = data.get(\"instructions\", None)\n        if not instructions:\n            raise ValueError(\n                \"instructions must be provided for binary classification\"\n            )\n        data[\"labels\"] = [\"YES\", \"NO\"]\n        return data\n\n    elif classification_type == ClassificationTypes.MULTI_LABEL:\n        labels = data.get(\"labels\", None)\n        if not labels:\n            raise ValueError(\n                \"labels must be provided for multi_label classification\"\n            )\n        return data\n</code></pre>"},{"location":"reference/tasks/classification/#docprompt.tasks.classification.base.ConfidenceLevel","title":"<code>ConfidenceLevel</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>The confidence level of the classification.</p> Source code in <code>docprompt/tasks/classification/base.py</code> <pre><code>class ConfidenceLevel(str, Enum):\n    \"\"\"The confidence level of the classification.\"\"\"\n\n    LOW = \"low\"\n    MEDIUM = \"medium\"\n    HIGH = \"high\"\n</code></pre>"},{"location":"reference/tasks/classification/anthropic/","title":"anthropic","text":"<p>The antrhopic implementation of page level calssification.</p>"},{"location":"reference/tasks/classification/anthropic/#docprompt.tasks.classification.anthropic.AnthropicClassificationProvider","title":"<code>AnthropicClassificationProvider</code>","text":"<p>               Bases: <code>BaseClassificationProvider</code></p> <p>The Anthropic implementation of unscored page classification.</p> Source code in <code>docprompt/tasks/classification/anthropic.py</code> <pre><code>class AnthropicClassificationProvider(BaseClassificationProvider):\n    \"\"\"The Anthropic implementation of unscored page classification.\"\"\"\n\n    name = \"anthropic\"\n\n    async def _ainvoke(\n        self, input: Iterable[bytes], config: ClassificationConfig = None, **kwargs\n    ) -&gt; List[ClassificationOutput]:\n        messages = _prepare_messages(input, config)\n\n        parser = AnthropicPageClassificationOutputParser.from_task_input(\n            config, provider_name=self.name\n        )\n\n        completions = await inference.run_batch_inference_anthropic(messages)\n\n        return [parser.parse(res) for res in completions]\n</code></pre>"},{"location":"reference/tasks/classification/anthropic/#docprompt.tasks.classification.anthropic.AnthropicPageClassificationOutputParser","title":"<code>AnthropicPageClassificationOutputParser</code>","text":"<p>               Bases: <code>BasePageClassificationOutputParser</code></p> <p>The output parser for the page classification system.</p> Source code in <code>docprompt/tasks/classification/anthropic.py</code> <pre><code>class AnthropicPageClassificationOutputParser(BasePageClassificationOutputParser):\n    \"\"\"The output parser for the page classification system.\"\"\"\n\n    def parse(self, text: str) -&gt; ClassificationOutput:\n        \"\"\"Parse the results of the classification task.\"\"\"\n        pattern = re.compile(r\"Answer: (.+)\")\n        match = pattern.search(text)\n\n        result = self.resolve_match(match)\n\n        if self.confidence:\n            conf_pattern = re.compile(r\"Confidence: (.+)\")\n            conf_match = conf_pattern.search(text)\n            conf_result = self.resolve_confidence(conf_match)\n\n            return ClassificationOutput(\n                type=self.type,\n                labels=result,\n                score=conf_result,\n                provider_name=self.name,\n            )\n\n        return ClassificationOutput(\n            type=self.type, labels=result, provider_name=self.name\n        )\n</code></pre>"},{"location":"reference/tasks/classification/anthropic/#docprompt.tasks.classification.anthropic.AnthropicPageClassificationOutputParser.parse","title":"<code>parse(text)</code>","text":"<p>Parse the results of the classification task.</p> Source code in <code>docprompt/tasks/classification/anthropic.py</code> <pre><code>def parse(self, text: str) -&gt; ClassificationOutput:\n    \"\"\"Parse the results of the classification task.\"\"\"\n    pattern = re.compile(r\"Answer: (.+)\")\n    match = pattern.search(text)\n\n    result = self.resolve_match(match)\n\n    if self.confidence:\n        conf_pattern = re.compile(r\"Confidence: (.+)\")\n        conf_match = conf_pattern.search(text)\n        conf_result = self.resolve_confidence(conf_match)\n\n        return ClassificationOutput(\n            type=self.type,\n            labels=result,\n            score=conf_result,\n            provider_name=self.name,\n        )\n\n    return ClassificationOutput(\n        type=self.type, labels=result, provider_name=self.name\n    )\n</code></pre>"},{"location":"reference/tasks/classification/base/","title":"base","text":""},{"location":"reference/tasks/classification/base/#docprompt.tasks.classification.base.BaseClassificationProvider","title":"<code>BaseClassificationProvider</code>","text":"<p>               Bases: <code>AbstractPageTaskProvider[bytes, ClassificationConfig, ClassificationOutput]</code></p> <p>The base classification provider.</p> Source code in <code>docprompt/tasks/classification/base.py</code> <pre><code>class BaseClassificationProvider(\n    AbstractPageTaskProvider[bytes, ClassificationConfig, ClassificationOutput]\n):\n    \"\"\"\n    The base classification provider.\n    \"\"\"\n\n    capabilities = [PageLevelCapabilities.PAGE_CLASSIFICATION]\n\n    class Meta:\n        abstract = True\n\n    def process_document_node(\n        self,\n        document_node: \"DocumentNode\",\n        task_config: ClassificationConfig = None,\n        start: Optional[int] = None,\n        stop: Optional[int] = None,\n        contribute_to_document: bool = True,\n        **kwargs,\n    ):\n        assert (\n            task_config is not None\n        ), \"task_config must be provided for classification tasks\"\n\n        raster_bytes = []\n        for page_number in range(start or 1, (stop or len(document_node)) + 1):\n            image_bytes = document_node.page_nodes[\n                page_number - 1\n            ].rasterizer.rasterize(\"default\")\n            raster_bytes.append(image_bytes)\n\n        # TODO: This is a somewhat dangerous way of requiring these kwargs to be drilled\n        # through, potentially a decorator solution to be had here\n        kwargs = {**self._default_invoke_kwargs, **kwargs}\n        results = self._invoke(raster_bytes, config=task_config, **kwargs)\n\n        return {\n            i: res\n            for i, res in zip(\n                range(start or 1, (stop or len(document_node)) + 1), results\n            )\n        }\n</code></pre>"},{"location":"reference/tasks/classification/base/#docprompt.tasks.classification.base.BasePageClassificationOutputParser","title":"<code>BasePageClassificationOutputParser</code>","text":"<p>               Bases: <code>ABC</code>, <code>BaseOutputParser[ClassificationConfig, ClassificationOutput]</code></p> <p>The output parser for the page classification system.</p> Source code in <code>docprompt/tasks/classification/base.py</code> <pre><code>class BasePageClassificationOutputParser(\n    ABC, BaseOutputParser[ClassificationConfig, ClassificationOutput]\n):\n    \"\"\"The output parser for the page classification system.\"\"\"\n\n    name: str = Field(...)\n    type: ClassificationTypes = Field(...)\n    labels: LabelType = Field(...)\n    confidence: bool = Field(False)\n\n    @classmethod\n    def from_task_input(cls, task_input: ClassificationConfig, provider_name: str):\n        return cls(\n            type=task_input.type,\n            name=provider_name,\n            labels=task_input.labels,\n            confidence=task_input.confidence,\n        )\n\n    def resolve_match(self, _match: Union[re.Match, None]) -&gt; LabelType:\n        \"\"\"Get the regex pattern for the output parser.\"\"\"\n\n        if not _match:\n            raise ValueError(\"Could not find the answer in the text.\")\n\n        val = _match.group(1)\n        if self.type == ClassificationTypes.BINARY:\n            if val not in self.labels:\n                raise ValueError(f\"Invalid label: {val}\")\n            return val\n\n        elif self.type == ClassificationTypes.SINGLE_LABEL:\n            if val not in self.labels:\n                raise ValueError(f\"Invalid label: {val}\")\n            return val\n\n        elif self.type == ClassificationTypes.MULTI_LABEL:\n            labels = val.split(\", \")\n            for label in labels:\n                if label not in self.labels:\n                    raise ValueError(f\"Invalid label: {label}\")\n            return labels\n        else:\n            raise ValueError(f\"Invalid classification type: {self.type}\")\n\n    def resolve_confidence(self, _match: Union[re.Match, None]) -&gt; ConfidenceLevel:\n        \"\"\"Get the confidence level from the text.\"\"\"\n\n        if not _match:\n            return None\n\n        val = _match.group(1).lower()\n\n        return ConfidenceLevel(val)\n\n    @abstractmethod\n    def parse(self, text: str) -&gt; ClassificationOutput: ...\n</code></pre>"},{"location":"reference/tasks/classification/base/#docprompt.tasks.classification.base.BasePageClassificationOutputParser.resolve_confidence","title":"<code>resolve_confidence(_match)</code>","text":"<p>Get the confidence level from the text.</p> Source code in <code>docprompt/tasks/classification/base.py</code> <pre><code>def resolve_confidence(self, _match: Union[re.Match, None]) -&gt; ConfidenceLevel:\n    \"\"\"Get the confidence level from the text.\"\"\"\n\n    if not _match:\n        return None\n\n    val = _match.group(1).lower()\n\n    return ConfidenceLevel(val)\n</code></pre>"},{"location":"reference/tasks/classification/base/#docprompt.tasks.classification.base.BasePageClassificationOutputParser.resolve_match","title":"<code>resolve_match(_match)</code>","text":"<p>Get the regex pattern for the output parser.</p> Source code in <code>docprompt/tasks/classification/base.py</code> <pre><code>def resolve_match(self, _match: Union[re.Match, None]) -&gt; LabelType:\n    \"\"\"Get the regex pattern for the output parser.\"\"\"\n\n    if not _match:\n        raise ValueError(\"Could not find the answer in the text.\")\n\n    val = _match.group(1)\n    if self.type == ClassificationTypes.BINARY:\n        if val not in self.labels:\n            raise ValueError(f\"Invalid label: {val}\")\n        return val\n\n    elif self.type == ClassificationTypes.SINGLE_LABEL:\n        if val not in self.labels:\n            raise ValueError(f\"Invalid label: {val}\")\n        return val\n\n    elif self.type == ClassificationTypes.MULTI_LABEL:\n        labels = val.split(\", \")\n        for label in labels:\n            if label not in self.labels:\n                raise ValueError(f\"Invalid label: {label}\")\n        return labels\n    else:\n        raise ValueError(f\"Invalid classification type: {self.type}\")\n</code></pre>"},{"location":"reference/tasks/classification/base/#docprompt.tasks.classification.base.ClassificationConfig","title":"<code>ClassificationConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>docprompt/tasks/classification/base.py</code> <pre><code>class ClassificationConfig(BaseModel):\n    type: ClassificationTypes\n    labels: LabelType\n    descriptions: Optional[List[str]] = Field(\n        None, description=\"The descriptions for each label (if any).\"\n    )\n\n    instructions: Optional[str] = Field(\n        None,\n        description=\"Additional instructions to pass to the LLM for the task. Required for Binary Classification.\",\n    )\n\n    confidence: bool = Field(False)\n\n    @model_validator(mode=\"before\")\n    def validate_label_bindings(cls, data: Any) -&gt; Any:\n        \"\"\"Validate the the label/description bindings based on the type.\"\"\"\n\n        classification_type = data.get(\"type\", None)\n        if classification_type == ClassificationTypes.SINGLE_LABEL:\n            labels = data.get(\"labels\", None)\n            if not labels:\n                raise ValueError(\n                    \"labels must be provided for single_label classification\"\n                )\n            return data\n\n        elif classification_type == ClassificationTypes.BINARY:\n            instructions = data.get(\"instructions\", None)\n            if not instructions:\n                raise ValueError(\n                    \"instructions must be provided for binary classification\"\n                )\n            data[\"labels\"] = [\"YES\", \"NO\"]\n            return data\n\n        elif classification_type == ClassificationTypes.MULTI_LABEL:\n            labels = data.get(\"labels\", None)\n            if not labels:\n                raise ValueError(\n                    \"labels must be provided for multi_label classification\"\n                )\n            return data\n\n    @model_validator(mode=\"after\")\n    def validate_descriptions_length(self):\n        if self.descriptions is not None:\n            labels = self.labels\n            if labels is not None and len(self.descriptions) != len(labels):\n                raise ValueError(\"descriptions must have the same length as labels\")\n        return self\n\n    @property\n    def formatted_labels(self):\n        \"\"\"Produce the formatted labels for the prompt template.\"\"\"\n        raw_labels = self.labels\n        if self.descriptions:\n            for label, description in zip(raw_labels, self.descriptions):\n                yield f\"{label}: {description}\"\n        else:\n            yield from raw_labels\n</code></pre>"},{"location":"reference/tasks/classification/base/#docprompt.tasks.classification.base.ClassificationConfig.formatted_labels","title":"<code>formatted_labels</code>  <code>property</code>","text":"<p>Produce the formatted labels for the prompt template.</p>"},{"location":"reference/tasks/classification/base/#docprompt.tasks.classification.base.ClassificationConfig.validate_label_bindings","title":"<code>validate_label_bindings(data)</code>","text":"<p>Validate the the label/description bindings based on the type.</p> Source code in <code>docprompt/tasks/classification/base.py</code> <pre><code>@model_validator(mode=\"before\")\ndef validate_label_bindings(cls, data: Any) -&gt; Any:\n    \"\"\"Validate the the label/description bindings based on the type.\"\"\"\n\n    classification_type = data.get(\"type\", None)\n    if classification_type == ClassificationTypes.SINGLE_LABEL:\n        labels = data.get(\"labels\", None)\n        if not labels:\n            raise ValueError(\n                \"labels must be provided for single_label classification\"\n            )\n        return data\n\n    elif classification_type == ClassificationTypes.BINARY:\n        instructions = data.get(\"instructions\", None)\n        if not instructions:\n            raise ValueError(\n                \"instructions must be provided for binary classification\"\n            )\n        data[\"labels\"] = [\"YES\", \"NO\"]\n        return data\n\n    elif classification_type == ClassificationTypes.MULTI_LABEL:\n        labels = data.get(\"labels\", None)\n        if not labels:\n            raise ValueError(\n                \"labels must be provided for multi_label classification\"\n            )\n        return data\n</code></pre>"},{"location":"reference/tasks/classification/base/#docprompt.tasks.classification.base.ConfidenceLevel","title":"<code>ConfidenceLevel</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>The confidence level of the classification.</p> Source code in <code>docprompt/tasks/classification/base.py</code> <pre><code>class ConfidenceLevel(str, Enum):\n    \"\"\"The confidence level of the classification.\"\"\"\n\n    LOW = \"low\"\n    MEDIUM = \"medium\"\n    HIGH = \"high\"\n</code></pre>"},{"location":"reference/tasks/markerize/","title":"Index","text":""},{"location":"reference/tasks/markerize/#docprompt.tasks.markerize.anthropic","title":"<code>anthropic</code>","text":""},{"location":"reference/tasks/markerize/#docprompt.tasks.markerize.base","title":"<code>base</code>","text":""},{"location":"reference/tasks/markerize/anthropic/","title":"anthropic","text":""},{"location":"reference/tasks/markerize/base/","title":"base","text":""},{"location":"reference/tasks/ocr/","title":"Index","text":""},{"location":"reference/tasks/ocr/#docprompt.tasks.ocr.amazon","title":"<code>amazon</code>","text":""},{"location":"reference/tasks/ocr/#docprompt.tasks.ocr.base","title":"<code>base</code>","text":""},{"location":"reference/tasks/ocr/#docprompt.tasks.ocr.gcp","title":"<code>gcp</code>","text":""},{"location":"reference/tasks/ocr/#docprompt.tasks.ocr.gcp.GoogleOcrProvider","title":"<code>GoogleOcrProvider</code>","text":"<p>               Bases: <code>BaseOCRProvider</code></p> Source code in <code>docprompt/tasks/ocr/gcp.py</code> <pre><code>class GoogleOcrProvider(BaseOCRProvider):\n    name = \"gcp_documentai\"\n\n    capabilities = [\n        PageLevelCapabilities.PAGE_TEXT_OCR,\n        PageLevelCapabilities.PAGE_LAYOUT_OCR,\n        PageLevelCapabilities.PAGE_RASTERIZATION,\n    ]\n\n    max_bytes_per_request: ClassVar[int] = (\n        1024 * 1024 * 20\n    )  # 20MB is the max size for a single sync request\n    max_page_count: ClassVar[int] = 15\n\n    project_id: str = Field(...)\n    processor_id: str = Field(...)\n\n    service_account_info: Optional[Dict[str, str]] = Field(None)\n    service_account_file: Optional[str] = Field(None)\n    location: str = Field(\"us\")\n    max_workers: int = Field(multiprocessing.cpu_count() * 2)\n    exclude_bounding_poly: bool = Field(False)\n    return_images: bool = Field(False)\n    return_image_quality_scores: bool = Field(False)\n\n    _documentai: \"documentai.DocumentProcessorServiceClient\" = PrivateAttr()\n\n    def __init__(\n        self,\n        project_id: str,\n        processor_id: str,\n        **kwargs,\n    ):\n        super().__init__(project_id=project_id, processor_id=processor_id, **kwargs)\n\n        self.service_account_info = self._default_invoke_kwargs.get(\n            \"service_account_info\", None\n        )\n        self.service_account_file = self._default_invoke_kwargs.get(\n            \"service_account_file\", None\n        )\n\n        try:\n            from google.cloud import documentai\n\n            self._documentai = documentai\n        except ImportError:\n            raise ImportError(\n                \"Please install 'google-cloud-documentai' to use the GoogleCloudVisionTextExtractionProvider\"\n            )\n\n    def get_documentai_client(self, client_option_kwargs: dict = {}, **kwargs):\n        from google.api_core.client_options import ClientOptions\n\n        opts = ClientOptions(\n            **{\n                \"api_endpoint\": \"us-documentai.googleapis.com\",\n                **client_option_kwargs,\n            }\n        )\n\n        base_service_client_kwargs = {\n            **kwargs,\n            \"client_options\": opts,\n        }\n\n        if self.service_account_info is not None:\n            return self._documentai.DocumentProcessorServiceClient.from_service_account_info(\n                info=self.service_account_info,\n                **base_service_client_kwargs,\n            )\n        elif self.service_account_file is not None:\n            with service_account_file_read_lock:\n                return self._documentai.DocumentProcessorServiceClient.from_service_account_file(\n                    filename=self.service_account_file,\n                    **base_service_client_kwargs,\n                )\n        else:\n            raise ValueError(\"Missing account info and service file path.\")\n\n    def _get_process_options(self):\n        if not self.return_image_quality_scores:\n            return None\n\n        return self._documentai.ProcessOptions(\n            ocr_config=self._documentai.OcrConfig(\n                enable_image_quality_scores=True,\n            )\n        )\n\n    def _process_document_sync(self, document: Document):\n        \"\"\"\n        Split the document into chunks of 15 pages or less, and process each chunk\n        synchronously.\n        \"\"\"\n        client = self.get_documentai_client()\n        processor_name = client.processor_path(\n            project=self.project_id,\n            location=self.location,\n            processor=self.processor_id,\n        )\n\n        documents: List[\"documentai.Document\"] = []\n\n        file_bytes = document.get_bytes()\n\n        @default_retry_decorator\n        def process_byte_chunk(split_bytes: bytes) -&gt; \"documentai.Document\":\n            raw_document = self._documentai.RawDocument(\n                content=split_bytes,\n                mime_type=\"application/pdf\",\n            )\n\n            field_mask = (\n                \"text,pages.layout,pages.words,pages.lines,pages.tokens,pages.blocks\"\n            )\n\n            if self.return_images:\n                field_mask += \",pages.image\"\n\n            if self.return_image_quality_scores:\n                field_mask += \",image_quality_scores\"\n\n            request = self._documentai.ProcessRequest(\n                name=processor_name,\n                raw_document=raw_document,\n                process_options=self._get_process_options(),\n            )\n\n            result = client.process_document(request=request)\n\n            return result.document\n\n        with tqdm.tqdm(\n            total=len(file_bytes), unit=\"B\", unit_scale=True, desc=\"Processing document\"\n        ) as pbar:\n            for split_bytes in pdf_split_iter_with_max_bytes(\n                file_bytes,\n                max_page_count=self.max_page_count,\n                max_bytes=self.max_bytes_per_request,\n            ):\n                document = process_byte_chunk(split_bytes)\n\n                documents.append(document)\n\n                pbar.update(len(split_bytes))\n\n        return gcp_documents_to_result(\n            documents,\n            self.name,\n            document_name=document.name,\n            file_hash=document.document_hash,\n            exclude_bounding_poly=self.exclude_bounding_poly,\n            return_images=self.return_images,\n        )\n\n    def _process_document_concurrent(\n        self,\n        document: Document,\n        start: Optional[int] = None,\n        stop: Optional[int] = None,\n        include_raster: bool = False,\n    ):\n        # Process page chunks concurrently\n        client = self.get_documentai_client()\n        processor_name = client.processor_path(\n            project=self.project_id,\n            location=self.location,\n            processor=self.processor_id,\n        )\n\n        file_bytes = document.file_bytes\n\n        if document.bytes_per_page &gt; 1024 * 1024 * 2:\n            logger.info(\"Document has few pages but is large, compressing first\")\n            file_bytes = document.to_compressed_bytes()\n\n        logger.info(\"Splitting document into chunks...\")\n        document_byte_splits = list(\n            pdf_split_iter_with_max_bytes(\n                file_bytes,\n                max_page_count=self.max_page_count,\n                max_bytes=self.max_bytes_per_request,\n            )\n        )\n\n        max_workers = min(len(document_byte_splits), self.max_workers)\n\n        @default_retry_decorator\n        def process_byte_chunk(split_bytes: bytes):\n            raw_document = self._documentai.RawDocument(\n                content=split_bytes,\n                mime_type=\"application/pdf\",\n            )\n\n            field_mask = (\n                \"text,pages.layout,pages.words,pages.lines,pages.tokens,pages.blocks\"\n            )\n\n            if self.return_images:\n                field_mask += \",pages.image\"\n\n            if self.return_image_quality_scores:\n                field_mask += \",image_quality_scores\"\n\n            request = self._documentai.ProcessRequest(\n                name=processor_name,\n                raw_document=raw_document,\n                process_options=self._get_process_options(),\n            )\n\n            result = client.process_document(request=request)\n\n            document = result.document\n\n            return document\n\n        logger.info(f\"Processing {len(document_byte_splits)} chunks...\")\n        with tqdm.tqdm(\n            total=len(document_byte_splits), desc=\"Processing document\"\n        ) as pbar:\n            with ThreadPoolExecutor(max_workers=max_workers) as executor:\n                future_to_index = {\n                    executor.submit(process_byte_chunk, split): index\n                    for index, split in enumerate(document_byte_splits)\n                }\n\n                documents: List[\"documentai.Document\"] = [None] * len(\n                    document_byte_splits\n                )  # type: ignore\n\n                for future in as_completed(future_to_index):\n                    index = future_to_index[future]\n                    documents[index] = future.result()\n                    pbar.update(1)\n\n        logger.info(\"Recombining OCR results...\")\n        return gcp_documents_to_result(\n            documents,\n            self.name,\n            document_name=document.name,\n            file_hash=document.document_hash,\n            exclude_bounding_poly=self.exclude_bounding_poly,\n            return_images=self.return_images,\n        )\n\n    def _invoke(\n        self,\n        input: List[PdfDocument],\n        config: None = None,\n        start: Optional[int] = None,\n        stop: Optional[int] = None,\n        **kwargs,\n    ):\n        if len(input) != 1:\n            raise ValueError(\n                \"GoogleOcrProvider only supports processing a single document at a time.\"\n            )\n\n        return self._process_document_concurrent(input[0], start=start, stop=stop)\n\n    def process_document_node(\n        self,\n        document_node: \"DocumentNode\",\n        task_config: None = None,\n        start: Optional[int] = None,\n        stop: Optional[int] = None,\n        contribute_to_document: bool = True,\n        **kwargs,\n    ) -&gt; Dict[int, OcrPageResult]:\n        base_result = self.invoke(\n            [document_node.document.file_bytes], start=start, stop=stop, **kwargs\n        )\n\n        # For OCR, we also need to populate the ocr_results for powered search\n        self._populate_ocr_results(document_node, base_result)\n\n        return base_result\n</code></pre>"},{"location":"reference/tasks/ocr/#docprompt.tasks.ocr.gcp.text_from_layout","title":"<code>text_from_layout(layout, document_text, offset=0)</code>","text":"<p>Offset is used to account for the fact that text references are relative to the entire document.</p> Source code in <code>docprompt/tasks/ocr/gcp.py</code> <pre><code>def text_from_layout(\n    layout: Union[\"documentai.Document.Page.Layout\", \"documentai.Document.Page.Token\"],\n    document_text: str,\n    offset: int = 0,\n) -&gt; str:\n    \"\"\"\n    Offset is used to account for the fact that text references\n    are relative to the entire document.\n    \"\"\"\n    working_text = \"\"\n\n    for segment in sorted(layout.text_anchor.text_segments, key=lambda x: x.end_index):\n        start = getattr(segment, \"start_index\", 0)\n        end = segment.end_index\n\n        working_text += document_text[start - offset : end - offset]\n\n    return working_text\n</code></pre>"},{"location":"reference/tasks/ocr/#docprompt.tasks.ocr.result","title":"<code>result</code>","text":""},{"location":"reference/tasks/ocr/#docprompt.tasks.ocr.tesseract","title":"<code>tesseract</code>","text":""},{"location":"reference/tasks/ocr/amazon/","title":"amazon","text":""},{"location":"reference/tasks/ocr/base/","title":"base","text":""},{"location":"reference/tasks/ocr/gcp/","title":"gcp","text":""},{"location":"reference/tasks/ocr/gcp/#docprompt.tasks.ocr.gcp.GoogleOcrProvider","title":"<code>GoogleOcrProvider</code>","text":"<p>               Bases: <code>BaseOCRProvider</code></p> Source code in <code>docprompt/tasks/ocr/gcp.py</code> <pre><code>class GoogleOcrProvider(BaseOCRProvider):\n    name = \"gcp_documentai\"\n\n    capabilities = [\n        PageLevelCapabilities.PAGE_TEXT_OCR,\n        PageLevelCapabilities.PAGE_LAYOUT_OCR,\n        PageLevelCapabilities.PAGE_RASTERIZATION,\n    ]\n\n    max_bytes_per_request: ClassVar[int] = (\n        1024 * 1024 * 20\n    )  # 20MB is the max size for a single sync request\n    max_page_count: ClassVar[int] = 15\n\n    project_id: str = Field(...)\n    processor_id: str = Field(...)\n\n    service_account_info: Optional[Dict[str, str]] = Field(None)\n    service_account_file: Optional[str] = Field(None)\n    location: str = Field(\"us\")\n    max_workers: int = Field(multiprocessing.cpu_count() * 2)\n    exclude_bounding_poly: bool = Field(False)\n    return_images: bool = Field(False)\n    return_image_quality_scores: bool = Field(False)\n\n    _documentai: \"documentai.DocumentProcessorServiceClient\" = PrivateAttr()\n\n    def __init__(\n        self,\n        project_id: str,\n        processor_id: str,\n        **kwargs,\n    ):\n        super().__init__(project_id=project_id, processor_id=processor_id, **kwargs)\n\n        self.service_account_info = self._default_invoke_kwargs.get(\n            \"service_account_info\", None\n        )\n        self.service_account_file = self._default_invoke_kwargs.get(\n            \"service_account_file\", None\n        )\n\n        try:\n            from google.cloud import documentai\n\n            self._documentai = documentai\n        except ImportError:\n            raise ImportError(\n                \"Please install 'google-cloud-documentai' to use the GoogleCloudVisionTextExtractionProvider\"\n            )\n\n    def get_documentai_client(self, client_option_kwargs: dict = {}, **kwargs):\n        from google.api_core.client_options import ClientOptions\n\n        opts = ClientOptions(\n            **{\n                \"api_endpoint\": \"us-documentai.googleapis.com\",\n                **client_option_kwargs,\n            }\n        )\n\n        base_service_client_kwargs = {\n            **kwargs,\n            \"client_options\": opts,\n        }\n\n        if self.service_account_info is not None:\n            return self._documentai.DocumentProcessorServiceClient.from_service_account_info(\n                info=self.service_account_info,\n                **base_service_client_kwargs,\n            )\n        elif self.service_account_file is not None:\n            with service_account_file_read_lock:\n                return self._documentai.DocumentProcessorServiceClient.from_service_account_file(\n                    filename=self.service_account_file,\n                    **base_service_client_kwargs,\n                )\n        else:\n            raise ValueError(\"Missing account info and service file path.\")\n\n    def _get_process_options(self):\n        if not self.return_image_quality_scores:\n            return None\n\n        return self._documentai.ProcessOptions(\n            ocr_config=self._documentai.OcrConfig(\n                enable_image_quality_scores=True,\n            )\n        )\n\n    def _process_document_sync(self, document: Document):\n        \"\"\"\n        Split the document into chunks of 15 pages or less, and process each chunk\n        synchronously.\n        \"\"\"\n        client = self.get_documentai_client()\n        processor_name = client.processor_path(\n            project=self.project_id,\n            location=self.location,\n            processor=self.processor_id,\n        )\n\n        documents: List[\"documentai.Document\"] = []\n\n        file_bytes = document.get_bytes()\n\n        @default_retry_decorator\n        def process_byte_chunk(split_bytes: bytes) -&gt; \"documentai.Document\":\n            raw_document = self._documentai.RawDocument(\n                content=split_bytes,\n                mime_type=\"application/pdf\",\n            )\n\n            field_mask = (\n                \"text,pages.layout,pages.words,pages.lines,pages.tokens,pages.blocks\"\n            )\n\n            if self.return_images:\n                field_mask += \",pages.image\"\n\n            if self.return_image_quality_scores:\n                field_mask += \",image_quality_scores\"\n\n            request = self._documentai.ProcessRequest(\n                name=processor_name,\n                raw_document=raw_document,\n                process_options=self._get_process_options(),\n            )\n\n            result = client.process_document(request=request)\n\n            return result.document\n\n        with tqdm.tqdm(\n            total=len(file_bytes), unit=\"B\", unit_scale=True, desc=\"Processing document\"\n        ) as pbar:\n            for split_bytes in pdf_split_iter_with_max_bytes(\n                file_bytes,\n                max_page_count=self.max_page_count,\n                max_bytes=self.max_bytes_per_request,\n            ):\n                document = process_byte_chunk(split_bytes)\n\n                documents.append(document)\n\n                pbar.update(len(split_bytes))\n\n        return gcp_documents_to_result(\n            documents,\n            self.name,\n            document_name=document.name,\n            file_hash=document.document_hash,\n            exclude_bounding_poly=self.exclude_bounding_poly,\n            return_images=self.return_images,\n        )\n\n    def _process_document_concurrent(\n        self,\n        document: Document,\n        start: Optional[int] = None,\n        stop: Optional[int] = None,\n        include_raster: bool = False,\n    ):\n        # Process page chunks concurrently\n        client = self.get_documentai_client()\n        processor_name = client.processor_path(\n            project=self.project_id,\n            location=self.location,\n            processor=self.processor_id,\n        )\n\n        file_bytes = document.file_bytes\n\n        if document.bytes_per_page &gt; 1024 * 1024 * 2:\n            logger.info(\"Document has few pages but is large, compressing first\")\n            file_bytes = document.to_compressed_bytes()\n\n        logger.info(\"Splitting document into chunks...\")\n        document_byte_splits = list(\n            pdf_split_iter_with_max_bytes(\n                file_bytes,\n                max_page_count=self.max_page_count,\n                max_bytes=self.max_bytes_per_request,\n            )\n        )\n\n        max_workers = min(len(document_byte_splits), self.max_workers)\n\n        @default_retry_decorator\n        def process_byte_chunk(split_bytes: bytes):\n            raw_document = self._documentai.RawDocument(\n                content=split_bytes,\n                mime_type=\"application/pdf\",\n            )\n\n            field_mask = (\n                \"text,pages.layout,pages.words,pages.lines,pages.tokens,pages.blocks\"\n            )\n\n            if self.return_images:\n                field_mask += \",pages.image\"\n\n            if self.return_image_quality_scores:\n                field_mask += \",image_quality_scores\"\n\n            request = self._documentai.ProcessRequest(\n                name=processor_name,\n                raw_document=raw_document,\n                process_options=self._get_process_options(),\n            )\n\n            result = client.process_document(request=request)\n\n            document = result.document\n\n            return document\n\n        logger.info(f\"Processing {len(document_byte_splits)} chunks...\")\n        with tqdm.tqdm(\n            total=len(document_byte_splits), desc=\"Processing document\"\n        ) as pbar:\n            with ThreadPoolExecutor(max_workers=max_workers) as executor:\n                future_to_index = {\n                    executor.submit(process_byte_chunk, split): index\n                    for index, split in enumerate(document_byte_splits)\n                }\n\n                documents: List[\"documentai.Document\"] = [None] * len(\n                    document_byte_splits\n                )  # type: ignore\n\n                for future in as_completed(future_to_index):\n                    index = future_to_index[future]\n                    documents[index] = future.result()\n                    pbar.update(1)\n\n        logger.info(\"Recombining OCR results...\")\n        return gcp_documents_to_result(\n            documents,\n            self.name,\n            document_name=document.name,\n            file_hash=document.document_hash,\n            exclude_bounding_poly=self.exclude_bounding_poly,\n            return_images=self.return_images,\n        )\n\n    def _invoke(\n        self,\n        input: List[PdfDocument],\n        config: None = None,\n        start: Optional[int] = None,\n        stop: Optional[int] = None,\n        **kwargs,\n    ):\n        if len(input) != 1:\n            raise ValueError(\n                \"GoogleOcrProvider only supports processing a single document at a time.\"\n            )\n\n        return self._process_document_concurrent(input[0], start=start, stop=stop)\n\n    def process_document_node(\n        self,\n        document_node: \"DocumentNode\",\n        task_config: None = None,\n        start: Optional[int] = None,\n        stop: Optional[int] = None,\n        contribute_to_document: bool = True,\n        **kwargs,\n    ) -&gt; Dict[int, OcrPageResult]:\n        base_result = self.invoke(\n            [document_node.document.file_bytes], start=start, stop=stop, **kwargs\n        )\n\n        # For OCR, we also need to populate the ocr_results for powered search\n        self._populate_ocr_results(document_node, base_result)\n\n        return base_result\n</code></pre>"},{"location":"reference/tasks/ocr/gcp/#docprompt.tasks.ocr.gcp.text_from_layout","title":"<code>text_from_layout(layout, document_text, offset=0)</code>","text":"<p>Offset is used to account for the fact that text references are relative to the entire document.</p> Source code in <code>docprompt/tasks/ocr/gcp.py</code> <pre><code>def text_from_layout(\n    layout: Union[\"documentai.Document.Page.Layout\", \"documentai.Document.Page.Token\"],\n    document_text: str,\n    offset: int = 0,\n) -&gt; str:\n    \"\"\"\n    Offset is used to account for the fact that text references\n    are relative to the entire document.\n    \"\"\"\n    working_text = \"\"\n\n    for segment in sorted(layout.text_anchor.text_segments, key=lambda x: x.end_index):\n        start = getattr(segment, \"start_index\", 0)\n        end = segment.end_index\n\n        working_text += document_text[start - offset : end - offset]\n\n    return working_text\n</code></pre>"},{"location":"reference/tasks/ocr/result/","title":"result","text":""},{"location":"reference/tasks/ocr/tesseract/","title":"tesseract","text":""},{"location":"reference/tasks/table_extraction/","title":"Index","text":""},{"location":"reference/tasks/table_extraction/#docprompt.tasks.table_extraction.anthropic","title":"<code>anthropic</code>","text":""},{"location":"reference/tasks/table_extraction/#docprompt.tasks.table_extraction.base","title":"<code>base</code>","text":""},{"location":"reference/tasks/table_extraction/#docprompt.tasks.table_extraction.schema","title":"<code>schema</code>","text":""},{"location":"reference/tasks/table_extraction/anthropic/","title":"anthropic","text":""},{"location":"reference/tasks/table_extraction/base/","title":"base","text":""},{"location":"reference/tasks/table_extraction/schema/","title":"schema","text":""},{"location":"reference/utils/","title":"Index","text":""},{"location":"reference/utils/#docprompt.utils.extract_dates_from_text","title":"<code>extract_dates_from_text(input_string, *, date_formats=default_date_formats)</code>","text":"<p>Extract dates from a string using a set of predefined regex patterns.</p> <p>Returns a list of tuples, where the first element is the date object and the second is the full date string.</p> Source code in <code>docprompt/utils/date_extraction.py</code> <pre><code>def extract_dates_from_text(\n    input_string: str, *, date_formats: DateFormatsType = default_date_formats\n) -&gt; List[Tuple[date, str]]:\n    \"\"\"\n    Extract dates from a string using a set of predefined regex patterns.\n\n    Returns a list of tuples, where the first element is the date object and the second is the full date string.\n    \"\"\"\n    extracted_dates = []\n\n    for regex, date_format in date_formats:\n        matches = regex.findall(input_string)\n\n        for match_obj in matches:\n            # Extract the full date from the match\n            full_date = match_obj[0]  # First group captures the entire date\n\n            if \"%d\" in date_format:\n                parse_date = re.sub(r\"(st|nd|rd|th)\", \"\", full_date)\n            else:\n                parse_date = full_date\n\n            parse_date = re.sub(r\"\\s+\", \" \", parse_date).strip()\n            parse_date = re.sub(\n                r\"\\s{1,},\", \",\", parse_date\n            ).strip()  # Commas shouldnt have spaces before them\n\n            # Convert to datetime object\n            try:\n                date_obj = datetime.strptime(parse_date, date_format)\n            except ValueError as e:\n                print(f\"Error parsing date '{full_date}': {e}\")\n                continue\n\n            extracted_dates.append((date_obj.date(), full_date))\n\n    return extracted_dates\n</code></pre>"},{"location":"reference/utils/#docprompt.utils.get_page_count","title":"<code>get_page_count(fd)</code>","text":"<p>Determines the number of pages in a PDF</p> Source code in <code>docprompt/utils/util.py</code> <pre><code>def get_page_count(fd: Union[Path, PathLike, bytes]) -&gt; int:\n    \"\"\"\n    Determines the number of pages in a PDF\n    \"\"\"\n    if not isinstance(fd, bytes):\n        with open(fd, \"rb\") as f:\n            fd = f.read()\n\n    with get_pdfium_document(fd) as pdf:\n        return len(pdf)\n</code></pre>"},{"location":"reference/utils/#docprompt.utils.hash_from_bytes","title":"<code>hash_from_bytes(byte_data, hash_func=hashlib.md5, threshold=1024 * 1024 * 128)</code>","text":"<p>Gets a hash from bytes. If the bytes are larger than the threshold, the hash is computed in chunks to avoid memory issues. The default hash function is MD5 with a threshold of 128MB which is optimal for most machines and use cases.</p> Source code in <code>docprompt/utils/util.py</code> <pre><code>def hash_from_bytes(\n    byte_data: bytes, hash_func=hashlib.md5, threshold=1024 * 1024 * 128\n) -&gt; str:\n    \"\"\"\n    Gets a hash from bytes. If the bytes are larger than the threshold, the hash is computed in chunks\n    to avoid memory issues. The default hash function is MD5 with a threshold of 128MB which is optimal\n    for most machines and use cases.\n    \"\"\"\n    if len(byte_data) &lt; 1024 * 1024 * 10:  # 10MB\n        return hashlib.md5(byte_data).hexdigest()\n\n    hash = hash_func()\n\n    if len(byte_data) &gt; threshold:\n        stream = BytesIO(byte_data)\n        b = bytearray(128 * 1024)\n        mv = memoryview(b)\n\n        while n := stream.readinto(mv):\n            hash.update(mv[:n])\n    else:\n        hash.update(byte_data)\n\n    return hash.hexdigest()\n</code></pre>"},{"location":"reference/utils/#docprompt.utils.is_pdf","title":"<code>is_pdf(fd)</code>","text":"<p>Determines if a file is a PDF</p> Source code in <code>docprompt/utils/util.py</code> <pre><code>def is_pdf(fd: Union[Path, PathLike, bytes]) -&gt; bool:\n    \"\"\"\n    Determines if a file is a PDF\n    \"\"\"\n    if isinstance(fd, (bytes, str)):\n        mime = filetype.guess_mime(fd)\n    else:\n        with open(fd, \"rb\") as f:\n            # We only need the first 1024 bytes to determine if it's a PDF\n            mime = filetype.guess_mime(f.read(1024))\n\n    return mime == \"application/pdf\"\n</code></pre>"},{"location":"reference/utils/#docprompt.utils.load_pdf_document","title":"<code>load_pdf_document(fp, *, file_name=None, password=None)</code>","text":"<p>Loads a document from a file path</p> Source code in <code>docprompt/utils/util.py</code> <pre><code>def load_pdf_document(\n    fp: Union[Path, PathLike, bytes],\n    *,\n    file_name: Optional[str] = None,\n    password: Optional[str] = None,\n) -&gt; PdfDocument:\n    \"\"\"\n    Loads a document from a file path\n    \"\"\"\n    if isinstance(fp, bytes):\n        file_bytes = fp\n        file_name = file_name or determine_pdf_name_from_bytes(file_bytes)\n    else:\n        file_name = name_from_path(fp) if file_name is None else file_name\n\n        file_bytes = read_pdf_bytes_from_path(fp)\n\n    if not is_pdf(file_bytes):\n        raise ValueError(\"File is not a PDF\")\n\n    return PdfDocument(\n        name=unquote(file_name),\n        file_path=str(fp),\n        file_bytes=file_bytes,\n        password=password,\n    )\n</code></pre>"},{"location":"reference/utils/#docprompt.utils.load_pdf_documents","title":"<code>load_pdf_documents(fps, *, max_threads=12, passwords=None)</code>","text":"<p>Loads multiple documents from file paths, using a thread pool</p> Source code in <code>docprompt/utils/util.py</code> <pre><code>def load_pdf_documents(\n    fps: List[Union[Path, PathLike, bytes]],\n    *,\n    max_threads: int = 12,\n    passwords: Optional[List[str]] = None,\n):\n    \"\"\"\n    Loads multiple documents from file paths, using a thread pool\n    \"\"\"\n    futures = []\n\n    thread_count = min(max_threads, len(fps))\n\n    with ThreadPoolExecutor(max_workers=thread_count) as executor:\n        for fp in fps:\n            futures.append(executor.submit(load_document, fp))\n\n    results = []\n\n    for future in as_completed(futures):\n        results.append(future.result())\n\n    return results\n</code></pre>"},{"location":"reference/utils/#docprompt.utils.date_extraction","title":"<code>date_extraction</code>","text":""},{"location":"reference/utils/#docprompt.utils.date_extraction.extract_dates_from_text","title":"<code>extract_dates_from_text(input_string, *, date_formats=default_date_formats)</code>","text":"<p>Extract dates from a string using a set of predefined regex patterns.</p> <p>Returns a list of tuples, where the first element is the date object and the second is the full date string.</p> Source code in <code>docprompt/utils/date_extraction.py</code> <pre><code>def extract_dates_from_text(\n    input_string: str, *, date_formats: DateFormatsType = default_date_formats\n) -&gt; List[Tuple[date, str]]:\n    \"\"\"\n    Extract dates from a string using a set of predefined regex patterns.\n\n    Returns a list of tuples, where the first element is the date object and the second is the full date string.\n    \"\"\"\n    extracted_dates = []\n\n    for regex, date_format in date_formats:\n        matches = regex.findall(input_string)\n\n        for match_obj in matches:\n            # Extract the full date from the match\n            full_date = match_obj[0]  # First group captures the entire date\n\n            if \"%d\" in date_format:\n                parse_date = re.sub(r\"(st|nd|rd|th)\", \"\", full_date)\n            else:\n                parse_date = full_date\n\n            parse_date = re.sub(r\"\\s+\", \" \", parse_date).strip()\n            parse_date = re.sub(\n                r\"\\s{1,},\", \",\", parse_date\n            ).strip()  # Commas shouldnt have spaces before them\n\n            # Convert to datetime object\n            try:\n                date_obj = datetime.strptime(parse_date, date_format)\n            except ValueError as e:\n                print(f\"Error parsing date '{full_date}': {e}\")\n                continue\n\n            extracted_dates.append((date_obj.date(), full_date))\n\n    return extracted_dates\n</code></pre>"},{"location":"reference/utils/#docprompt.utils.inference","title":"<code>inference</code>","text":"<p>A utility file for running inference with various LLM providers.</p>"},{"location":"reference/utils/#docprompt.utils.inference.run_batch_inference_anthropic","title":"<code>run_batch_inference_anthropic(model_name, messages, **kwargs)</code>  <code>async</code>","text":"<p>Run batch inference using an Anthropic model asynchronously.</p> Source code in <code>docprompt/utils/inference.py</code> <pre><code>async def run_batch_inference_anthropic(\n    model_name: str, messages: List[List[OpenAIMessage]], **kwargs\n) -&gt; List[str]:\n    \"\"\"Run batch inference using an Anthropic model asynchronously.\"\"\"\n    retry_decorator = get_anthropic_retry_decorator()\n\n    @retry_decorator\n    async def process_message_set(msg_set):\n        return await run_inference_anthropic(model_name, msg_set, **kwargs)\n\n    tasks = [process_message_set(msg_set) for msg_set in messages]\n\n    responses: List[str] = []\n    for f in tqdm(asyncio.as_completed(tasks), desc=\"Processing messages\"):\n        response = await f\n        responses.append(response)\n\n    return responses\n</code></pre>"},{"location":"reference/utils/#docprompt.utils.inference.run_inference_anthropic","title":"<code>run_inference_anthropic(model_name, messages, **kwargs)</code>  <code>async</code>","text":"<p>Run inference using an Anthropic model asynchronously.</p> Source code in <code>docprompt/utils/inference.py</code> <pre><code>async def run_inference_anthropic(\n    model_name: str, messages: List[OpenAIMessage], **kwargs\n) -&gt; str:\n    \"\"\"Run inference using an Anthropic model asynchronously.\"\"\"\n    from anthropic import AsyncAnthropic\n\n    api_key = kwargs.pop(\"api_key\", os.environ.get(\"ANTHROPIC_API_KEY\"))\n    client = AsyncAnthropic(api_key=api_key)\n\n    system = None\n    if messages and messages[0].role == \"system\":\n        system = messages[0].content\n        messages = messages[1:]\n\n    processed_messages = []\n    for msg in messages:\n        if isinstance(msg.content, list):\n            processed_content = []\n            for content in msg.content:\n                if isinstance(content, OpenAIComplexContent):\n                    content = content.to_anthropic_message()\n                    processed_content.append(content)\n                else:\n                    pass\n                    # raise ValueError(f\"Invalid content type: {type(content)} Expected OpenAIComplexContent\")\n\n            dumped = msg.model_dump()\n            dumped[\"content\"] = processed_content\n            processed_messages.append(dumped)\n        else:\n            processed_messages.append(msg)\n\n    client_kwargs = {\n        \"model\": model_name,\n        \"max_tokens\": 2048,\n        \"messages\": processed_messages,\n        **kwargs,\n    }\n\n    if system:\n        client_kwargs[\"system\"] = system\n\n    response = await client.messages.create(**client_kwargs)\n\n    content = response.content[0].text\n\n    return content\n</code></pre>"},{"location":"reference/utils/#docprompt.utils.masking","title":"<code>masking</code>","text":""},{"location":"reference/utils/#docprompt.utils.masking.image","title":"<code>image</code>","text":""},{"location":"reference/utils/#docprompt.utils.masking.image.mask_image_from_bounding_boxes","title":"<code>mask_image_from_bounding_boxes(image, *bounding_boxes, mask_color='#000000')</code>","text":"<p>Create a copy of the image with the positions of the bounding boxes masked.</p> Source code in <code>docprompt/utils/masking/image.py</code> <pre><code>def mask_image_from_bounding_boxes(\n    image: Image.Image,\n    *bounding_boxes: NormBBox,\n    mask_color: str = \"#000000\",\n):\n    \"\"\"\n    Create a copy of the image with the positions of the bounding boxes masked.\n    \"\"\"\n\n    width, height = image.size\n\n    mask = Image.new(\"RGBA\", (width, height), (0, 0, 0, 0))\n\n    for bbox in bounding_boxes:\n        mask.paste(\n            Image.new(\"RGBA\", (bbox.width, bbox.height), mask_color),\n            (int(bbox.x0 * width), int(bbox.top * height)),\n        )\n\n    return Image.alpha_composite(image, mask)\n</code></pre>"},{"location":"reference/utils/#docprompt.utils.splitter","title":"<code>splitter</code>","text":""},{"location":"reference/utils/#docprompt.utils.splitter.pdf_split_iter_fast","title":"<code>pdf_split_iter_fast(file_bytes, max_page_count)</code>","text":"<p>Splits a PDF into batches of pages up to <code>max_page_count</code> pages quickly.</p> Source code in <code>docprompt/utils/splitter.py</code> <pre><code>def pdf_split_iter_fast(file_bytes: bytes, max_page_count: int) -&gt; Iterator[bytes]:\n    \"\"\"\n    Splits a PDF into batches of pages up to `max_page_count` pages quickly.\n    \"\"\"\n    with get_pdfium_document(file_bytes) as src_pdf:\n        current_page = 0\n        total_pages = len(src_pdf)\n\n        while current_page &lt; total_pages:\n            # Determine the last page for the current batch\n            last_page = min(current_page + max_page_count, total_pages)\n\n            with writable_temp_pdf() as dst_pdf:\n                # Append pages to the batch\n                dst_pdf.import_pages(src_pdf, list(range(current_page, last_page)))\n\n                # Save the batch PDF to a bytes buffer\n                pdf_bytes_buffer = io.BytesIO()\n                dst_pdf.save(pdf_bytes_buffer)\n                pdf_bytes_buffer.seek(0)  # Reset buffer pointer to the beginning\n\n            # Yield the bytes of the batch PDF\n            yield pdf_bytes_buffer.getvalue()\n\n            # Update the current page for the next batch\n            current_page += max_page_count\n</code></pre>"},{"location":"reference/utils/#docprompt.utils.splitter.pdf_split_iter_with_max_bytes","title":"<code>pdf_split_iter_with_max_bytes(file_bytes, max_page_count, max_bytes)</code>","text":"<p>Splits a PDF into batches of pages up to <code>max_page_count</code> pages and <code>max_bytes</code> bytes.</p> Source code in <code>docprompt/utils/splitter.py</code> <pre><code>def pdf_split_iter_with_max_bytes(\n    file_bytes: bytes, max_page_count: int, max_bytes: int\n) -&gt; Iterator[bytes]:\n    \"\"\"\n    Splits a PDF into batches of pages up to `max_page_count` pages and `max_bytes` bytes.\n    \"\"\"\n    for batch_bytes in pdf_split_iter_fast(file_bytes, max_page_count):\n        if len(batch_bytes) &lt;= max_bytes:\n            yield batch_bytes\n        else:\n            # If batch size is greater than max_bytes, reduce the number of pages\n            pages_in_batch = max_page_count\n            while len(batch_bytes) &gt; max_bytes and pages_in_batch &gt; 1:\n                pages_in_batch -= 1\n                batch_bytes = next(pdf_split_iter_fast(file_bytes, pages_in_batch))\n\n            if len(batch_bytes) &gt; max_bytes and pages_in_batch == 1:\n                # If a single page is still too large, compress it\n                with tempfile.NamedTemporaryFile(suffix=\".pdf\") as f:\n                    f.write(batch_bytes)\n                    f.flush()\n                    compressed_bytes = compress_pdf_to_bytes(f.name)\n                yield compressed_bytes\n            else:\n                yield batch_bytes\n</code></pre>"},{"location":"reference/utils/#docprompt.utils.splitter.split_pdf_to_bytes","title":"<code>split_pdf_to_bytes(file_bytes, *, start_page=None, stop_page=None)</code>","text":"<p>Splits a PDF into a list of bytes.</p> Source code in <code>docprompt/utils/splitter.py</code> <pre><code>def split_pdf_to_bytes(\n    file_bytes: bytes,\n    *,\n    start_page: Optional[int] = None,\n    stop_page: Optional[int] = None,\n):\n    \"\"\"\n    Splits a PDF into a list of bytes.\n    \"\"\"\n    if start_page is None:\n        start_page = 0\n    if stop_page is None:\n        stop_page = get_page_count(file_bytes)\n\n    if stop_page &lt;= start_page:\n        raise ValueError(\"stop_page must be greater than start_page\")\n\n    # Load the PDF from bytes\n    with get_pdfium_document(file_bytes) as src_pdf:\n        # Create a new PDF for the current batch\n        dst_pdf = pdfium.PdfDocument.new()\n\n        # Append pages to the batch\n        dst_pdf.import_pages(src_pdf, list(range(start_page, stop_page)))\n\n        # Save the batch PDF to a bytes buffer\n        pdf_bytes_buffer = io.BytesIO()\n        dst_pdf.save(pdf_bytes_buffer)\n        pdf_bytes_buffer.seek(0)  # Reset buffer pointer to the beginning\n\n        # Yield the bytes of the batch PDF\n        return pdf_bytes_buffer.getvalue()\n</code></pre>"},{"location":"reference/utils/#docprompt.utils.util","title":"<code>util</code>","text":""},{"location":"reference/utils/#docprompt.utils.util.determine_pdf_name_from_bytes","title":"<code>determine_pdf_name_from_bytes(file_bytes)</code>","text":"<p>Attempts to determine the name of a PDF by exaimining metadata</p> Source code in <code>docprompt/utils/util.py</code> <pre><code>def determine_pdf_name_from_bytes(file_bytes: bytes) -&gt; str:\n    \"\"\"\n    Attempts to determine the name of a PDF by exaimining metadata\n    \"\"\"\n    with get_pdfium_document(file_bytes) as pdf:\n        metadata_dict = pdf.get_metadata_dict(skip_empty=True)\n\n    name = None\n\n    if metadata_dict:\n        name = (\n            metadata_dict.get(\"Title\")\n            or metadata_dict.get(\"Subject\")\n            or metadata_dict.get(\"Author\")\n        )\n\n    if name:\n        return f\"{name.strip()}.pdf\"\n\n    return f\"document-{hash_from_bytes(file_bytes)}.pdf\"\n</code></pre>"},{"location":"reference/utils/#docprompt.utils.util.get_page_count","title":"<code>get_page_count(fd)</code>","text":"<p>Determines the number of pages in a PDF</p> Source code in <code>docprompt/utils/util.py</code> <pre><code>def get_page_count(fd: Union[Path, PathLike, bytes]) -&gt; int:\n    \"\"\"\n    Determines the number of pages in a PDF\n    \"\"\"\n    if not isinstance(fd, bytes):\n        with open(fd, \"rb\") as f:\n            fd = f.read()\n\n    with get_pdfium_document(fd) as pdf:\n        return len(pdf)\n</code></pre>"},{"location":"reference/utils/#docprompt.utils.util.hash_from_bytes","title":"<code>hash_from_bytes(byte_data, hash_func=hashlib.md5, threshold=1024 * 1024 * 128)</code>","text":"<p>Gets a hash from bytes. If the bytes are larger than the threshold, the hash is computed in chunks to avoid memory issues. The default hash function is MD5 with a threshold of 128MB which is optimal for most machines and use cases.</p> Source code in <code>docprompt/utils/util.py</code> <pre><code>def hash_from_bytes(\n    byte_data: bytes, hash_func=hashlib.md5, threshold=1024 * 1024 * 128\n) -&gt; str:\n    \"\"\"\n    Gets a hash from bytes. If the bytes are larger than the threshold, the hash is computed in chunks\n    to avoid memory issues. The default hash function is MD5 with a threshold of 128MB which is optimal\n    for most machines and use cases.\n    \"\"\"\n    if len(byte_data) &lt; 1024 * 1024 * 10:  # 10MB\n        return hashlib.md5(byte_data).hexdigest()\n\n    hash = hash_func()\n\n    if len(byte_data) &gt; threshold:\n        stream = BytesIO(byte_data)\n        b = bytearray(128 * 1024)\n        mv = memoryview(b)\n\n        while n := stream.readinto(mv):\n            hash.update(mv[:n])\n    else:\n        hash.update(byte_data)\n\n    return hash.hexdigest()\n</code></pre>"},{"location":"reference/utils/#docprompt.utils.util.is_pdf","title":"<code>is_pdf(fd)</code>","text":"<p>Determines if a file is a PDF</p> Source code in <code>docprompt/utils/util.py</code> <pre><code>def is_pdf(fd: Union[Path, PathLike, bytes]) -&gt; bool:\n    \"\"\"\n    Determines if a file is a PDF\n    \"\"\"\n    if isinstance(fd, (bytes, str)):\n        mime = filetype.guess_mime(fd)\n    else:\n        with open(fd, \"rb\") as f:\n            # We only need the first 1024 bytes to determine if it's a PDF\n            mime = filetype.guess_mime(f.read(1024))\n\n    return mime == \"application/pdf\"\n</code></pre>"},{"location":"reference/utils/#docprompt.utils.util.load_pdf_document","title":"<code>load_pdf_document(fp, *, file_name=None, password=None)</code>","text":"<p>Loads a document from a file path</p> Source code in <code>docprompt/utils/util.py</code> <pre><code>def load_pdf_document(\n    fp: Union[Path, PathLike, bytes],\n    *,\n    file_name: Optional[str] = None,\n    password: Optional[str] = None,\n) -&gt; PdfDocument:\n    \"\"\"\n    Loads a document from a file path\n    \"\"\"\n    if isinstance(fp, bytes):\n        file_bytes = fp\n        file_name = file_name or determine_pdf_name_from_bytes(file_bytes)\n    else:\n        file_name = name_from_path(fp) if file_name is None else file_name\n\n        file_bytes = read_pdf_bytes_from_path(fp)\n\n    if not is_pdf(file_bytes):\n        raise ValueError(\"File is not a PDF\")\n\n    return PdfDocument(\n        name=unquote(file_name),\n        file_path=str(fp),\n        file_bytes=file_bytes,\n        password=password,\n    )\n</code></pre>"},{"location":"reference/utils/#docprompt.utils.util.load_pdf_documents","title":"<code>load_pdf_documents(fps, *, max_threads=12, passwords=None)</code>","text":"<p>Loads multiple documents from file paths, using a thread pool</p> Source code in <code>docprompt/utils/util.py</code> <pre><code>def load_pdf_documents(\n    fps: List[Union[Path, PathLike, bytes]],\n    *,\n    max_threads: int = 12,\n    passwords: Optional[List[str]] = None,\n):\n    \"\"\"\n    Loads multiple documents from file paths, using a thread pool\n    \"\"\"\n    futures = []\n\n    thread_count = min(max_threads, len(fps))\n\n    with ThreadPoolExecutor(max_workers=thread_count) as executor:\n        for fp in fps:\n            futures.append(executor.submit(load_document, fp))\n\n    results = []\n\n    for future in as_completed(futures):\n        results.append(future.result())\n\n    return results\n</code></pre>"},{"location":"reference/utils/async_utils/","title":"async_utils","text":""},{"location":"reference/utils/compressor/","title":"compressor","text":""},{"location":"reference/utils/date_extraction/","title":"date_extraction","text":""},{"location":"reference/utils/date_extraction/#docprompt.utils.date_extraction.extract_dates_from_text","title":"<code>extract_dates_from_text(input_string, *, date_formats=default_date_formats)</code>","text":"<p>Extract dates from a string using a set of predefined regex patterns.</p> <p>Returns a list of tuples, where the first element is the date object and the second is the full date string.</p> Source code in <code>docprompt/utils/date_extraction.py</code> <pre><code>def extract_dates_from_text(\n    input_string: str, *, date_formats: DateFormatsType = default_date_formats\n) -&gt; List[Tuple[date, str]]:\n    \"\"\"\n    Extract dates from a string using a set of predefined regex patterns.\n\n    Returns a list of tuples, where the first element is the date object and the second is the full date string.\n    \"\"\"\n    extracted_dates = []\n\n    for regex, date_format in date_formats:\n        matches = regex.findall(input_string)\n\n        for match_obj in matches:\n            # Extract the full date from the match\n            full_date = match_obj[0]  # First group captures the entire date\n\n            if \"%d\" in date_format:\n                parse_date = re.sub(r\"(st|nd|rd|th)\", \"\", full_date)\n            else:\n                parse_date = full_date\n\n            parse_date = re.sub(r\"\\s+\", \" \", parse_date).strip()\n            parse_date = re.sub(\n                r\"\\s{1,},\", \",\", parse_date\n            ).strip()  # Commas shouldnt have spaces before them\n\n            # Convert to datetime object\n            try:\n                date_obj = datetime.strptime(parse_date, date_format)\n            except ValueError as e:\n                print(f\"Error parsing date '{full_date}': {e}\")\n                continue\n\n            extracted_dates.append((date_obj.date(), full_date))\n\n    return extracted_dates\n</code></pre>"},{"location":"reference/utils/inference/","title":"inference","text":"<p>A utility file for running inference with various LLM providers.</p>"},{"location":"reference/utils/inference/#docprompt.utils.inference.run_batch_inference_anthropic","title":"<code>run_batch_inference_anthropic(model_name, messages, **kwargs)</code>  <code>async</code>","text":"<p>Run batch inference using an Anthropic model asynchronously.</p> Source code in <code>docprompt/utils/inference.py</code> <pre><code>async def run_batch_inference_anthropic(\n    model_name: str, messages: List[List[OpenAIMessage]], **kwargs\n) -&gt; List[str]:\n    \"\"\"Run batch inference using an Anthropic model asynchronously.\"\"\"\n    retry_decorator = get_anthropic_retry_decorator()\n\n    @retry_decorator\n    async def process_message_set(msg_set):\n        return await run_inference_anthropic(model_name, msg_set, **kwargs)\n\n    tasks = [process_message_set(msg_set) for msg_set in messages]\n\n    responses: List[str] = []\n    for f in tqdm(asyncio.as_completed(tasks), desc=\"Processing messages\"):\n        response = await f\n        responses.append(response)\n\n    return responses\n</code></pre>"},{"location":"reference/utils/inference/#docprompt.utils.inference.run_inference_anthropic","title":"<code>run_inference_anthropic(model_name, messages, **kwargs)</code>  <code>async</code>","text":"<p>Run inference using an Anthropic model asynchronously.</p> Source code in <code>docprompt/utils/inference.py</code> <pre><code>async def run_inference_anthropic(\n    model_name: str, messages: List[OpenAIMessage], **kwargs\n) -&gt; str:\n    \"\"\"Run inference using an Anthropic model asynchronously.\"\"\"\n    from anthropic import AsyncAnthropic\n\n    api_key = kwargs.pop(\"api_key\", os.environ.get(\"ANTHROPIC_API_KEY\"))\n    client = AsyncAnthropic(api_key=api_key)\n\n    system = None\n    if messages and messages[0].role == \"system\":\n        system = messages[0].content\n        messages = messages[1:]\n\n    processed_messages = []\n    for msg in messages:\n        if isinstance(msg.content, list):\n            processed_content = []\n            for content in msg.content:\n                if isinstance(content, OpenAIComplexContent):\n                    content = content.to_anthropic_message()\n                    processed_content.append(content)\n                else:\n                    pass\n                    # raise ValueError(f\"Invalid content type: {type(content)} Expected OpenAIComplexContent\")\n\n            dumped = msg.model_dump()\n            dumped[\"content\"] = processed_content\n            processed_messages.append(dumped)\n        else:\n            processed_messages.append(msg)\n\n    client_kwargs = {\n        \"model\": model_name,\n        \"max_tokens\": 2048,\n        \"messages\": processed_messages,\n        **kwargs,\n    }\n\n    if system:\n        client_kwargs[\"system\"] = system\n\n    response = await client.messages.create(**client_kwargs)\n\n    content = response.content[0].text\n\n    return content\n</code></pre>"},{"location":"reference/utils/splitter/","title":"splitter","text":""},{"location":"reference/utils/splitter/#docprompt.utils.splitter.pdf_split_iter_fast","title":"<code>pdf_split_iter_fast(file_bytes, max_page_count)</code>","text":"<p>Splits a PDF into batches of pages up to <code>max_page_count</code> pages quickly.</p> Source code in <code>docprompt/utils/splitter.py</code> <pre><code>def pdf_split_iter_fast(file_bytes: bytes, max_page_count: int) -&gt; Iterator[bytes]:\n    \"\"\"\n    Splits a PDF into batches of pages up to `max_page_count` pages quickly.\n    \"\"\"\n    with get_pdfium_document(file_bytes) as src_pdf:\n        current_page = 0\n        total_pages = len(src_pdf)\n\n        while current_page &lt; total_pages:\n            # Determine the last page for the current batch\n            last_page = min(current_page + max_page_count, total_pages)\n\n            with writable_temp_pdf() as dst_pdf:\n                # Append pages to the batch\n                dst_pdf.import_pages(src_pdf, list(range(current_page, last_page)))\n\n                # Save the batch PDF to a bytes buffer\n                pdf_bytes_buffer = io.BytesIO()\n                dst_pdf.save(pdf_bytes_buffer)\n                pdf_bytes_buffer.seek(0)  # Reset buffer pointer to the beginning\n\n            # Yield the bytes of the batch PDF\n            yield pdf_bytes_buffer.getvalue()\n\n            # Update the current page for the next batch\n            current_page += max_page_count\n</code></pre>"},{"location":"reference/utils/splitter/#docprompt.utils.splitter.pdf_split_iter_with_max_bytes","title":"<code>pdf_split_iter_with_max_bytes(file_bytes, max_page_count, max_bytes)</code>","text":"<p>Splits a PDF into batches of pages up to <code>max_page_count</code> pages and <code>max_bytes</code> bytes.</p> Source code in <code>docprompt/utils/splitter.py</code> <pre><code>def pdf_split_iter_with_max_bytes(\n    file_bytes: bytes, max_page_count: int, max_bytes: int\n) -&gt; Iterator[bytes]:\n    \"\"\"\n    Splits a PDF into batches of pages up to `max_page_count` pages and `max_bytes` bytes.\n    \"\"\"\n    for batch_bytes in pdf_split_iter_fast(file_bytes, max_page_count):\n        if len(batch_bytes) &lt;= max_bytes:\n            yield batch_bytes\n        else:\n            # If batch size is greater than max_bytes, reduce the number of pages\n            pages_in_batch = max_page_count\n            while len(batch_bytes) &gt; max_bytes and pages_in_batch &gt; 1:\n                pages_in_batch -= 1\n                batch_bytes = next(pdf_split_iter_fast(file_bytes, pages_in_batch))\n\n            if len(batch_bytes) &gt; max_bytes and pages_in_batch == 1:\n                # If a single page is still too large, compress it\n                with tempfile.NamedTemporaryFile(suffix=\".pdf\") as f:\n                    f.write(batch_bytes)\n                    f.flush()\n                    compressed_bytes = compress_pdf_to_bytes(f.name)\n                yield compressed_bytes\n            else:\n                yield batch_bytes\n</code></pre>"},{"location":"reference/utils/splitter/#docprompt.utils.splitter.split_pdf_to_bytes","title":"<code>split_pdf_to_bytes(file_bytes, *, start_page=None, stop_page=None)</code>","text":"<p>Splits a PDF into a list of bytes.</p> Source code in <code>docprompt/utils/splitter.py</code> <pre><code>def split_pdf_to_bytes(\n    file_bytes: bytes,\n    *,\n    start_page: Optional[int] = None,\n    stop_page: Optional[int] = None,\n):\n    \"\"\"\n    Splits a PDF into a list of bytes.\n    \"\"\"\n    if start_page is None:\n        start_page = 0\n    if stop_page is None:\n        stop_page = get_page_count(file_bytes)\n\n    if stop_page &lt;= start_page:\n        raise ValueError(\"stop_page must be greater than start_page\")\n\n    # Load the PDF from bytes\n    with get_pdfium_document(file_bytes) as src_pdf:\n        # Create a new PDF for the current batch\n        dst_pdf = pdfium.PdfDocument.new()\n\n        # Append pages to the batch\n        dst_pdf.import_pages(src_pdf, list(range(start_page, stop_page)))\n\n        # Save the batch PDF to a bytes buffer\n        pdf_bytes_buffer = io.BytesIO()\n        dst_pdf.save(pdf_bytes_buffer)\n        pdf_bytes_buffer.seek(0)  # Reset buffer pointer to the beginning\n\n        # Yield the bytes of the batch PDF\n        return pdf_bytes_buffer.getvalue()\n</code></pre>"},{"location":"reference/utils/util/","title":"util","text":""},{"location":"reference/utils/util/#docprompt.utils.util.determine_pdf_name_from_bytes","title":"<code>determine_pdf_name_from_bytes(file_bytes)</code>","text":"<p>Attempts to determine the name of a PDF by exaimining metadata</p> Source code in <code>docprompt/utils/util.py</code> <pre><code>def determine_pdf_name_from_bytes(file_bytes: bytes) -&gt; str:\n    \"\"\"\n    Attempts to determine the name of a PDF by exaimining metadata\n    \"\"\"\n    with get_pdfium_document(file_bytes) as pdf:\n        metadata_dict = pdf.get_metadata_dict(skip_empty=True)\n\n    name = None\n\n    if metadata_dict:\n        name = (\n            metadata_dict.get(\"Title\")\n            or metadata_dict.get(\"Subject\")\n            or metadata_dict.get(\"Author\")\n        )\n\n    if name:\n        return f\"{name.strip()}.pdf\"\n\n    return f\"document-{hash_from_bytes(file_bytes)}.pdf\"\n</code></pre>"},{"location":"reference/utils/util/#docprompt.utils.util.get_page_count","title":"<code>get_page_count(fd)</code>","text":"<p>Determines the number of pages in a PDF</p> Source code in <code>docprompt/utils/util.py</code> <pre><code>def get_page_count(fd: Union[Path, PathLike, bytes]) -&gt; int:\n    \"\"\"\n    Determines the number of pages in a PDF\n    \"\"\"\n    if not isinstance(fd, bytes):\n        with open(fd, \"rb\") as f:\n            fd = f.read()\n\n    with get_pdfium_document(fd) as pdf:\n        return len(pdf)\n</code></pre>"},{"location":"reference/utils/util/#docprompt.utils.util.hash_from_bytes","title":"<code>hash_from_bytes(byte_data, hash_func=hashlib.md5, threshold=1024 * 1024 * 128)</code>","text":"<p>Gets a hash from bytes. If the bytes are larger than the threshold, the hash is computed in chunks to avoid memory issues. The default hash function is MD5 with a threshold of 128MB which is optimal for most machines and use cases.</p> Source code in <code>docprompt/utils/util.py</code> <pre><code>def hash_from_bytes(\n    byte_data: bytes, hash_func=hashlib.md5, threshold=1024 * 1024 * 128\n) -&gt; str:\n    \"\"\"\n    Gets a hash from bytes. If the bytes are larger than the threshold, the hash is computed in chunks\n    to avoid memory issues. The default hash function is MD5 with a threshold of 128MB which is optimal\n    for most machines and use cases.\n    \"\"\"\n    if len(byte_data) &lt; 1024 * 1024 * 10:  # 10MB\n        return hashlib.md5(byte_data).hexdigest()\n\n    hash = hash_func()\n\n    if len(byte_data) &gt; threshold:\n        stream = BytesIO(byte_data)\n        b = bytearray(128 * 1024)\n        mv = memoryview(b)\n\n        while n := stream.readinto(mv):\n            hash.update(mv[:n])\n    else:\n        hash.update(byte_data)\n\n    return hash.hexdigest()\n</code></pre>"},{"location":"reference/utils/util/#docprompt.utils.util.is_pdf","title":"<code>is_pdf(fd)</code>","text":"<p>Determines if a file is a PDF</p> Source code in <code>docprompt/utils/util.py</code> <pre><code>def is_pdf(fd: Union[Path, PathLike, bytes]) -&gt; bool:\n    \"\"\"\n    Determines if a file is a PDF\n    \"\"\"\n    if isinstance(fd, (bytes, str)):\n        mime = filetype.guess_mime(fd)\n    else:\n        with open(fd, \"rb\") as f:\n            # We only need the first 1024 bytes to determine if it's a PDF\n            mime = filetype.guess_mime(f.read(1024))\n\n    return mime == \"application/pdf\"\n</code></pre>"},{"location":"reference/utils/util/#docprompt.utils.util.load_pdf_document","title":"<code>load_pdf_document(fp, *, file_name=None, password=None)</code>","text":"<p>Loads a document from a file path</p> Source code in <code>docprompt/utils/util.py</code> <pre><code>def load_pdf_document(\n    fp: Union[Path, PathLike, bytes],\n    *,\n    file_name: Optional[str] = None,\n    password: Optional[str] = None,\n) -&gt; PdfDocument:\n    \"\"\"\n    Loads a document from a file path\n    \"\"\"\n    if isinstance(fp, bytes):\n        file_bytes = fp\n        file_name = file_name or determine_pdf_name_from_bytes(file_bytes)\n    else:\n        file_name = name_from_path(fp) if file_name is None else file_name\n\n        file_bytes = read_pdf_bytes_from_path(fp)\n\n    if not is_pdf(file_bytes):\n        raise ValueError(\"File is not a PDF\")\n\n    return PdfDocument(\n        name=unquote(file_name),\n        file_path=str(fp),\n        file_bytes=file_bytes,\n        password=password,\n    )\n</code></pre>"},{"location":"reference/utils/util/#docprompt.utils.util.load_pdf_documents","title":"<code>load_pdf_documents(fps, *, max_threads=12, passwords=None)</code>","text":"<p>Loads multiple documents from file paths, using a thread pool</p> Source code in <code>docprompt/utils/util.py</code> <pre><code>def load_pdf_documents(\n    fps: List[Union[Path, PathLike, bytes]],\n    *,\n    max_threads: int = 12,\n    passwords: Optional[List[str]] = None,\n):\n    \"\"\"\n    Loads multiple documents from file paths, using a thread pool\n    \"\"\"\n    futures = []\n\n    thread_count = min(max_threads, len(fps))\n\n    with ThreadPoolExecutor(max_workers=thread_count) as executor:\n        for fp in fps:\n            futures.append(executor.submit(load_document, fp))\n\n    results = []\n\n    for future in as_completed(futures):\n        results.append(future.result())\n\n    return results\n</code></pre>"},{"location":"reference/utils/masking/","title":"Index","text":""},{"location":"reference/utils/masking/#docprompt.utils.masking.image","title":"<code>image</code>","text":""},{"location":"reference/utils/masking/#docprompt.utils.masking.image.mask_image_from_bounding_boxes","title":"<code>mask_image_from_bounding_boxes(image, *bounding_boxes, mask_color='#000000')</code>","text":"<p>Create a copy of the image with the positions of the bounding boxes masked.</p> Source code in <code>docprompt/utils/masking/image.py</code> <pre><code>def mask_image_from_bounding_boxes(\n    image: Image.Image,\n    *bounding_boxes: NormBBox,\n    mask_color: str = \"#000000\",\n):\n    \"\"\"\n    Create a copy of the image with the positions of the bounding boxes masked.\n    \"\"\"\n\n    width, height = image.size\n\n    mask = Image.new(\"RGBA\", (width, height), (0, 0, 0, 0))\n\n    for bbox in bounding_boxes:\n        mask.paste(\n            Image.new(\"RGBA\", (bbox.width, bbox.height), mask_color),\n            (int(bbox.x0 * width), int(bbox.top * height)),\n        )\n\n    return Image.alpha_composite(image, mask)\n</code></pre>"},{"location":"reference/utils/masking/image/","title":"image","text":""},{"location":"reference/utils/masking/image/#docprompt.utils.masking.image.mask_image_from_bounding_boxes","title":"<code>mask_image_from_bounding_boxes(image, *bounding_boxes, mask_color='#000000')</code>","text":"<p>Create a copy of the image with the positions of the bounding boxes masked.</p> Source code in <code>docprompt/utils/masking/image.py</code> <pre><code>def mask_image_from_bounding_boxes(\n    image: Image.Image,\n    *bounding_boxes: NormBBox,\n    mask_color: str = \"#000000\",\n):\n    \"\"\"\n    Create a copy of the image with the positions of the bounding boxes masked.\n    \"\"\"\n\n    width, height = image.size\n\n    mask = Image.new(\"RGBA\", (width, height), (0, 0, 0, 0))\n\n    for bbox in bounding_boxes:\n        mask.paste(\n            Image.new(\"RGBA\", (bbox.width, bbox.height), mask_color),\n            (int(bbox.x0 * width), int(bbox.top * height)),\n        )\n\n    return Image.alpha_composite(image, mask)\n</code></pre>"}]}